import os
from itertools import zip_longest
import pandas as pd
import numpy as np
import argparse
import sys
import datetime
import probe_summary_generator

# Read input arguments
parser = argparse.ArgumentParser(
    description=""" Generate bash scripts to be used for processing
    after a MIP sequencing run.
    """
)
parser.add_argument(
    "-e",
    "--experiment-id",
    help="A Unique id given to each sequencing run by the user.",
    required=True,
)
parser.add_argument(
    "-c",
    "--cpu-count",
    type=int,
    help="Number of available processors to use.",
    default=1,
)
parser.add_argument(
    "-n",
    "--server-num",
    type=int,
    help="Starting number for MIP server.",
    default=1,
)
parser.add_argument(
    "-d",
    "--data-dir",
    help=(
        "Absolute path to the directory where sequencing (.fastq/.fastq.gz)"
        "files are located."
    ),
    default="/opt/data",
)
parser.add_argument(
    "-a",
    "--analysis-dir",
    help="Absolute path to base directory for MIPWrangler working directory.",
    default="/opt/analysis",
)
parser.add_argument(
    "-w",
    "--cluster-script",
    help="Absolute path to MIPWrangler run script.",
    default="/opt/bin/runMIPWranglerCurrent.sh",
)
parser.add_argument(
    "-r",
    "--project-resource-dir",
    help=(
        "Path to directory where project specific resources such as probe sets"
        "used, MIP arm info etc. are."
    ),
    default="/opt/project_resources",
)
parser.add_argument(
    "-b",
    "--base-resource-dir",
    help=(
        "Path to directory where general resources such as barcode dictionary,"
        "sample sheet templates etc. are."
    ),
    default="/opt/resources",
)
parser.add_argument(
    "-l",
    "--sample-list",
    help="File providing a list of samples with associated information.",
    required=True,
)
parser.add_argument(
    "-s", "--sample-sets", help=("Sample sets to be processed."), required=True
)
parser.add_argument(
    "-p", "--probe-sets", help=("Probe sets to be processed."), required=True
)
parser.add_argument(
    "-k",
    "--keep-files",
    help="Keep intermediate files generated by MIPWrangler.",
    action="store_true",
)
parser.add_argument(
    "-x",
    "--stitch-options",
    help=(
        "Additional arguments to pass to MIPWrangler mipSetupAndExtractByArm."
        "This command extracts sequences and stitches paired end reads to"
        "single sequences."
    ),
    required=True,
)
parser.add_argument(
    "-m",
    "--min-capture-length",
    help="Minimum capture length for stitching, excluding probe arms.",
    type=int,
)

# Parse arguments from command line
args = vars(parser.parse_args())
experiment_id = args["experiment_id"]
cluster_script = args["cluster_script"]
cpu_count = args["cpu_count"]
server_num = args["server_num"]
fastq_dir = os.path.abspath(args["data_dir"])
analysis_dir = os.path.abspath(args["analysis_dir"])
project_resource_dir = os.path.abspath(args["project_resource_dir"])
base_resource_dir = os.path.abspath(args["base_resource_dir"])
sample_list_file = os.path.join(analysis_dir, args["sample_list"])
raw_mip_ids_dir = os.path.join(analysis_dir, "mip_ids")

# Get sample sets and probe sets to be processed.
# These sets should be provided as comma separated text, however, sometimes a
# semicolon is used by mistake. We split on commas and semicolons.
sam_set = args["sample_sets"]
sam_set = sam_set.split(",")
sample_sets = []
for s in sam_set:
    sample_sets.extend(s.split(";"))
sample_sets = set(sample_sets)

pr_set = args["probe_sets"]
pr_set = pr_set.split(",")
probe_sets = []
for p in pr_set:
    probe_sets.extend(p.split(";"))
probe_sets = set(probe_sets)

# Get MIPWrangler mipSetupAndExtractByArm options
keep_files = args["keep_files"]
stitch_options = args["stitch_options"]
min_capture_length = args["min_capture_length"]
if stitch_options == "none":
    stitch_options = []
else:
    stitch_options = stitch_options.split(",")
if min_capture_length != "none":
    for o in stitch_options:
        if "minCaptureLength" in o:
            break
    else:
        stitch_options.append("--minCaptureLength=" + str(min_capture_length))

# Create dirs if they do not exist
if not os.path.exists(raw_mip_ids_dir):
    os.makedirs(raw_mip_ids_dir)

# First part of the MIPWrangler process is to extract the sequences and
# stitch forward and reverse reads. This is done with mipSetupAndExtractByArm.
# We first read in sample information
sample_info = pd.read_table(sample_list_file)
sample_info["Sample ID"] = sample_info.apply(
    lambda a: "-".join(
        a[["sample_name", "sample_set", "replicate"]].astype(str)
    ),
    axis=1,
)
sample_info_dict = (sample_info.set_index("Sample ID")).to_dict(orient="index")

# Next, we select the libraries belonging to sample and probe sets specified
selected_samples = set()
for sid in sample_info_dict:
    if sample_info_dict[sid]["sample_set"] in sample_sets:
        pset = sample_info_dict[sid]["probe_set"]
        pset = pset.split(",")
        ps = []
        for s in pset:
            ps.extend(s.strip().split(";"))
        ps = set([p.strip() for p in ps])
        if len(ps.intersection(probe_sets)) > 0:
            selected_samples.add(sid)


mipset_table = os.path.join(project_resource_dir, "mip_ids", "mipsets.csv")
mipsets = pd.read_csv(mipset_table)
mipset_list = mipsets.to_dict(orient="list")

# Convert the mip sets dataframe to dict for easy access and keep mip arm files
# for each mip set in a dictionary
all_probes = {}
mip_arms_dict = {}
for mipset in mipset_list:
    list_m = mipset_list[mipset]

    # The file name should be the second line in the mipsets.csv
    mip_arms_dict[mipset] = list_m[0]

    # The rest of the lines have probe names in the set
    set_m = set(list_m[1:])
    set_m.discard(np.nan)
    all_probes[mipset] = set_m

# For the sample and probe set create:
# 1) MIPWrangler input files (samples etc.)
# 2) Scripts for MIPWrangler Part I (extract + stitch)
# 3) Scripts for MIPWrangler Part II (barcode correction + clustering)
probes = set()
mip_arms_list = []
for p_name in probe_sets:
    try:
        temp_probes = all_probes[p_name]
    except KeyError:
        print(
            (
                "Probe set name {} is not present in the mipsets "
                "file. This probe set will be ignored."
            ).format(p_name)
        )
        continue
    arm_file = os.path.join(
        project_resource_dir, "mip_ids", mip_arms_dict[p_name]
    )
    try:
        with open(arm_file) as infile:
            mip_arms_list.append(pd.read_table(infile))
            probes.update(temp_probes)
    except IOError:
        print(
            (
                "MIP arm file {} is required but missing for "
                "the probe set {}. It will be generated if the "
                "mip_info.json resource file is available at "
                "/opt/project_resources/mip_ids/mip_info.json"
            ).format(arm_file, p_name)
        )
        try:
            # Generate the probe arm file from the mip info file
            probe_summary_generator.generate_mip_arms_file(
                p_name, probe_sets_file=mipset_table
            )
            # Load the arm file generated
            with open(arm_file) as infile:
                mip_arms_list.append(pd.read_table(infile))
                probes.update(temp_probes)
        except Exception as e:
            print(
                (
                    "MIP arm file generation for probe set {} "
                    "failed due to {}."
                )
            ).format(p_name, e)
if len(mip_arms_list) == 0:
    print(
        (
            "No MIP arms file were found for the probe sets {}"
            " scripts will not be generated for them. Make sure "
            "relevant files are present in the {} directory"
        ).format(probe_sets, project_resource_dir)
    )
    sys.exit(1)
mip_arms_table = pd.concat(mip_arms_list, ignore_index=True).drop_duplicates()
mip_arms_table = mip_arms_table.loc[mip_arms_table["mip_family"].isin(probes)]
mip_family_names = probes

# Create MIPWrangler Input files
sample_subset = list(selected_samples)
with open(
    os.path.join(raw_mip_ids_dir, "allMipsSamplesNames.tab.txt"), "w"
) as outfile:
    outfile_list = ["\t".join(["mips", "samples"])]
    mips_samples = zip_longest(mip_family_names, sample_subset, fillvalue="")
    for ms in mips_samples:
        outfile_list.append("\t".join(ms))
    outfile.write("\n".join(outfile_list))
    pd.DataFrame(mip_arms_table).groupby("mip_id").first().reset_index().dropna(
        how="all", axis=1
    ).to_csv(
        os.path.join(raw_mip_ids_dir, "mipArms.txt"), sep="\t", index=False
    )

# Create MIPWrangler part I script commands (extract + stitch)
stitch_commands = [
    ["cd", analysis_dir],
    [
        "nohup MIPWrangler mipSetupAndExtractByArm",
        "--mipArmsFilename",
        os.path.join(raw_mip_ids_dir, "mipArms.txt"),
        "--mipSampleFile",
        os.path.join(raw_mip_ids_dir, "allMipsSamplesNames.tab.txt"),
        "--numThreads",
        str(cpu_count),
        "--masterDir analysis",
        "--dir",
        fastq_dir,
        "--mipServerNumber",
        str(server_num),
    ],
]
stitch_commands[-1].extend(stitch_options)
if keep_files:
    stitch_commands[-1].append("--keepIntermediateFiles")

# Create MIPWrangler part II script commands (barcode correction + clustering)
now = datetime.datetime.now()
run_date = now.strftime("%Y%m%d")
info_file = os.path.join(
    analysis_dir, "analysis/populationClustering/allInfo.tab.txt"
)
renamed_info = os.path.join(
    analysis_dir, "run_" + experiment_id + "_wrangled_" + run_date + ".txt"
)
wrangler_commands = [
    ["cd", "analysis"],
    ["nohup", "bash", cluster_script, str(server_num), str(cpu_count)],
    ["mv", os.path.join(analysis_dir, "analysis/logs"), analysis_dir],
    ["mv", os.path.join(analysis_dir, "analysis/scripts"), analysis_dir],
    ["mv", os.path.join(analysis_dir, "analysis/resources"), analysis_dir],
    [
        "mv",
        os.path.join(analysis_dir, "analysis/nohup.out"),
        os.path.join(analysis_dir, "nohup2.out"),
        "2>/dev/null ||true",
    ],
    ["mv", info_file, renamed_info],
    ["pigz", "-9", "-p", str(cpu_count), renamed_info],
]
extraction_summary_file = "extractInfoSummary.txt"
extraction_per_target_file = "extractInfoByTarget.txt"
stitching_per_target_file = "stitchInfoByTarget.txt"
for filename in [
    extraction_summary_file,
    extraction_per_target_file,
    stitching_per_target_file,
]:
    stat_command = [
        "find",
        os.path.join(analysis_dir, "analysis"),
        "-name",
        filename,
        "-exec",
        "cat",
        "{}",
        "+",
        ">",
        os.path.join(analysis_dir, filename),
    ]
    wrangler_commands.append(stat_command)
wrangler_commands.append(["cd", "/opt/analysis"])
for filename in [
    extraction_summary_file,
    extraction_per_target_file,
    stitching_per_target_file,
]:
    zip_command = ["pigz", "-9", "-p", str(cpu_count), filename]
    wrangler_commands.append(zip_command)
    mv_command = [
        "mv",
        filename + ".gz",
        experiment_id + "_" + run_date + "_" + filename + ".gz",
    ]
server_num += 1

# Save the final script to a file to run
with open(os.path.join(analysis_dir, "wrangle.sh"), "w") as outfile:
    outfile.write("\n".join([" ".join(c) for c in stitch_commands]) + "\n")
    outfile.write("\n".join([" ".join(c) for c in wrangler_commands]) + "\n")
