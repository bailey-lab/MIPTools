{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use code cells in this notebook\n",
    "\n",
    "If a code cell starts with \n",
    "```python\n",
    "# RUN\n",
    "```\n",
    "Run the cell by CTRL+Enter, or the Run button above.  \n",
    "\n",
    "If a code cell starts with\n",
    "```python\n",
    "# USER INPUT\n",
    "```\n",
    "User input is needed before running the cell. Usually there will be a cell preceding this which gives an example for the values to be provided.\n",
    "\n",
    "If a code cell starts with\n",
    "```python\n",
    "# OPTIONAL USER INPUT\n",
    "```\n",
    "User input is needed before running the cell. However, some defaults are provided, so make sure that either the settings will work for your run, or change them appropriately.\n",
    "\n",
    "If a cell starts with\n",
    "\n",
    "**Example cell**\n",
    "\n",
    "These cells are not code cells but examples of user inputs from the test data analysis for the actual code cell that follows it, informing the user about the formatting etc.\n",
    "\n",
    "**Important note on entering input:** When entering user input, please make sure you follow the formatting provided in the example cells. For example, when the parameter is text, make sure you have quotation marks around the parameters but when it is a number, do not enclose in quotes. If it is a list, then provide a list in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes reloading.\n",
      "functions reloading\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "# these commands import necessary functions for the rest of the program\n",
    "import sys\n",
    "sys.path.append(\"/opt/src\")\n",
    "import mip_functions as mip\n",
    "import os\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "wdir = \"/opt/analysis/\"\n",
    "data_dir = \"/opt/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "\n",
    "What is your wrangler output data called? This is usually a file name that ends in run_(experiment_id)\\_wrangled\\_(date).txt.gz and is in the folder that you bound as /opt/data. What sample sheet did you use when you wrangled the data? What species did you use? This will likely be the (species) portion of a folder called (species)\\_species\\_resources. For example, Plasmodium falciparum in the tutorial dataset is in a folder called pf_species_resources and has been assigned the species name 'pf'\n",
    "\n",
    "```python\n",
    "\n",
    "# provide the MIPWrangler output file\n",
    "# which must be located in the /opt/data directory within the container.\n",
    "info_file = \"run_test_run_wrangled_20221102.txt.gz\" \n",
    "\n",
    "# sample sheet used by the wrangler run (must be located in /opt/data)\n",
    "sample_sheet = \"sample_list.tsv\"\n",
    "\n",
    "# species name associated with the species_resources folder\n",
    "species = 'pf'\n",
    "\n",
    "# No input below\n",
    "info_file = data_dir + info_file\n",
    "sample_sheet = data_dir + sample_sheet\n",
    "pd.read_table(sample_sheet).groupby([\"sample_set\", \"probe_set\"]).first()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide the MIPWrangler output file\n",
    "# which must be located in the /opt/data directory within the container.\n",
    "info_file = \"\" \n",
    "\n",
    "# sample sheet associated with the wrangler file\n",
    "sample_sheet = \"\"\n",
    "\n",
    "# No input below\n",
    "info_file = [data_dir + info_file]\n",
    "sample_sheet = [data_dir + sample_sheet]\n",
    "pd.read_table(sample_sheet[0]).groupby([\"sample_set\", \"probe_set\"]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "Which sample sets and probe sets would you like to analyze? These are listed in your sample sheet under the \"sample_set\" and \"probe_set\" columns, and given to Jupyter as a [sample_set, probe_set] pair. You can also see an example row by running the Jupyter cell above.\n",
    "\n",
    "```python\n",
    "sample_groups = [[\"JJJ\", \"DR1,VAR4\"]]\n",
    "```\n",
    "\n",
    "If more than one combination is to be used, the input will be a list of lists, for example:\n",
    "```python\n",
    "sample_groups = [[\"sample_set_1\", \"probe_set_1\"], [\"sample_set_2\", \"probe_set_2\"]]\n",
    "```\n",
    "\n",
    "There are two closely related names in this section: probe_set and probe_sets_used. probe_set is a column from the input sample sheet. If a sample was captured/sequenced with multiple probe sets at the same time, there might optionally be multiple comma delimited probe sets in this column (e.g. DR1,VAR4 if sequencing was performed on DR1 and VAR4 probe sets). probe_sets_used is a string of the probe sets you'd like to analyze here in this Jupyter notebook (e.g. DR1 if you only want to analyze DR1 probes, or DR1,VAR4 if you'd like to analyze both DR1 and VAR4 probes.\n",
    "\n",
    "probe_sets_used = \"DR1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "sample_groups = [[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "How many processors would you like to use? Enter an integer that is less than or equal to the number of available processors on the computer/compute node that you are using. More processors means faster run time but higher likelihood of CPU crashes if your machine doesn't have enough RAM to handle the job.\n",
    "\n",
    "```python\n",
    "# available cpu count\n",
    "processorNumber = 12\n",
    "\n",
    "## extra bwa options for haplotype alignment\n",
    "# use \"-a\" for getting all alignments\n",
    "# use \"-L 500\" to penalize soft clipping \n",
    "# use \"-t\" to set number of available processors\n",
    "bwaExtra = [\"-t\", str(processorNumber)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "# available cpu count\n",
    "processorNumber = 12\n",
    "\n",
    "## extra bwa options for haplotype alignment\n",
    "# use \"-a\" for getting all alignments\n",
    "# use \"-L 500\" to penalize soft clipping \n",
    "# use \"-t\" to set number of available processors\n",
    "bwaExtra = [\"-t\", str(processorNumber)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get/Set the analysis settings\n",
    "The cell below will retrieve a template of default analysis settings to use. It will then modify these settings to match the variables you defined above, and save them to whatever folder you bound to the singularity container's /opt/analysis folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# copy the template settings file\n",
    "temp_settings_file = \"/opt/resources/templates/analysis_settings_templates/settings.txt\"\n",
    "subprocess.call([\"scp\", temp_settings_file, \"/opt/analysis/template_settings.txt\"])\n",
    "\n",
    "# extract the settings template\n",
    "temp_settings = mip.get_analysis_settings(\"/opt/analysis/template_settings.txt\")\n",
    "\n",
    "# update bwa settings with the options set above\n",
    "bwaOptions = temp_settings[\"bwaOptions\"]\n",
    "try:\n",
    "    bwaOptions.extend(bwaExtra)\n",
    "except AttributeError:\n",
    "    bwaOptions = [bwaOptions]\n",
    "    bwaOptions.extend(bwaExtra)\n",
    "\n",
    "# Create a list from the probe_sets string\n",
    "mipSetKey = probe_sets_used.split(\",\") + [\"\"]\n",
    "\n",
    "# create a dictionary for which settings should be updated\n",
    "# using the user specified parameters.\n",
    "update_keys = {\"processorNumber\": processorNumber,\n",
    "               \"bwaOptions\": bwaOptions,\n",
    "               \"species\": species,\n",
    "               \"mipSetKey\" : mipSetKey}\n",
    "# update the settings\n",
    "for k, v in update_keys.items():\n",
    "    temp_settings[k] = v\n",
    "# create a settings file in the analysis directory.\n",
    "settings_file = \"settings.txt\"\n",
    "settings_path = os.path.join(wdir, settings_file)\n",
    "mip.write_analysis_settings(temp_settings, settings_path)\n",
    "settings = mip.get_analysis_settings(wdir + settings_file)\n",
    "# create probe sets dictionary\n",
    "try:\n",
    "    mip.update_probe_sets(\"/opt/project_resources/mip_ids/mipsets.csv\",\n",
    "                         \"/opt/project_resources/mip_ids/probe_sets.json\")\n",
    "except IOError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIPWrangler output file processing\n",
    "The operation below filters and renames some of the columns from the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "mip.process_info_file(wdir,\n",
    "                      settings_file, \n",
    "                      info_files,\n",
    "                      sample_sheets,\n",
    "                      settings[\"mipsterFile\"],\n",
    "                      sample_sets=sample_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and map haplotype sequences\n",
    "Align each haplotype sequence to the reference genome. Remove off target haplotypes. All haplotype mappings will be saved to the disk so off targets can be inspected if needed. \n",
    "\n",
    "Some filters can be applied to remove noise:\n",
    "*  minHaplotypeBarcodes: minimum total UMI cut off across all samples.\n",
    "*  minHaplotypeSamples: minimum number of samples a haplotype is observed in.\n",
    "*  minHaplotypeSampleFraction: minimum fraction of samples a haplotype is observed in.  \n",
    "\n",
    "It is probably safe to apply minimal count filters like at least 10 UMIs across samples and at least two samples. However, most data sets will be easily handled without these filters. So it may be better to not filter at this step unless the downstream operations are taking too much resources. However, filters can and should be applied after variant calls are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# filter haplotype sequences based on the number of total supporting UMIs\n",
    "settings[\"minHaplotypeBarcodes\"] = 1\n",
    "# filter haplotype sequences based on the number of samples they were observed in\n",
    "settings[\"minHaplotypeSamples\"] = 1\n",
    "# filter haplotype sequences based on the fraction of samples they were observed in\n",
    "settings[\"minHaplotypeSampleFraction\"] = 0.0001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "# filter haplotype sequences based on the number of total supporting UMIs\n",
    "settings[\"minHaplotypeBarcodes\"] = 1\n",
    "# filter haplotype sequences based on the number of samples they were observed in\n",
    "settings[\"minHaplotypeSamples\"] = 1\n",
    "# filter haplotype sequences based on the fraction of samples they were observed in\n",
    "settings[\"minHaplotypeSampleFraction\"] = 0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN\n",
    "mip.map_haplotypes(settings)\n",
    "mip.get_haplotype_counts(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the mapping results\n",
    "Plotting the probe coverage by samples is a good  way to see overall experiment perfomance. It shows if a probe has at least 1 barcode (or however many is specified below) for a given sample.  \n",
    "\n",
    "Dark columns point to poor performing probes whereas dark rows indicate poor samples. Note that this excludes samples with no reads at all. Use \"all_barcode_counts.csv\" file if those are of interest as well.\n",
    "\n",
    "Some parameters can be supplied to the plotting function as noted in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# coverage filter: anything below this number will be considered absent\n",
    "barcode_threshold = 10\n",
    "# font size for tick labels for x and y axis\n",
    "tick_label_size=5\n",
    "# font size for heat map color bar\n",
    "cbar_label_size=5\n",
    "# figure resolution\n",
    "dpi=300\n",
    "# present/absent colors\n",
    "absent_color='black'\n",
    "present_color='green'\n",
    "# Save the plot in the analysis directory?\n",
    "# If false, plots the graph here.\n",
    "save=False\n",
    "# How frequent the x and y-axis ticks should be\n",
    "# every nth column will have  a tick\n",
    "ytick_freq=None\n",
    "xtick_freq=None\n",
    "# rotation of xtick labels\n",
    "xtick_rotation=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "barcode_counts = pd.read_csv(wdir + \"barcode_counts.csv\",\n",
    "             header = [0,1], index_col = 0)\n",
    "mip.plot_performance(barcode_counts,\n",
    "                     barcode_threshold=barcode_threshold,\n",
    "                     tick_label_size=tick_label_size,\n",
    "                     cbar_label_size=cbar_label_size,\n",
    "                     dpi=dpi,\n",
    "                     absent_color=absent_color,\n",
    "                     present_color=present_color,\n",
    "                     save=save,\n",
    "                     ytick_freq=ytick_freq,\n",
    "                     xtick_freq=xtick_freq,\n",
    "                     xtick_rotation=xtick_rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at summary stats \n",
    "There are summary statistics and meta data (if provided) we can use to determine if coverage is enough, whether further sequencing is necessary, and how to proceed if further sequencing will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "sample_summary = pd.read_csv(wdir + \"sample_summary.csv\")\n",
    "sample_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot total barcode count vs probe coverage\n",
    "A scatter plot of total barcode count vs number of probes covered at a certain barcode count is a good way to see how the relationship between total coverage and probe coverage, which is useful in determining how to proceed to the next experiments or analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "f = sns.pairplot(data = sample_summary,\n",
    "                x_vars = \"Barcode Count\",\n",
    "                y_vars = \"targets_with_10_barcodes\",\n",
    "                plot_kws={\"s\": 10})\n",
    "f.fig.set_size_inches(5,5)\n",
    "f.fig.set_dpi(150)\n",
    "_ = plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repooling capture reactions for further sequencing.\n",
    "### Factors to consider:\n",
    "1. What do you we want to accomplish? In most cases, we would like to get enough coverage for a number of probes for each sample. For example, the test data contains **50 probes** in total. Let's say it is sufficient if we had a coverage of **10** or more for each probe for a sample. Then, we would not want to sequence any more of that sample. \n",
    "```python\n",
    "target_coverage_count = 50\n",
    "target_coverage_key='targets_with_10_barcodes'\n",
    "```\n",
    "Alternatively, we can set a goal of a fraction of total probes to reach a certain coverage rather than an absolute number of probes. For 95% of the maximum number of probes observed (47 in this case): \n",
    "```python\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key='targets_with_10_barcodes'\n",
    "```\n",
    "Although we set our goal to 47 probes, it is likely that some sample will never reach that number regardless of how much we sequence, if there is a deletion in the region, for example. So it makes sense to set a total coverage threshold after which we don't expect more data. Looking at the plot above, it seems like after 1000 barcode counts, we would reach our goal for most samples. \n",
    "```python\n",
    "high_barcode_threshold = 10000\n",
    "```\n",
    "Another metric to use for determining if we want to sequence a sample more is the average read count per barcode count. This value indicates we have sequenced each unique molecular index in our sample so many times, so when the value is high, it is unlikely that we'd get more UMIs by sequencing the same library more. It makes more sense for a fresh MIP capture from these samples if more data is needed.\n",
    "```python\n",
    "barcode_coverage_threshold=10\n",
    "```\n",
    "Some samples perform very poorly for one reason or another. There are two options for these samples for repooling consideration: 1) Repool as much as we can for the next run, 2) Assuming there is a problem in the capture reaction, set up a new MIP capture reaction for these samples. It makes more sense to use option 1 if this is the first sequencing data using this library. Use option 2 if this library have been repooled at a higher volume already, but still producing poor data.\n",
    "```python\n",
    "barcode_count_threshold=100 # samples below total barcode count of this value is considered low coverage\n",
    "low_coverage_action='Repool' # what to do for low coverage samples (Repool or Recapture)\n",
    "```\n",
    "Sometimes a handful of samples show uneven coverage of loci, i.e. they have very good coverage of a handful of loci but poor coverage in others, which may point to a problem with the sample or the experiment in general. These samples are determined by comparing the subset of samples that reached the goal we set (completed samples) and those that have not. We look at the number of barcodes per probe for _completed_ samples and get 25th percentile (or other percentile as set) and assume that if a sample on average has this many barcodes per target, it should have reached the set goal. For example, if on average _completed_ samples, i.e. samples that cover 47 probes at 10 barcodes or more, have 10000 total barcodes, they would have ~200 (10000/47) barcodes per target covered. And if an _incomplete_ sample has 5000 total barcodes and only 10 targets covered, this value would be 500 for that sample and it would be flagged as **uneven coverage** in repooling document.\n",
    "```python\n",
    "assesment_key='targets_with_1_barcodes' # coverage key to compare \"complete\" and \"incomplete\" samples\n",
    "good_coverage_quantile=0.25 # percentile to set the threshold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "high_barcode_threshold = 10000\n",
    "target_coverage_count = None\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key = 'targets_with_10_barcodes'\n",
    "barcode_coverage_threshold = 10\n",
    "barcode_count_threshold = 100\n",
    "low_coverage_action = 'Recapture'\n",
    "assesment_key = 'targets_with_1_barcodes'\n",
    "good_coverage_quantile = 0.25\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "high_barcode_threshold = \n",
    "low_coverage_action = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "target_coverage_count = None\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key = 'targets_with_10_barcodes'\n",
    "barcode_coverage_threshold = 10\n",
    "barcode_count_threshold = 100\n",
    "assesment_key = 'targets_with_1_barcodes'\n",
    "good_coverage_quantile = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "meta = pd.read_csv(wdir + \"run_meta.csv\")\n",
    "data_summary = pd.merge(sample_summary, meta)\n",
    "mip.repool(wdir, \n",
    "           data_summary, \n",
    "           high_barcode_threshold, \n",
    "           target_coverage_count=target_coverage_count, \n",
    "           target_coverage_fraction=target_coverage_fraction, \n",
    "           target_coverage_key=target_coverage_key,\n",
    "           barcode_coverage_threshold=barcode_coverage_threshold,\n",
    "           barcode_count_threshold=barcode_count_threshold, \n",
    "           low_coverage_action=low_coverage_action,\n",
    "           assesment_key=assesment_key,\n",
    "           good_coverage_quantile=good_coverage_quantile,\n",
    "           output_file='repool.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the repool document\n",
    "Library to completion field in the repool document has the value (volume) of how much from a sample should be pooled for re-sequencing. These values are only rough estimates and care should be taken to make sure there will be enough material to sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "pd.read_csv(wdir + \"repool.csv\").head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "299px",
    "width": "248px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1114px",
    "left": "977px",
    "top": "163px",
    "width": "256.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
