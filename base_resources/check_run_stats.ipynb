{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use code cells in this notebook\n",
    "If a code cell starts with \n",
    "```python\n",
    "# RUN\n",
    "```\n",
    "Run the cell by CTRL+Enter, or the Run button above.  \n",
    "\n",
    "If a code cell starts with\n",
    "```python\n",
    "# USER INPUT\n",
    "```\n",
    "User input is needed before running the cell. Usually there will be a cell preceding this which gives an example for the values to be provided.\n",
    "\n",
    "If a code cell starts with\n",
    "```python\n",
    "# OPTIONAL USER INPUT\n",
    "```\n",
    "User input is needed before running the cell. However, some defaults are provided, so make sure that either the settings will work for your run, or change them appropriately.\n",
    "\n",
    "If a cell starts with\n",
    "#### Example cell\n",
    "These cells are not code cells but examples of user inputs from the test data analysis for the actual code cell that follows it, informing the user about the formatting etc.\n",
    "\n",
    "**Important note on entering input:** When entering user input, please make sure you follow the formatting provided in the example cells. For example, when the parameter is text, make sure you have quotation marks around the parameters but when it is a number, do not enclose in quotes. If it is a list, then provide a list in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes reloading.\n",
      "functions reloading\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "import sys\n",
    "sys.path.append(\"/opt/src\")\n",
    "import mip_functions as mip\n",
    "import probe_summary_generator\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import allel\n",
    "wdir = \"/opt/analysis/\"\n",
    "data_dir = \"/opt/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "\n",
    "# provide the MIPWrangler output files\n",
    "# which must be located in the /opt/data directory within the container.\n",
    "# if more than one run is to be merged, provide all files\n",
    "info_files = [\"run_test_run_wrangled_20221102.txt.gz\"] \n",
    "\n",
    "# sample sheets associated with each wrangler file,\n",
    "# in the same order as the wrangler files.\n",
    "sample_sheets = [\"sample_list.tsv\"]\n",
    "\n",
    "# No input below\n",
    "info_files = [data_dir + i for i in info_files]\n",
    "sample_sheets = [data_dir + s for s in sample_sheets]\n",
    "pd.concat([pd.read_table(s) for s in sample_sheets],\n",
    "         ignore_index=True).groupby([\"sample_set\", \"probe_set\"]).first()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide the MIPWrangler output files\n",
    "# which must be located in the /opt/data directory within the container.\n",
    "# if more than one run is to be merged, provide all files\n",
    "info_files = [] \n",
    "\n",
    "# sample sheets associated with each wrangler file,\n",
    "#you should only have one sample sheet (in cases of multiple sample sheets, merge them first)\n",
    "sample_sheets = []\n",
    "\n",
    "# No input below\n",
    "info_files = [data_dir + i for i in info_files]\n",
    "sample_sheets = [data_dir + s for s in sample_sheets]\n",
    "pd.concat([pd.read_table(s) for s in sample_sheets],\n",
    "         ignore_index=True).groupby([\"sample_set\", \"probe_set\"]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the unique sample_set, probe_set combinations in the sample sheets provided. Select which combinations should be used for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "sample_groups = [[\"JJJ\", \"DR1,VAR4\"]]\n",
    "```\n",
    "\n",
    "If more than one combination is to be used, the input will be a list of lists, for example:\n",
    "```python\n",
    "sample_groups = [[\"sample_set_1\", \"probe_set_1\"], [\"sample_set_2\", \"probe_set_2\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "sample_groups = [[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the species and the probe set used\n",
    "These two are important parameters to determine which files will be used for analysis.  \n",
    "\n",
    "\n",
    "For the species, the options are: \"pf\" for *Plasmodium falciparum*, \"pv\" for *Plasmodium vivax*, \"hg19\" for *Homo sapiens* genome assembly hg19/GRCh37 and \"hg38\" for *Homo sapiens* genome assembly hg38/GRCh38  \n",
    "___\n",
    "Probe sets also must be specified. Check the output of the sample sheet summary above under **probe_set** field for a reminder of what the probe set of interest is. This is usually a three letter code or codes separated by a comma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "species = \"pf\"\n",
    "probe_sets_used = \"DR1,VAR4\"\n",
    "```\n",
    "\n",
    "It is also possible to analyse just a subset of probe sets that has been used. For example, if the data has both DR1 and VAR4 probe sets but I want to analyse only the DR1 set:\n",
    "```python\n",
    "species = \"pf\"\n",
    "probe_sets_used = \"DR1\"\n",
    "```\n",
    "Note that I'd still need to specifiy \"DR1,VAR4\" in the sample_groups above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "species = \n",
    "probe_sets_used = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# available cpu count\n",
    "processorNumber = 20\n",
    "\n",
    "## extra bwa options for haplotype alignment\n",
    "# use \"-a\" for getting all alignments\n",
    "# use \"-L 500\" to penalize soft clipping \n",
    "# use \"-t\" to set number of available processors\n",
    "bwaExtra = [\"-t\", str(processorNumber)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "# available cpu count\n",
    "processorNumber = 20\n",
    "\n",
    "## extra bwa options for haplotype alignment\n",
    "# use \"-a\" for getting all alignments\n",
    "# use \"-L 500\" to penalize soft clipping \n",
    "# use \"-t\" to set number of available processors\n",
    "bwaExtra = [\"-t\", str(processorNumber)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get/Set the analysis settings\n",
    "Use the settings template for the species specified to get the  analysis settings and change the vaules specified in the above cell. This will create a template_settings.txt file in your analysis directory and a settings.txt file to be used for the analysis. These files also will serve as a reference of analysis settings for the sake of reproducibility.  \n",
    "\n",
    "The last step of the below cell attempts to save a file to the /opt/project_resources directory. If you do not have write permission to the location, you cannot save that file. However, if a file has been previously saved in the directory, it will be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# copy the template settings file\n",
    "temp_settings_file = \"/opt/resources/templates/analysis_settings_templates/settings.txt\"\n",
    "subprocess.call([\"scp\", temp_settings_file, \"/opt/analysis/template_settings.txt\"])\n",
    "\n",
    "# extract the settings template\n",
    "temp_settings = mip.get_analysis_settings(\"/opt/analysis/template_settings.txt\")\n",
    "\n",
    "# update bwa settings with the options set above\n",
    "bwaOptions = temp_settings[\"bwaOptions\"]\n",
    "try:\n",
    "    bwaOptions.extend(bwaExtra)\n",
    "except AttributeError:\n",
    "    bwaOptions = [bwaOptions]\n",
    "    bwaOptions.extend(bwaExtra)\n",
    "\n",
    "# Create a list from the probe_sets string\n",
    "mipSetKey = probe_sets_used.split(\",\") + [\"\"]\n",
    "\n",
    "# create a dictionary for which settings should be updated\n",
    "# using the user specified parameters.\n",
    "update_keys = {\"processorNumber\": processorNumber,\n",
    "               \"bwaOptions\": bwaOptions,\n",
    "               \"species\": species,\n",
    "               \"mipSetKey\" : mipSetKey}\n",
    "# update the settings\n",
    "for k, v in update_keys.items():\n",
    "    temp_settings[k] = v\n",
    "# create a settings file in the analysis directory.\n",
    "settings_file = \"settings.txt\"\n",
    "settings_path = os.path.join(wdir, settings_file)\n",
    "mip.write_analysis_settings(temp_settings, settings_path)\n",
    "settings = mip.get_analysis_settings(wdir + settings_file)\n",
    "# create probe sets dictionary\n",
    "try:\n",
    "    mip.update_probe_sets(\"/opt/project_resources/mip_ids/mipsets.csv\",\n",
    "                         \"/opt/project_resources/mip_ids/probe_sets.json\")\n",
    "except IOError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process run data\n",
    "First section of the data analysis involves processing the MIPWrangler output files, combining data from multiple runs (if necessary), mapping haplotypes and creating summary files and plots showing how the sequencing runs went."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIPWrangler output file processing\n",
    "Below operation combines output files from multiple runs, summing up count data belonging to the same libraries.  \n",
    "\n",
    "Libraries are labeled by combining three fields in the sample sheet: sample_name-sample_set-replicate, which makes the Sample ID. If two different libraries has the same Sample ID (same three fields, but a different LibraryPrep identifier), the overlapping libraries will be assigned new replicate numbers such that there are no shared IDs any more. A warning will be printed in that case, and the original sample ID and the new one will be written to the samples.tsv file generated in the analysis directory.\n",
    "\n",
    "If only a single output file is used, then the below operation just filters and renames some columns from the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "if len(info_files) > 1:\n",
    "    mip.combine_info_files(wdir,\n",
    "                           settings_file, \n",
    "                           info_files,\n",
    "                           sample_sheets,\n",
    "                           settings[\"mipsterFile\"],\n",
    "                           sample_sets=sample_groups)\n",
    "else:\n",
    "    mip.process_info_file(wdir,\n",
    "                          settings_file, \n",
    "                          info_files,\n",
    "                          sample_sheets,\n",
    "                          settings[\"mipsterFile\"],\n",
    "                          sample_sets=sample_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and map haplotype sequences\n",
    "Align each haplotype sequence to the reference genome. Remove off target haplotypes. All haplotype mappings will be saved to the disk so off targets can be inspected if needed. \n",
    "\n",
    "Some filters can be applied to remove noise:\n",
    "*  minHaplotypeBarcodes: minimum total UMI cut off across all samples.\n",
    "*  minHaplotypeSamples: minimum number of samples a haplotype is observed in.\n",
    "*  minHaplotypeSampleFraction: minimum fraction of samples a haplotype is observed in.  \n",
    "\n",
    "It is probably safe to apply minimal count filters like at least 10 UMIs across samples and at least two samples. However, most data sets will be easily handled without these filters. So it may be better to not filter at this step unless the downstream operations are taking too much resources. However, filters can and should be applied after variant calls are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# filter haplotype sequences based on the number of total supporting UMIs\n",
    "settings[\"minHaplotypeBarcodes\"] = 1\n",
    "# filter haplotype sequences based on the number of samples they were observed in\n",
    "settings[\"minHaplotypeSamples\"] = 1\n",
    "# filter haplotype sequences based on the fraction of samples they were observed in\n",
    "settings[\"minHaplotypeSampleFraction\"] = 0.0001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "# filter haplotype sequences based on the number of total supporting UMIs\n",
    "settings[\"minHaplotypeBarcodes\"] = 1\n",
    "# filter haplotype sequences based on the number of samples they were observed in\n",
    "settings[\"minHaplotypeSamples\"] = 1\n",
    "# filter haplotype sequences based on the fraction of samples they were observed in\n",
    "settings[\"minHaplotypeSampleFraction\"] = 0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN\n",
    "mip.map_haplotypes(settings)\n",
    "mip.get_haplotype_counts(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the mapping results\n",
    "Plotting the probe coverage by samples is a good  way to see overall experiment perfomance. It shows if a probe has at least 1 barcode (or however many is specified below) for a given sample.  \n",
    "\n",
    "Dark columns point to poor performing probes whereas dark rows indicate poor samples. Note that this excludes samples with no reads at all. Use \"all_barcode_counts.csv\" file if those are of interest as well.\n",
    "\n",
    "Some parameters can be supplied to the plotting function as noted in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# coverage filter: anything below this number will be considered absent\n",
    "barcode_threshold = 10\n",
    "# font size for tick labels for x and y axis\n",
    "tick_label_size=5\n",
    "# font size for heat map color bar\n",
    "cbar_label_size=5\n",
    "# figure resolution\n",
    "dpi=300\n",
    "# present/absent colors\n",
    "absent_color='black'\n",
    "present_color='green'\n",
    "# Save the plot in the analysis directory?\n",
    "# If false, plots the graph here.\n",
    "save=False\n",
    "# How frequent the x and y-axis ticks should be\n",
    "# every nth column will have  a tick\n",
    "ytick_freq=None\n",
    "xtick_freq=None\n",
    "# rotation of xtick labels\n",
    "xtick_rotation=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "barcode_counts = pd.read_csv(wdir + \"barcode_counts.csv\",\n",
    "             header = [0,1], index_col = 0)\n",
    "mip.plot_performance(barcode_counts,\n",
    "                     barcode_threshold=barcode_threshold,\n",
    "                     tick_label_size=tick_label_size,\n",
    "                     cbar_label_size=cbar_label_size,\n",
    "                     dpi=dpi,\n",
    "                     absent_color=absent_color,\n",
    "                     present_color=present_color,\n",
    "                     save=save,\n",
    "                     ytick_freq=ytick_freq,\n",
    "                     xtick_freq=xtick_freq,\n",
    "                     xtick_rotation=xtick_rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at summary stats \n",
    "There are summary statistics and meta data (if provided) we can use to determine if coverage is enough, whether further sequencing is necessary, and how to proceed if further sequencing will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "sample_summary = pd.read_csv(wdir + \"sample_summary.csv\")\n",
    "sample_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot total barcode count vs probe coverage\n",
    "A scatter plot of total barcode count vs number of probes covered at a certain barcode count is a good way to see how the relationship between total coverage and probe coverage, which is useful in determining how to proceed to the next experiments or analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "f = sns.pairplot(data = sample_summary,\n",
    "                x_vars = \"Barcode Count\",\n",
    "                y_vars = \"targets_with_10_barcodes\",\n",
    "                plot_kws={\"s\": 10})\n",
    "f.fig.set_size_inches(5,5)\n",
    "f.fig.set_dpi(150)\n",
    "_ = plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repooling capture reactions for further sequencing.\n",
    "### Factors to consider:\n",
    "1. What do you we want to accomplish? In most cases, we would like to get enough coverage for a number of probes for each sample. For example, the test data contains **50 probes** in total. Let's say it is sufficient if we had a coverage of **10** or more for each probe for a sample. Then, we would not want to sequence any more of that sample. \n",
    "```python\n",
    "target_coverage_count = 50\n",
    "target_coverage_key='targets_with_10_barcodes'\n",
    "```\n",
    "Alternatively, we can set a goal of a fraction of total probes to reach a certain coverage rather than an absolute number of probes. For 95% of the maximum number of probes observed (47 in this case): \n",
    "```python\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key='targets_with_10_barcodes'\n",
    "```\n",
    "Although we set our goal to 47 probes, it is likely that some sample will never reach that number regardless of how much we sequence, if there is a deletion in the region, for example. So it makes sense to set a total coverage threshold after which we don't expect more data. Looking at the plot above, it seems like after 1000 barcode counts, we would reach our goal for most samples. \n",
    "```python\n",
    "high_barcode_threshold = 10000\n",
    "```\n",
    "Another metric to use for determining if we want to sequence a sample more is the average read count per barcode count. This value indicates we have sequenced each unique molecular index in our sample so many times, so when the value is high, it is unlikely that we'd get more UMIs by sequencing the same library more. It makes more sense for a fresh MIP capture from these samples if more data is needed.\n",
    "```python\n",
    "barcode_coverage_threshold=10\n",
    "```\n",
    "Some samples perform very poorly for one reason or another. There are two options for these samples for repooling consideration: 1) Repool as much as we can for the next run, 2) Assuming there is a problem in the capture reaction, set up a new MIP capture reaction for these samples. It makes more sense to use option 1 if this is the first sequencing data using this library. Use option 2 if this library have been repooled at a higher volume already, but still producing poor data.\n",
    "```python\n",
    "barcode_count_threshold=100 # samples below total barcode count of this value is considered low coverage\n",
    "low_coverage_action='Repool' # what to do for low coverage samples (Repool or Recapture)\n",
    "```\n",
    "Sometimes a handful of samples show uneven coverage of loci, i.e. they have very good coverage of a handful of loci but poor coverage in others, which may point to a problem with the sample or the experiment in general. These samples are determined by comparing the subset of samples that reached the goal we set (completed samples) and those that have not. We look at the number of barcodes per probe for _completed_ samples and get 25th percentile (or other percentile as set) and assume that if a sample on average has this many barcodes per target, it should have reached the set goal. For example, if on average _completed_ samples, i.e. samples that cover 47 probes at 10 barcodes or more, have 10000 total barcodes, they would have ~200 (10000/47) barcodes per target covered. And if an _incomplete_ sample has 5000 total barcodes and only 10 targets covered, this value would be 500 for that sample and it would be flagged as **uneven coverage** in repooling document.\n",
    "```python\n",
    "assesment_key='targets_with_1_barcodes' # coverage key to compare \"complete\" and \"incomplete\" samples\n",
    "good_coverage_quantile=0.25 # percentile to set the threshold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "high_barcode_threshold = 10000\n",
    "target_coverage_count = None\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key = 'targets_with_10_barcodes'\n",
    "barcode_coverage_threshold = 10\n",
    "barcode_count_threshold = 100\n",
    "low_coverage_action = 'Recapture'\n",
    "assesment_key = 'targets_with_1_barcodes'\n",
    "good_coverage_quantile = 0.25\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "high_barcode_threshold = \n",
    "low_coverage_action = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "target_coverage_count = None\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key = 'targets_with_10_barcodes'\n",
    "barcode_coverage_threshold = 10\n",
    "barcode_count_threshold = 100\n",
    "assesment_key = 'targets_with_1_barcodes'\n",
    "good_coverage_quantile = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "meta = pd.read_csv(wdir + \"run_meta.csv\")\n",
    "data_summary = pd.merge(sample_summary, meta)\n",
    "mip.repool(wdir, \n",
    "           data_summary, \n",
    "           high_barcode_threshold, \n",
    "           target_coverage_count=target_coverage_count, \n",
    "           target_coverage_fraction=target_coverage_fraction, \n",
    "           target_coverage_key=target_coverage_key,\n",
    "           barcode_coverage_threshold=barcode_coverage_threshold,\n",
    "           barcode_count_threshold=barcode_count_threshold, \n",
    "           low_coverage_action=low_coverage_action,\n",
    "           assesment_key=assesment_key,\n",
    "           good_coverage_quantile=good_coverage_quantile,\n",
    "           output_file='repool.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the repool document\n",
    "Library to completion field in the repool document has the value (volume) of how much from a sample should be pooled for re-sequencing. These values are only rough estimates and care should be taken to make sure there will be enough material to sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "pd.read_csv(wdir + \"repool.csv\").head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "299px",
    "width": "248px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1114px",
    "left": "977px",
    "top": "163px",
    "width": "256.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
