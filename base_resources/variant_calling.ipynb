{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use code cells in this notebook\n",
    "If a code cell starts with \n",
    "```python\n",
    "# RUN\n",
    "```\n",
    "Run the cell by CTRL+Enter, or the Run button above.  \n",
    "\n",
    "If a code cell starts with\n",
    "```python\n",
    "# USER INPUT\n",
    "```\n",
    "User input is needed before running the cell. Usually there will be a cell preceding this which gives an example for the values to be provided.\n",
    "\n",
    "If a code cell starts with\n",
    "```python\n",
    "# OPTIONAL USER INPUT\n",
    "```\n",
    "User input is needed before running the cell. However, some defaults are provided, so make sure that either the settings will work for your run, or change them appropriately.\n",
    "\n",
    "If a cell starts with\n",
    "#### Example cell\n",
    "These cells are not code cells but examples of user inputs from the test data analysis for the actual code cell that follows it, informing the user about the formatting etc.\n",
    "\n",
    "**Important note on entering input:** When entering user input, please make sure you follow the formatting provided in the example cells. For example, when the parameter is text, make sure you have quotation marks around the parameters but when it is a number, do not enclose in quotes. If it is a list, then provide a list in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes reloading.\n",
      "functions reloading\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "import sys\n",
    "sys.path.append(\"/opt/src\")\n",
    "import mip_functions as mip\n",
    "import probe_summary_generator\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import allel\n",
    "wdir = \"/opt/analysis/\"\n",
    "data_dir = \"/opt/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "\n",
    "# provide the MIPWrangler output files\n",
    "# which must be located in the /opt/data directory within the container.\n",
    "# if more than one run is to be merged, provide all files\n",
    "info_file = \"allInfo.tsv.gz\"\n",
    "\n",
    "# sample sheets associated with each wrangler file,\n",
    "# in the same order as the wrangler files.\n",
    "sample_sheet = \"sample_sheet.tsv\"\n",
    "\n",
    "\n",
    "# No input below\n",
    "info_file = [data_dir + info_file]\n",
    "sample_sheet = [data_dir + sample_sheet]\n",
    "pd.read_table(sample_sheet[0]).groupby([\"sample_set\", \"probe_set\"]).first()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide the MIPWrangler output file\n",
    "# which must be located in the /opt/data directory within the container.\n",
    "info_file = \"\"\n",
    "\n",
    "# sample sheet associated with wrangler file,\n",
    "#you should only have one sample sheet (in cases of multiple sample sheets, merge them first)\n",
    "sample_sheet = \"\"\n",
    "\n",
    "# No input below\n",
    "info_file = [data_dir + info_file]\n",
    "sample_sheet=[data_dir+sample_sheet]\n",
    "pd.read_table(sample_sheet[0]).groupby([\"sample_set\", \"probe_set\"]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the unique sample_set, probe_set combinations in the sample sheets provided. Select which combinations should be used for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "Which sample sets and probe sets would you like to analyze? These are listed in your sample sheet under the \"sample_set\" and \"probe_set\" columns.  Enter a single sample_set and a single probe_set.\n",
    "\n",
    "If a sample was captured/sequenced with multiple probe sets at the same time, there might optionally be multiple comma delimited probe sets in the probe set column from the sample sheet (e.g. DR1,VAR4 if sequencing was performed on DR1 and VAR4 probe sets).  You only need to enter the probe set you are interested in analyzing here.\n",
    "\n",
    "```python\n",
    "sample_set = \"PRX-00,PRX-04,PRX-07\"\n",
    "probe_set = \"DR23K\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "sample_set = \"\"\n",
    "probe_set = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the species\n",
    "For the species, the options are: \"pf\" for *Plasmodium falciparum*, \"pv\" for *Plasmodium vivax*, \"hg19\" for *Homo sapiens* genome assembly hg19/GRCh37 and \"hg38\" for *Homo sapiens* genome assembly hg38/GRCh38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "species = \"pf\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "species = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# available cpu count\n",
    "processorNumber = 20\n",
    "freebayes_threads = 8\n",
    "\n",
    "## extra bwa options for haplotype alignment\n",
    "# use \"-a\" for getting all alignments\n",
    "# use \"-L 500\" to penalize soft clipping \n",
    "# use \"-t\" to set number of available processors\n",
    "bwaExtra = [\"-t\", str(processorNumber)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "# available cpu count\n",
    "processorNumber = 20\n",
    "freebayes_threads = 8\n",
    "\n",
    "## extra bwa options for haplotype alignment\n",
    "# use \"-a\" for getting all alignments\n",
    "# use \"-L 500\" to penalize soft clipping \n",
    "# use \"-t\" to set number of available processors\n",
    "bwaExtra = [\"-t\", str(processorNumber)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get/Set the analysis settings\n",
    "Use the settings template for the species specified to get the  analysis settings and change the vaules specified in the above cell. This will create a template_settings.txt file in your analysis directory and a settings.txt file to be used for the analysis. These files also will serve as a reference of analysis settings for the sake of reproducibility.  \n",
    "\n",
    "The last step of the below cell attempts to save a file to the /opt/project_resources directory. If you do not have write permission to the location, you cannot save that file. However, if a file has been previously saved in the directory, it will be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# extract the settings template\n",
    "settings = mip.get_analysis_settings(\"/opt/resources/templates/analysis_settings_templates/settings.txt\")\n",
    "\n",
    "# update bwa settings with the options set above\n",
    "bwaOptions = [settings[\"bwaOptions\"]]\n",
    "bwaOptions.extend(bwaExtra)\n",
    "\n",
    "# create a dictionary for which settings should be updated\n",
    "# using the user specified parameters.\n",
    "\n",
    "settings['processorNumber']=processorNumber\n",
    "settings['freebayes_threads']=freebayes_threads\n",
    "settings['bwaOptions']=bwaOptions\n",
    "settings['species']=species\n",
    "settings['mipSetKey']=[probe_set.strip(), \"\"]\n",
    "# create a settings file in the analysis directory.\n",
    "settings_file='settings.txt'\n",
    "settings_path = os.path.join(wdir, settings_file)\n",
    "mip.write_analysis_settings(settings, settings_path)\n",
    "#reparse settings from settings file\n",
    "settings = mip.get_analysis_settings(wdir + settings_file)\n",
    "print(settings['mipSetKey'])\n",
    "# create probe sets dictionary\n",
    "try:\n",
    "    mip.update_probe_sets(\"/opt/project_resources/mip_ids/mipsets.csv\",\n",
    "                         \"/opt/project_resources/mip_ids/probe_sets.json\")\n",
    "except IOError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process run data\n",
    "First section of the data analysis involves processing the MIPWrangler output file, mapping haplotypes, and creating summary files and plots showing how the sequencing runs went."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIPWrangler output file processing\n",
    "Libraries are labeled by combining three fields in the sample sheet: sample_name-sample_set-replicate, which makes the Sample ID.\n",
    "\n",
    "The below operation just filters and renames some columns from the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "mip.process_info_file(wdir,\n",
    "                      settings_file, \n",
    "                      info_file,\n",
    "                      sample_sheet,\n",
    "                      settings[\"mipsterFile\"],\n",
    "                      sample_set.strip(),\n",
    "                      probe_set.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and map haplotype sequences\n",
    "Align each haplotype sequence to the reference genome. Remove off target haplotypes. All haplotype mappings will be saved to the disk so off targets can be inspected if needed. \n",
    "\n",
    "Some filters can be applied to remove noise and speed up processing:\n",
    "*  minHaplotypeBarcodes: minimum total UMI cut off across all samples.\n",
    "*  minHaplotypeSamples: minimum number of samples a haplotype is observed in.\n",
    "*  minHaplotypeSampleFraction: minimum fraction of samples a haplotype is observed in.  \n",
    "\n",
    "It is usually better to not filter at this step (using the difault filtering levels below of filtering nothing) unless the downstream operations are difficult to compute. However, filters can and should be applied after variant calls are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# filter haplotype sequences based on the number of total supporting UMIs\n",
    "settings[\"minHaplotypeBarcodes\"] = 1\n",
    "# filter haplotype sequences based on the number of samples they were observed in\n",
    "settings[\"minHaplotypeSamples\"] = 1\n",
    "# filter haplotype sequences based on the fraction of samples they were observed in\n",
    "settings[\"minHaplotypeSampleFraction\"] = 0.0001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "# filter haplotype sequences based on the number of total supporting UMIs\n",
    "settings[\"minHaplotypeBarcodes\"] = 1\n",
    "# filter haplotype sequences based on the number of samples they were observed in\n",
    "settings[\"minHaplotypeSamples\"] = 1\n",
    "# filter haplotype sequences based on the fraction of samples they were observed in\n",
    "settings[\"minHaplotypeSampleFraction\"] = 0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN\n",
    "mip.map_haplotypes(settings)\n",
    "mip.get_haplotype_counts(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the mapping results\n",
    "Plotting the probe coverage by samples is a good  way to see overall experiment perfomance. The chart below shows a heatmap of how many UMIs are present and uses a log scale\n",
    "\n",
    "Dark columns point to poor performing probes whereas dark rows indicate poor samples. Note that this excludes samples with no reads at all. Data is pulled from the file \"UMI_counts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "# alternate version of the chart above\n",
    "graphing_list, rows=[],[]\n",
    "for line_number, line in enumerate(open(\"UMI_counts.csv\")):\n",
    "\tline=line.strip().split(',')\n",
    "\tif line_number==0:\n",
    "\t\tcolumns=line[1:]\n",
    "\tif line_number>2:\n",
    "\t\trows.append(line[0])\n",
    "\t\tint_line=list(map(int, list(map(float, line[1:]))))\n",
    "\t\tlog_line=[math.log(number+1, 2) for number in int_line]\n",
    "\t\tgraphing_list.append(log_line)\n",
    "fig = px.imshow(graphing_list, aspect='auto', labels=dict(x=\"mips\", y=\"samples\",\n",
    "color='log2 of umi_counts+1'), x=columns, y=rows)\n",
    "fig.update_xaxes(side=\"top\")\n",
    "#fig.update_layout(width=2000, height=4000, autosize=False)\n",
    "fig.update_layout(height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at summary stats \n",
    "There are summary statistics and meta data (if provided) we can use to determine if coverage is enough, whether further sequencing is necessary, and how to proceed if further sequencing will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "sample_summary = pd.read_csv(wdir + \"sample_summary.csv\")\n",
    "sample_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot total UMI count vs probe coverage\n",
    "A scatter plot of total UMI count vs number of probes covered at a certain UMI count is a good way to see the relationship between total coverage and probe coverage, which is useful in determining how to proceed to the next experiments or analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "fig = px.scatter(sample_summary, \n",
    "        x=\"UMI Count\",\n",
    "        y=\"targets_with_>=10_UMIs\",\n",
    "        height=700,\n",
    "        hover_name=\"Sample ID\",\n",
    "        title=\"UMI Count vs. Probe Coverage\",\n",
    "        hover_data=\"Read Count\"\n",
    "\t)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repooling capture reactions for further sequencing.\n",
    "### Factors to consider:\n",
    "1. What do you we want to accomplish? In most cases, we would like to get enough coverage for a number of probes for each sample. For example, the test data contains **50 probes** in total. Let's say it is sufficient if we had a coverage of **10** or more for each probe for a sample. Then, we would not want to sequence any more of that sample. \n",
    "```python\n",
    "target_coverage_count = 50\n",
    "target_coverage_key='targets_with_>=10_UMIs'\n",
    "```\n",
    "Alternatively, we can set a goal of a fraction of total probes to reach a certain coverage rather than an absolute number of probes. For 95% of the maximum number of probes observed (47 in this case): \n",
    "```python\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key='targets_with_>=10_UMIs'\n",
    "```\n",
    "Although we set our goal to 47 probes, it is likely that some sample will never reach that number regardless of how much we sequence, if there is a deletion in the region, for example. So it makes sense to set a total coverage threshold after which we don't expect more data. Looking at the plot above, it seems like after 1000 UMI counts, we would reach our goal for most samples. \n",
    "```python\n",
    "high_UMI_threshold = 10000\n",
    "```\n",
    "Another metric to use for determining if we want to sequence a sample more is the average read count per UMI count. This value indicates we have sequenced each unique molecular index in our sample so many times, so when the value is high, it is unlikely that we'd get more UMIs by sequencing the same library more. It makes more sense for a fresh MIP capture from these samples if more data is needed.\n",
    "```python\n",
    "UMI_coverage_threshold=10\n",
    "```\n",
    "Some samples perform very poorly for one reason or another. There are two options for these samples for repooling consideration: 1) Repool as much as we can for the next run, 2) Assuming there is a problem in the capture reaction, set up a new MIP capture reaction for these samples. It makes more sense to use option 1 if this is the first sequencing data using this library. Use option 2 if this library have been repooled at a higher volume already, but is still producing poor data.\n",
    "```python\n",
    "UMI_count_threshold=100 # samples below total UMI count of this value is considered low coverage\n",
    "low_coverage_action='Repool' # what to do for low coverage samples (Repool or Recapture)\n",
    "```\n",
    "Sometimes a handful of samples show uneven coverage of loci, i.e. they have very good coverage of a handful of loci but poor coverage in others, which may point to a problem with the sample or the experiment in general. These samples are determined by comparing the subset of samples that reached the goal we set (completed samples) and those that have not. We look at the number of UMIs per probe for _completed_ samples and get 25th percentile (or other percentile as set) and assume that if a sample on average has this many UMIs per target, it should have reached the set goal. For example, if on average _completed_ samples, i.e. samples that cover 47 probes at 10 UMIs or more, have 10000 total UMIs, they would have ~200 (10000/47) UMIs per target covered. And if an _incomplete_ sample has 5000 total UMIs and only 10 targets covered, this value would be 500 for that sample and it would be flagged as **uneven coverage** in the repooling document.\n",
    "```python\n",
    "assesment_key='targets_with_>=1_UMIs' # coverage key to compare \"complete\" and \"incomplete\" samples\n",
    "good_coverage_quantile=0.25 # percentile to set the threshold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "high_UMI_threshold = 10000\n",
    "target_coverage_count = None\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key = 'targets_with_>=10_UMIs'\n",
    "UMI_coverage_threshold = 10\n",
    "UMI_count_threshold = 100\n",
    "low_coverage_action = 'Recapture'\n",
    "assesment_key = 'targets_with_>=1_UMIs'\n",
    "good_coverage_quantile = 0.25\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "high_UMI_threshold = \n",
    "low_coverage_action = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "target_coverage_count = None\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key = 'targets_with_>=10_UMIs'\n",
    "UMI_coverage_threshold = 10\n",
    "UMI_count_threshold = 100\n",
    "assesment_key = 'targets_with_>=1_UMIs'\n",
    "good_coverage_quantile = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "meta = pd.read_csv(wdir + \"run_meta.csv\")\n",
    "data_summary = pd.merge(sample_summary, meta)\n",
    "mip.repool(wdir, \n",
    "           data_summary, \n",
    "           high_UMI_threshold, \n",
    "           target_coverage_count=target_coverage_count, \n",
    "           target_coverage_fraction=target_coverage_fraction, \n",
    "           target_coverage_key=target_coverage_key,\n",
    "           UMI_coverage_threshold=UMI_coverage_threshold,\n",
    "           UMI_count_threshold=UMI_count_threshold, \n",
    "           low_coverage_action=low_coverage_action,\n",
    "           assesment_key=assesment_key,\n",
    "           good_coverage_quantile=good_coverage_quantile,\n",
    "           output_file='repool.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the repool document\n",
    "Library to completion field in the repool document has the value (volume) of how much from a sample should be pooled for re-sequencing. These values are only rough estimates and care should be taken to make sure there will be enough material to sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "pd.read_csv(wdir + \"repool.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant Calling\n",
    "Second part of the analysis involves variant calling and variant analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options for freebayes wrapper\n",
    "```Python\n",
    "align = True # Default is set to true, fastq files and bam files per sample\n",
    "# will be created in 'fastq_dir' and 'bam_dir'. \n",
    "# it should be set to false if bam files are available.\n",
    "\n",
    "settings = settings # analysis settings dictionary created above.\n",
    "\n",
    "bam_files = None # default is to use all bam files within the bam_dir.\n",
    "# if specific files should be used, then they can be specified in a list.\n",
    "\n",
    "verbose = True # prints errors and warnings as well as saving to disk.\n",
    "# if set to false, it will print that there is an error which will\n",
    "# be saved to disk which should be inspected for details.\n",
    "\n",
    "targets_file = None # force calls on specific loci even if there is\n",
    "# no observations satisfying filter criteria. Useful in cases of targeted\n",
    "# mutations such as drug resistance mutations.\n",
    "# Usually a file at \"/opt/project_resources/targets.tsv\" would be present\n",
    "# if the project requires it. Then targets_file should be set to this path.\n",
    "\n",
    "# paths for input-output files with default values that can be left unchanged\n",
    "fastq_dir, bam_dir, vcf_file, settings_file, errors_file, warnings_file\n",
    "\n",
    "# additional options to pass to freebayes directly:\n",
    "options = [] # see below for suggestions and possibilities.\n",
    "```\n",
    "#### Additional options for freebayes caller. \n",
    "Most of the freebayes options are shown below in the **freebayes help** section at the bottom of this document. Some options are integrated into the python wrapper freebayes_call, but others should be added depending on the data type, species etc.\n",
    "\n",
    "integrated options:\n",
    "```bash\n",
    "    -r region\n",
    "            limit calls to a specific region. \n",
    "            This is done internally, splitting the results into contigs and processing each contig\n",
    "            separately (in parallel if multiple cpus are available).\n",
    "            Per-contig vcf files are concatenated at the end into a single file.\n",
    "    -@ targets.vcf\n",
    "            force calls on positions provided in the vcf file\n",
    "            a vcf file is generated if a tab separated file containing targets are provided.\n",
    "    -L --bam-list\n",
    "            a list of bam files to be used. By default, all bams in bams directory will be used.\n",
    "            A list of specific bams can be specified to freebayes_call as bam_files option.\n",
    "```\n",
    "options to consider adding for parasite sequencing:\n",
    "```bash\n",
    "    --pooled-continuous\n",
    "             This option does not make assumptions about the ploidy when making genotype calls.\n",
    "             It makes sense for a mixed ploidy sample such as parasite infected blood DNA.\n",
    "             variants are still called as diploid. \n",
    "    --min-alternate-fraction 0.01\n",
    "             since we assume a pooled continuous sample, it would be better to set a within\n",
    "             samlpe allele frequency threshold to remove noise. \n",
    "             this is likely not needed when dealing with a diploid sample because a frequency \n",
    "             of 0.01 will likely be considered noise for a diploid sample.\n",
    "    --min-alternate-count 2\n",
    "             number of reads supporting a variant to consider for genotype calls.\n",
    "             having this at at least 2 is good. It will be possible to process\n",
    "             variants with 1 reads in postprocessing steps if a specific variant\n",
    "             is observed at least in one sample at > 1 reads. So this removes the \n",
    "             variant from consideration if no sample has > 1 reads supporting it.\n",
    "    --min-alternate-total 10\n",
    "             total read support for a variant across samples.\n",
    "```\n",
    "options to consider for human sequences:\n",
    "```bash\n",
    "    --min-mapping-quality 0\n",
    "             default for this setting is 1. I do not think this is helping much in \n",
    "             addressing mapping issues. However, reads in copy number variant regions\n",
    "             may have 0 mapping quality. These would be worth to keep, but they\n",
    "             should be handled appropriately at postprocessing steps.\n",
    "    --min-alternate-count 2\n",
    "    --min-alternate-fraction 0.05 (default)\n",
    "    --min-alternate-total 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Example cell\n",
    "```python\n",
    "# provide freebayes options.\n",
    "# These will be directy passed to freebayes\n",
    "\n",
    "# example for plasmodium falciparum calls\n",
    "original_options = [\"--pooled-continuous\",\n",
    "           \"--min-alternate-fraction\", \"0.01\",\n",
    "           \"--min-alternate-count\", \"2\",\n",
    "           \"--haplotype-length\", \"3\",\n",
    "           \"--min-alternate-total\", \"10\",\n",
    "           \"--use-best-n-alleles\", \"70\",\n",
    "           \"--genotype-qualities\", \"--gvcf\",\n",
    "           \"--gvcf-dont-use-chunk\", \"true\"]\n",
    "\n",
    "# example for human genome calls with gvcf output\n",
    "original_options = [\"--haplotype-length\", \"-1\",\n",
    "           \"--use-best-n-alleles\", \"50\",\n",
    "           \"--genotype-qualities\", \"--gvcf\",\n",
    "           \"--gvcf-dont-use-chunk\", \"true\"]\n",
    "\n",
    "# example for human genome calls without gvcf output\n",
    "original_options = [\"--haplotype-length\", \"-1\",\n",
    "           \"--use-best-n-alleles\", \"50\",\n",
    "           \"--genotype-qualities\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide freebayes options.\n",
    "# These will be directy passed to freebayes\n",
    "original_options = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "align=True\n",
    "verbose=True\n",
    "# where to save generated fastq files\n",
    "fastq_dir=\"/opt/analysis/padded_fastqs\"\n",
    "# where to save generated bam files\n",
    "bam_dir=\"/opt/analysis/padded_bams\"\n",
    "# where to save the output vcf file\n",
    "vcf_file=\"/opt/analysis/variants.vcf.gz\"\n",
    "# where is the targeted variants file\n",
    "targets_file=\"/opt/project_resources/targets.tsv\"\n",
    "# where to save errors and warnings generated by freebayes\n",
    "errors_file=\"/opt/analysis/freebayes_errors.txt\"\n",
    "warnings_file=\"/opt/analysis/freebayes_warnings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# freebayes caller creates fastq files from the haplotype sequences\n",
    "# by default 20 bp flanking sequence from the reference genome is added\n",
    "# to ensure correct deletion calls when they are towards the ends.\n",
    "# This assumes the 20 bp flank is wild type, however the sequence\n",
    "# is given a quality of 1, which should help avoiding some issues.\n",
    "# If this is not desired, set the below parameter to 0\n",
    "fastq_padding = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import multiprocessing.pool\n",
    "import copy\n",
    "\n",
    "freebayes_command_dict, contig_vcf_gz_paths = mip.freebayes_call(\n",
    "        settings=settings,\n",
    "        options=copy.deepcopy(original_options),\n",
    "        align=align,\n",
    "        verbose=verbose,\n",
    "        fastq_dir=fastq_dir,\n",
    "        bam_dir=bam_dir,\n",
    "        vcf_file=vcf_file,\n",
    "        targets_file=targets_file,\n",
    "        bam_files=None,\n",
    "        errors_file=errors_file,\n",
    "        warnings_file=warnings_file,\n",
    "        fastq_padding=fastq_padding)\n",
    "freebayes_commands=list(freebayes_command_dict.values())\n",
    "pool = Pool(int(settings[\"freebayes_threads\"]))\n",
    "# run the freebayes worker program in parallel\n",
    "# create a results container for the return values from the worker function\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "pool.map_async(mip.freebayes_worker, freebayes_commands, callback=results.extend,\n",
    "                   error_callback=errors.extend)\n",
    "#print(results)\n",
    "pool.close()\n",
    "pool.join()\n",
    "#comment in these print statements if you get any errors for more details on which contigs failed to run in freebayes\n",
    "#print('\\n\\n\\n\\n\\n')\n",
    "#print(results, '\\n\\n\\n')\n",
    "#print(errors, '\\n\\n\\n')\n",
    "\n",
    "mip.concatenate_headers(settings=settings, wdir='/opt/analysis', freebayes_settings=original_options, vcf_paths=contig_vcf_gz_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Exit Point\n",
    "The above cell should create the vcf file **variants.vcf.gz** in the analysis directory (assuming the vcf_file parameter was not changed). You can use this file in any downstream pipeline that utilizes vcf files. The variants are called rather generously, i.e. even when there is a good chance that a called variant is not there, with the assumption that the vcf will be further processed using whatever metric is deemed suitable for the data set.  \n",
    "\n",
    "In addition, you should now have a **padded_fastqs** subdirectory in your analysis directory containing fastq files for each sample. These fastq files contain 1 read per UMI and they are stitched together and cleaned up using MIPWrangler. You should be able to use these files in any pipeline that accepts fastq inputs (virtually all bioinformatics pipelines).  \n",
    "\n",
    "Finally, there is a **padded_bams** folder containing bam files for each sample obtained by mapping the *padded fastqs* to the reference genome.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The next steps in this notebook are dealing with postprocessing the vcf file in the ways that we found useful so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Variant Calls\n",
    "Freebayes produces high quality vcf files with haplotype based variant calls. This is important for getting more accurate calls, especially for complex regions where SNVs may overlap with indels and there may be many possible alleles as opposed to a simple biallelic SNV call.   \n",
    "\n",
    "haplotype based variant example:  \n",
    "\n",
    "chr1  1000 AAA,AGC,TGC  \n",
    "\n",
    "However, it may be desired to \"decompose\" these complex variants for some applications. For example, if we are interested in knowing the prevalence of a specific drug resistance mutation, it would make sense to combine all variants containing this mutation even though they may be part of different haplotypes, and hence are represented in the vcf in different variants.  \n",
    "\n",
    "Decomposed variants:  \n",
    "\n",
    "chr1  1000 A T  \n",
    "chr1  1001 A G  \n",
    "chr1  1002 A C  \n",
    "\n",
    "vcf_to_tables function takes the vcf file generated by freebayes and generates allele count and coverage data in table form. It is possible to decompose and aggregate amino acid and/or nucleotide level variants. 3 files containing count data are generated: alternate_table.csv, reference_table.csv, coverage_table.csv, for alt allele, ref allele and coverage count values for each variant, respectively.\n",
    "\n",
    "It first separates the multiallelic calls to bi-allelic calls.\n",
    "\n",
    "#### annotate, default=True\n",
    "It then annotates variants using snpEff.\n",
    "\n",
    "#### geneid_to_genename, default=None\n",
    "Variant annotation provides a gene ID (e.g. PF3D7_0709000) but it does not provide common gene names (e.g. crt). If common names are used in target files, or they are desired in general, a tab separated gene ID to gene name file can be used. **gene_name and gene_id** columns are required. If no file is provided, gene name will be the same as the gene ID.\n",
    "\n",
    "#### aggregate_aminoacids, default=False\n",
    "If aminoacid level aggregation is requested, it decomposes multi amino acid missense variants into single components and aggregates the alternate allele and coverage counts per amino acid change. For example, Asn75Glu change for crt gene is a known drug resistance mutation in Plasmodium falciparum. There may be 3 separate variants in the vcf file that contain this mutation: Asn75Glu, MetAsn75IleGlu, Asn75Glu_del76-80*. All three has the missense variant Asn75Glu. While the first two  are simple changes, the third is a complex change including a 5 amino acid deletion and a stop codon following Asn75Glu. In this case, it makes sense to combine the counts of the first two variants towards Asn75Glu counts but the third one is debatable because of the complexity; i.e. the drug resistance mutation Asn75Glu probably is not that improtant in that context because of the stop codon following it. So we decompose the simple changes and aggregate but leave complex changes as they are. If aminoacid aggregation is carried out, file names will contain AA tag.\n",
    "\n",
    "#### target_aa_annotation, default=None\n",
    "It is also possible to annotate the targeted variants (such as Asn75Glu above) in the generated tables as 'Targeted' in case some analysis should be carried out on targeted variants only. A tab separated file containing the annotation details is required for this operation. **gene_name, aminoacid_change and mutation_name** are required fields. If a variants gene_name and aminoacid_change are matching to a row in the target file, that variant will be marked as targeted and will have the correspondign mutation name. Note that if common gene name conversion (see above) is not used, the gene_name column in this file must match the actual gene ID and not the common name. It may be more convenient to keep the gene IDs in the target file as well and use that file for ID to name mapping. **aggregate_aminoacids must be set to True** for this option to be used.\n",
    "\n",
    "#### aggregate_nucleotides, default=False\n",
    "A similar aggregation can be done at nucleotide level. If specified, biallelic variants will be decomposed using the tool **vt decompose_blocksub**. By default it decomposes block substitutions that do not include indels. However, it is also possible to decompose complex variants including indels by providing -a option. For possible decompose options see vt help:\n",
    "```bash\n",
    "vt decompose_blocksub options : \n",
    "  -p  Output phased genotypes and PS tags for decomposed variants [false]\n",
    "  -m  keep MNVs (multi-nucleotide variants) [false]\n",
    "  -a  enable aggressive/alignment mode [false]\n",
    "  -d  MNVs max distance (when -m option is used) [2]\n",
    "  -o  output VCF file [-]\n",
    "  -I  file containing list of intervals []\n",
    "  -i  intervals []\n",
    "  -?  displays help\n",
    "```\n",
    "If nucleotide level aggregation is done, the file names will include AN tag.\n",
    "\n",
    "#### target_nt_annotation, default=None\n",
    "Annotation of targeted nucleotides requires a file similar to the targeted amino acid annotation. However, the required fields for this annotation are: CHROM, POS, REF, ALT and mutation_name. **aggregate_nucleotides must be set to True** for this option to be used.\n",
    "\n",
    "#### aggregate_none, default=False\n",
    "It is also possible to generate count tables without doing any aggregation. This will generate the 3 count files, and all of the variant information included in the vcf file will be a separate column in the table's index. For annotated initial vcf files, or if annotate option is selected, each subfield in the INFO/ANN field will have its own column.\n",
    "\n",
    "#### min_site_qual, default=-1\n",
    "Filter variant sites for a minimum QUAL value assigned by the variant caller. This value is described in freebayes manual as:\n",
    "```bash\n",
    "Of primary interest to most users is the QUAL field, which estimates the probability that there is a polymorphism at the loci described by the record. In freebayes, this value can be understood as 1 - P(locus is homozygous given the data). It is recommended that users use this value to filter their results, rather than accepting anything output by freebayes as ground truth.\n",
    "\n",
    "By default, records are output even if they have very low probability of variation, in expectation that the VCF will be filtered using tools such as vcffilter in vcflib, which is also included in the repository under vcflib/. For instance,\n",
    "\n",
    "freebayes -f ref.fa aln.bam | vcffilter -f \"QUAL > 20\" >results.vcf\n",
    "\n",
    "removes any sites with estimated probability of not being polymorphic less than phred 20 (aka 0.01), or probability of polymorphism > 0.99.\n",
    "\n",
    "In simulation, the receiver-operator characteristic (ROC) tends to have a very sharp inflection between Q1 and Q30, depending on input data characteristics, and a filter setting in this range should provide decent performance. Users are encouraged to examine their output and both variants which are retained and those they filter out. Most problems tend to occur in low-depth areas, and so users may wish to remove these as well, which can also be done by filtering on the DP flag.\n",
    "```\n",
    "Therefore, a **minimum of 1** should be used as a min_site_qual to remove low quality sites. If a site is annotated as **targeted**, the site will be kept regardless of its qual value, however, the alternate observation counts for the site may be reset to zero depending on the min_target_site_qual value described below.\n",
    "\n",
    "#### min_target_site_qual, default=-1\n",
    "If a variant site is targeted but the site qual is lower than this,\n",
    "reset the alternate observation counts to 0. It may be best to leave\n",
    "this at the default value since there is usually additional evidence\n",
    "that a targeted variant exists in a samples compared to a de novo\n",
    "variant, i.e. those variants that are targeted had been observed in other samples/studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# provide a file that maps gene names to gene IDs\n",
    "# this is necessary when targeted variant annotations use\n",
    "# gene names instead of gene IDs\n",
    "geneid_to_genename = \"/opt/project_resources/geneid_to_genename.tsv\"\n",
    "# annotate targted amino acid changes in the tables.\n",
    "target_aa_annotation = \"/opt/project_resources/targets.tsv\"\n",
    "# decompose multi amino acid changes and combine counts of\n",
    "# resulting single amino acid changes\n",
    "aggregate_aminoacids = True\n",
    "# decompose MNVs and combine counts for resulting SNVs\n",
    "aggregate_nucleotides = True\n",
    "# annotate targeted nucleotide changes in the tables.\n",
    "target_nt_annotation = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide a file that maps gene names to gene IDs\n",
    "# this is necessary when targeted variant annotations use\n",
    "# gene names instead of gene IDs. Otherwise provide None\n",
    "geneid_to_genename = \n",
    "# annotate targeted amino acid changes in the tables\n",
    "# using the file, or otherwise provide None\n",
    "target_aa_annotation = \n",
    "# decompose multi amino acid changes and combine counts of\n",
    "# resulting single amino acid changes\n",
    "aggregate_aminoacids = \n",
    "# decompose MNVs and combine counts for resulting SNVs\n",
    "aggregate_nucleotides = \n",
    "# annotate targeted nucleotide changes in the tables.\n",
    "target_nt_annotation = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# analysis settings dictionary\n",
    "settings = settings\n",
    "# provide the path to the settings file\n",
    "# if settings dictionary has not been loaded\n",
    "settings_file = None\n",
    "# use snpEff to annotate the variants\n",
    "annotate = True\n",
    "# additional vt options for decomposing nucleotides.\n",
    "# Supply [\"-a\"] to include indels and complex variants\n",
    "# in decomposition, or other options shown above if desired.\n",
    "decompose_options = []\n",
    "# was the initial vcf file was annotated by snpEff?\n",
    "annotated_vcf = False\n",
    "# create tables for variants as they are in the vcf file\n",
    "# without decomposing compex variants or indels.\n",
    "# Multiallelic variants will be split into biallelic.\n",
    "aggregate_none = True\n",
    "# filter variant sites for quality\n",
    "min_site_qual = 1\n",
    "# reset targeted variant counts to zero\n",
    "# when the site quality is below this value\n",
    "min_target_site_qual = -1\n",
    "# reset genotypes in the vcf file to NA\n",
    "# and depth to 0 if FORMAT/GQ value for a variant/sample\n",
    "# is below this value:\n",
    "min_genotype_qual = -1\n",
    "# reset alt allele count in the vcf file to 0\n",
    "# if FORMAT/QA value divided by FORMAT/AO for a variant/sample\n",
    "# is below this value:\n",
    "min_mean_alt_qual = -1 # average quality cut off for variants\n",
    "# There are also available, similar filters for:\n",
    "# min_mean_ref_qual : resetting low qual reference allele counts\n",
    "# min_alt_qual : similar to min_mean_alt_qual, but for total qual score\n",
    "# min_ref_qual : similar to min_alt_qual but for reference alleles\n",
    "\n",
    "# prefix for output files, if desired.\n",
    "# this is useful when different quality thresholds etc will be used\n",
    "# to avoid overwriting the files. For example, if min_genotype_qual = 1\n",
    "# and min_mean_alt_qual = 15 is used, a suitable prefix could be\n",
    "# \"gq1.mqa15.\"\n",
    "output_prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# input vcf file\n",
    "vcf_file = vcf_file.split(\"/\")[-1]\n",
    "mip.vcf_to_tables_fb(\n",
    "     vcf_file,\n",
    "     settings=settings,\n",
    "     settings_file=settings_file,\n",
    "     annotate=annotate,\n",
    "     geneid_to_genename=geneid_to_genename,\n",
    "     target_aa_annotation=target_aa_annotation,\n",
    "     aggregate_aminoacids=aggregate_aminoacids,\n",
    "     target_nt_annotation=target_nt_annotation, \n",
    "     aggregate_nucleotides=aggregate_nucleotides, \n",
    "     decompose_options=decompose_options,\n",
    "     annotated_vcf=annotated_vcf,\n",
    "     aggregate_none=aggregate_none,\n",
    "     min_site_qual=min_site_qual,\n",
    "     min_target_site_qual=min_target_site_qual,\n",
    "     min_genotype_qual=min_genotype_qual,\n",
    "     min_mean_alt_qual=min_mean_alt_qual,\n",
    "     output_prefix=output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables created\n",
    "alternate_XX_table.csv files will contain the ALT allele count for that table type while coverage_XX_table.csv will contain the depth of coverage at each locus.\n",
    "### Nucleotide changes (aggregated)\n",
    "For some projects we may be interested in specific single nucleotide changes. For these, it would make sense to decompose multi nucleotide changes and combine counts of the same single nucleotide changes. Two tables will be generated for count and coverage data for aggregated nucleotide changes:  \n",
    "\n",
    "**alternate_AN_table.csv** file in the analysis directory is created if aggregate_nucleotides option was selected when creating data tables. This table has the UMI counts for each alternate nucleotide.  \n",
    "\n",
    "**coverage_AN_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_AN_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1. When calls from multiple variants are aggregated; if all 0/0 then -> 0, if any 0/0 and non-0/0 then -> 1, if all 1/1 then -> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino acid changes (aggregated)\n",
    "For some projects we may be interested in the amino acid changes, particularly specific, targeted amino acid changes, such as drug resistance mutations in *Plasmodium falciparum*, which is the data set provided for pipeline test. For these type of projects, we may want to analyze the variants from the amino acid perspective, rather than nucleotide changes which is standard output for variant callers.  \n",
    "\n",
    "**alternate_AA_table.csv** file in the analysis directory is created if aggregate_aminoacids option was selected when creating data tables. This table has the UMI counts for each alternate amino acid.  \n",
    "\n",
    "**coverage_AA_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_AA_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1. When calls from multiple variants are aggregated; if all 0/0 then -> 0, if any 0/0 and non-0/0 then -> 1, if all 1/1 then -> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleotide changes (not aggregated)\n",
    "For some projects we may be interested in keeping composite variants as they are called by the pipeline. These will include MNVs, comlplex variants including indels, etc. Two tables will be generated for count and coverage data for original nucleotide changes:  \n",
    "\n",
    "**alternate_table.csv** file in the analysis directory is created if aggregate_none option was selected when creating data tables. This table has the UMI counts for each alternate nucleotide.  \n",
    "\n",
    "**coverage_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "299px",
    "width": "248px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1114px",
    "left": "977px",
    "top": "163px",
    "width": "256.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
