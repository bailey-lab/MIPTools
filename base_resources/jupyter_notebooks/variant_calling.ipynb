{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook:\n",
    "This notebook converts microhaplotypes to VCF files and variant tables (counts of how many UMIs were associated with\n",
    "each mutation in each sample). This notebook uses a tool called freebayes to convert microhaplotypes into individual\n",
    "variants, and SNPEff to assign amino acid numbers and names to mutations that fall within protein coding genes.\n",
    "\n",
    "This notebook is meant to be run after completing the wrangler_QC_repool_stats notebook.\n",
    "\n",
    "**WARNING**: If you run this notebook without completing the wrangler_QC_repool_stats notebook you will get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use code cells in this notebook\n",
    "If a code cell starts with # RUN, Run the cell by CTRL+Enter, or the Run button above. If a code cell starts with # USER INPUT\n",
    "User input is needed before running the cell. If a code cell starts with # OPTIONAL USER INPUT, the cell needs to be run but default values are pre-filled, with the option for the user to change them if needed.\n",
    "\n",
    "**Important note on entering input:** When entering user input, please make sure you follow the formatting provided in the comments and default values (ignoring the # that denotes the comment). For example, if there are quotes, brackets, commas, or spaces in the examples, make sure your values follow these conventions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "import sys\n",
    "sys.path.append(\"/opt/src\")\n",
    "import mip_functions as mip\n",
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "wdir = \"/opt/user/stats_and_variant_calling/\"\n",
    "data_dir = \"/opt/user/wrangled_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "# set the number of processors to use to parallelize non-freebayes portions of this notebook. You can set this number\n",
    "# high and it will help the job complete faster with relatively low likelihood of crashing your machine.\n",
    "processorNumber = 20\n",
    "\n",
    "# set the number of processors to use to parallelize freebayes - freebayes is very memory intensive. Set this number\n",
    "# high enough to take advantage of the number of processors in your machine, but low enough to avoid running out of\n",
    "# the RAM available on your machine and crashing the run. The \"correct\" number will depend on your machine stats\n",
    "# and the size of your dataset, but I like to set this number as low as I can afford to go without the run taking\n",
    "# forever - out of memory crashes are very frequent when this number is set too high with large datasets.\n",
    "freebayes_threads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# extract the settings from the previous jupyter notebook\n",
    "settings_file='settings.txt'\n",
    "settings = mip.get_analysis_settings(wdir + settings_file)\n",
    "settings['processorNumber']=processorNumber\n",
    "settings['freebayes_threads']=freebayes_threads\n",
    "settings_path = os.path.join(wdir, settings_file)\n",
    "mip.write_analysis_settings(settings, settings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options for freebayes wrapper\n",
    "```Python\n",
    "align = True # Default is set to true, fastq files and bam files per sample\n",
    "# will be created in 'fastq_dir' and 'bam_dir'. \n",
    "# it should be set to false if bam files are available.\n",
    "\n",
    "settings = settings # analysis settings dictionary created above.\n",
    "\n",
    "bam_files = None # default is to use all bam files within the bam_dir.\n",
    "# if specific files should be used, then they can be specified in a list.\n",
    "\n",
    "verbose = True # prints errors and warnings as well as saving to disk.\n",
    "# if set to false, it will print that there is an error which will\n",
    "# be saved to disk which should be inspected for details.\n",
    "\n",
    "targets_file = None # force calls on specific loci even if there is\n",
    "# no observations satisfying filter criteria. Useful in cases of targeted\n",
    "# mutations such as drug resistance mutations.\n",
    "# Usually a file at \"/opt/project_resources/targets.tsv\" would be present\n",
    "# if the project requires it. Then targets_file should be set to this path.\n",
    "\n",
    "# paths for input-output files with default values that can be left unchanged\n",
    "fastq_dir, bam_dir, vcf_file, settings_file, errors_file, warnings_file\n",
    "\n",
    "# additional options to pass to freebayes directly:\n",
    "options = [] # see below for suggestions and possibilities.\n",
    "```\n",
    "#### Additional options for freebayes caller. \n",
    "Most of the freebayes options are shown below in the **freebayes help** section at the bottom of this document. Some options are integrated into the python wrapper freebayes_call, but others should be added depending on the data type, species etc.\n",
    "\n",
    "integrated options:\n",
    "```bash\n",
    "    -r region\n",
    "            limit calls to a specific region. \n",
    "            This is done internally, splitting the results into contigs and processing each contig\n",
    "            separately (in parallel if multiple cpus are available).\n",
    "            Per-contig vcf files are concatenated at the end into a single file.\n",
    "    -@ targets.vcf\n",
    "            force calls on positions provided in the vcf file\n",
    "            a vcf file is generated if a tab separated file containing targets are provided.\n",
    "    -L --bam-list\n",
    "            a list of bam files to be used. By default, all bams in bams directory will be used.\n",
    "            A list of specific bams can be specified to freebayes_call as bam_files option.\n",
    "```\n",
    "options to consider adding for parasite sequencing:\n",
    "```bash\n",
    "    --pooled-continuous\n",
    "             This option does not make assumptions about the ploidy when making genotype calls.\n",
    "             It makes sense for a mixed ploidy sample such as parasite infected blood DNA.\n",
    "             variants are still called as diploid. \n",
    "    --min-alternate-fraction 0.01\n",
    "             since we assume a pooled continuous sample, it would be better to set a within\n",
    "             samlpe allele frequency threshold to remove noise. \n",
    "             this is likely not needed when dealing with a diploid sample because a frequency \n",
    "             of 0.01 will likely be considered noise for a diploid sample.\n",
    "    --min-alternate-count 2\n",
    "             number of reads supporting a variant to consider for genotype calls.\n",
    "             having this at at least 2 is good. It will be possible to process\n",
    "             variants with 1 reads in postprocessing steps if a specific variant\n",
    "             is observed at least in one sample at > 1 reads. So this removes the \n",
    "             variant from consideration if no sample has > 1 reads supporting it.\n",
    "    --min-alternate-total 10\n",
    "             total read support for a variant across samples.\n",
    "```\n",
    "options to consider for human sequences:\n",
    "```bash\n",
    "    --min-mapping-quality 0\n",
    "             default for this setting is 1. I do not think this is helping much in \n",
    "             addressing mapping issues. However, reads in copy number variant regions\n",
    "             may have 0 mapping quality. These would be worth to keep, but they\n",
    "             should be handled appropriately at postprocessing steps.\n",
    "    --min-alternate-count 2\n",
    "    --min-alternate-fraction 0.05 (default)\n",
    "    --min-alternate-total 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Example cell\n",
    "```python\n",
    "# provide freebayes options.\n",
    "# These will be directy passed to freebayes\n",
    "\n",
    "# example for plasmodium falciparum calls\n",
    "original_options = [\"--pooled-continuous\",\n",
    "           \"--min-alternate-fraction\", \"0.01\",\n",
    "           \"--min-alternate-count\", \"2\",\n",
    "           \"--haplotype-length\", \"3\",\n",
    "           \"--min-alternate-total\", \"10\",\n",
    "           \"--use-best-n-alleles\", \"70\",\n",
    "           \"--genotype-qualities\", \"--gvcf\",\n",
    "           \"--gvcf-dont-use-chunk\", \"true\"]\n",
    "\n",
    "# example for human genome calls with gvcf output\n",
    "original_options = [\"--haplotype-length\", \"-1\",\n",
    "           \"--use-best-n-alleles\", \"50\",\n",
    "           \"--genotype-qualities\", \"--gvcf\",\n",
    "           \"--gvcf-dont-use-chunk\", \"true\"]\n",
    "\n",
    "# example for human genome calls without gvcf output\n",
    "original_options = [\"--haplotype-length\", \"-1\",\n",
    "           \"--use-best-n-alleles\", \"50\",\n",
    "           \"--genotype-qualities\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide freebayes options.\n",
    "# These will be directy passed to freebayes\n",
    "original_options = [\"--pooled-continuous\",\n",
    "           \"--min-alternate-fraction\", \"0.01\",\n",
    "           \"--min-alternate-count\", \"2\",\n",
    "           \"--haplotype-length\", \"3\",\n",
    "           \"--min-alternate-total\", \"10\",\n",
    "           \"--use-best-n-alleles\", \"70\",\n",
    "           \"--genotype-qualities\", \"--gvcf\",\n",
    "           \"--gvcf-dont-use-chunk\", \"true\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "align=True\n",
    "verbose=True\n",
    "# where to save generated fastq files\n",
    "fastq_dir=\"/opt/user/stats_and_variant_calling/padded_fastqs\"\n",
    "# where to save generated bam files\n",
    "bam_dir=\"/opt/user/stats_and_variant_calling/padded_bams\"\n",
    "# where to save the output vcf file\n",
    "vcf_file=\"/opt/user/stats_and_variant_calling/variants.vcf.gz\"\n",
    "# where is the targeted variants file\n",
    "targets_file=\"/opt/project_resources/targets.tsv\"\n",
    "# where to save errors and warnings generated by freebayes\n",
    "errors_file=\"/opt/user/stats_and_variant_calling/freebayes_errors.txt\"\n",
    "warnings_file=\"/opt/user/stats_and_variant_calling/freebayes_warnings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# freebayes caller creates fastq files from the haplotype sequences\n",
    "# by default 20 bp flanking sequence from the reference genome is added\n",
    "# to ensure correct deletion calls when they are towards the ends.\n",
    "# This assumes the 20 bp flank is wild type, however the sequence\n",
    "# is given a quality of 1, which should help avoiding some issues.\n",
    "# If this is not desired, set the below parameter to 0\n",
    "fastq_padding = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking the headers and starting positions of 7 files\n",
      "Concatenating /opt/user/stats_and_variant_calling/contig_vcfs/chr13_0.vcf.gz\t0.142431 seconds\n",
      "\t0.069318 secondst/user/stats_and_variant_calling/contig_vcfs/chr14_0.vcf.gz\n",
      "Concatenating /opt/user/stats_and_variant_calling/contig_vcfs/chr14_1.vcf.gz\t0.060304 seconds\n",
      "Concatenating /opt/user/stats_and_variant_calling/contig_vcfs/chr4_0.vcf.gz\t0.069989 seconds\n",
      "\t0.155633 secondst/user/stats_and_variant_calling/contig_vcfs/chr5_0.vcf.gz\n",
      "Concatenating /opt/user/stats_and_variant_calling/contig_vcfs/chr7_0.vcf.gz\t0.127667 seconds\n",
      "\t0.089557 secondst/user/stats_and_variant_calling/contig_vcfs/chr8_0.vcf.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did a reheader\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import multiprocessing.pool\n",
    "import copy\n",
    "import gzip\n",
    "\n",
    "freebayes_command_dict, contig_vcf_gz_paths = mip.freebayes_call(\n",
    "        settings=settings,\n",
    "        options=copy.deepcopy(original_options),\n",
    "        align=align,\n",
    "        verbose=verbose,\n",
    "        fastq_dir=fastq_dir,\n",
    "        bam_dir=bam_dir,\n",
    "        vcf_file=vcf_file,\n",
    "        targets_file=targets_file,\n",
    "        bam_files=None,\n",
    "        errors_file=errors_file,\n",
    "        warnings_file=warnings_file,\n",
    "        fastq_padding=fastq_padding)\n",
    "freebayes_commands=list(freebayes_command_dict.values())\n",
    "pool = Pool(int(settings[\"freebayes_threads\"]))\n",
    "# run the freebayes worker program in parallel\n",
    "# create a results container for the return values from the worker function\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "pool.map_async(mip.freebayes_worker, freebayes_commands, callback=results.extend,\n",
    "                   error_callback=errors.extend)\n",
    "#print(results)\n",
    "pool.close()\n",
    "pool.join()\n",
    "#comment in these print statements if you get any errors for more details on which contigs failed to run in freebayes\n",
    "#print('\\n\\n\\n\\n\\n')\n",
    "#print(results, '\\n\\n\\n')\n",
    "#print(errors, '\\n\\n\\n')\n",
    "\n",
    "mip.concatenate_headers(settings=settings, wdir='/opt/user/stats_and_variant_calling', freebayes_settings=original_options, vcf_paths=contig_vcf_gz_paths)\n",
    "\n",
    "file_in = gzip.open(vcf_file, 'rt')\n",
    "file_out = gzip.open(vcf_file.replace('.vcf.gz','_mutations_only.vcf.gz'),'wt')\n",
    "for line in file_in:\n",
    "    if \"<*>\" not in line:\n",
    "        file_out.write(line)\n",
    "file_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Exit Point\n",
    "The above cell should create the vcf file **variants.vcf.gz** in the analysis directory (assuming the vcf_file parameter was not changed). You can use this file in any downstream pipeline that utilizes vcf files. The variants are called rather generously, i.e. even when there is a good chance that a called variant is not there, with the assumption that the vcf will be further processed using whatever metric is deemed suitable for the data set.  \n",
    "\n",
    "In addition, you should now have a **padded_fastqs** subdirectory in your analysis directory containing fastq files for each sample. These fastq files contain 1 read per UMI and they are stitched together and cleaned up using MIPWrangler. You should be able to use these files in any pipeline that accepts fastq inputs (virtually all bioinformatics pipelines).  \n",
    "\n",
    "Finally, there is a **padded_bams** folder containing bam files for each sample obtained by mapping the *padded fastqs* to the reference genome.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The next steps in this notebook are dealing with postprocessing the vcf file in the ways that we found useful so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Variant Calls\n",
    "Freebayes produces high quality vcf files with haplotype based variant calls. This is important for getting more accurate calls, especially for complex regions where SNVs may overlap with indels and there may be many possible alleles as opposed to a simple biallelic SNV call.   \n",
    "\n",
    "haplotype based variant example:  \n",
    "\n",
    "chr1  1000 AAA,AGC,TGC  \n",
    "\n",
    "However, it may be desired to \"decompose\" these complex variants for some applications. For example, if we are interested in knowing the prevalence of a specific drug resistance mutation, it would make sense to combine all variants containing this mutation even though they may be part of different haplotypes, and hence are represented in the vcf in different variants.  \n",
    "\n",
    "Decomposed variants:  \n",
    "\n",
    "chr1  1000 A T  \n",
    "chr1  1001 A G  \n",
    "chr1  1002 A C  \n",
    "\n",
    "vcf_to_tables function takes the vcf file generated by freebayes and generates allele count and coverage data in table form. It is possible to decompose and aggregate amino acid and/or nucleotide level variants. 3 files containing count data are generated: alternate_table.csv, reference_table.csv, coverage_table.csv, for alt allele, ref allele and coverage count values for each variant, respectively.\n",
    "\n",
    "It first separates the multiallelic calls to bi-allelic calls.\n",
    "\n",
    "#### annotate, default=True\n",
    "It then annotates variants using snpEff.\n",
    "\n",
    "#### geneid_to_genename, default=None\n",
    "Variant annotation provides a gene ID (e.g. PF3D7_0709000) but it does not provide common gene names (e.g. crt). If common names are used in target files, or they are desired in general, a tab separated gene ID to gene name file can be used. **gene_name and gene_id** columns are required. If no file is provided, gene name will be the same as the gene ID.\n",
    "\n",
    "#### aggregate_aminoacids, default=False\n",
    "If aminoacid level aggregation is requested, it decomposes multi amino acid missense variants into single components and aggregates the alternate allele and coverage counts per amino acid change. For example, Asn75Glu change for crt gene is a known drug resistance mutation in Plasmodium falciparum. There may be 3 separate variants in the vcf file that contain this mutation: Asn75Glu, MetAsn75IleGlu, Asn75Glu_del76-80*. All three has the missense variant Asn75Glu. While the first two  are simple changes, the third is a complex change including a 5 amino acid deletion and a stop codon following Asn75Glu. In this case, it makes sense to combine the counts of the first two variants towards Asn75Glu counts but the third one is debatable because of the complexity; i.e. the drug resistance mutation Asn75Glu probably is not that improtant in that context because of the stop codon following it. So we decompose the simple changes and aggregate but leave complex changes as they are. If aminoacid aggregation is carried out, file names will contain AA tag.\n",
    "\n",
    "#### target_aa_annotation, default=None\n",
    "It is also possible to annotate the targeted variants (such as Asn75Glu above) in the generated tables as 'Targeted' in case some analysis should be carried out on targeted variants only. A tab separated file containing the annotation details is required for this operation. **gene_name, aminoacid_change and mutation_name** are required fields. If a variants gene_name and aminoacid_change are matching to a row in the target file, that variant will be marked as targeted and will have the correspondign mutation name. Note that if common gene name conversion (see above) is not used, the gene_name column in this file must match the actual gene ID and not the common name. It may be more convenient to keep the gene IDs in the target file as well and use that file for ID to name mapping. **aggregate_aminoacids must be set to True** for this option to be used.\n",
    "\n",
    "#### aggregate_nucleotides, default=False\n",
    "A similar aggregation can be done at nucleotide level. If specified, biallelic variants will be decomposed using the tool **vt decompose_blocksub**. By default it decomposes block substitutions that do not include indels. However, it is also possible to decompose complex variants including indels by providing -a option. For possible decompose options see vt help:\n",
    "```bash\n",
    "vt decompose_blocksub options : \n",
    "  -p  Output phased genotypes and PS tags for decomposed variants [false]\n",
    "  -m  keep MNVs (multi-nucleotide variants) [false]\n",
    "  -a  enable aggressive/alignment mode [false]\n",
    "  -d  MNVs max distance (when -m option is used) [2]\n",
    "  -o  output VCF file [-]\n",
    "  -I  file containing list of intervals []\n",
    "  -i  intervals []\n",
    "  -?  displays help\n",
    "```\n",
    "If nucleotide level aggregation is done, the file names will include AN tag.\n",
    "\n",
    "#### target_nt_annotation, default=None\n",
    "Annotation of targeted nucleotides requires a file similar to the targeted amino acid annotation. However, the required fields for this annotation are: CHROM, POS, REF, ALT and mutation_name. **aggregate_nucleotides must be set to True** for this option to be used.\n",
    "\n",
    "#### aggregate_none, default=False\n",
    "It is also possible to generate count tables without doing any aggregation. This will generate the 3 count files, and all of the variant information included in the vcf file will be a separate column in the table's index. For annotated initial vcf files, or if annotate option is selected, each subfield in the INFO/ANN field will have its own column.\n",
    "\n",
    "#### min_site_qual, default=-1\n",
    "Filter variant sites for a minimum QUAL value assigned by the variant caller. This value is described in freebayes manual as:\n",
    "```bash\n",
    "Of primary interest to most users is the QUAL field, which estimates the probability that there is a polymorphism at the loci described by the record. In freebayes, this value can be understood as 1 - P(locus is homozygous given the data). It is recommended that users use this value to filter their results, rather than accepting anything output by freebayes as ground truth.\n",
    "\n",
    "By default, records are output even if they have very low probability of variation, in expectation that the VCF will be filtered using tools such as vcffilter in vcflib, which is also included in the repository under vcflib/. For instance,\n",
    "\n",
    "freebayes -f ref.fa aln.bam | vcffilter -f \"QUAL > 20\" >results.vcf\n",
    "\n",
    "removes any sites with estimated probability of not being polymorphic less than phred 20 (aka 0.01), or probability of polymorphism > 0.99.\n",
    "\n",
    "In simulation, the receiver-operator characteristic (ROC) tends to have a very sharp inflection between Q1 and Q30, depending on input data characteristics, and a filter setting in this range should provide decent performance. Users are encouraged to examine their output and both variants which are retained and those they filter out. Most problems tend to occur in low-depth areas, and so users may wish to remove these as well, which can also be done by filtering on the DP flag.\n",
    "```\n",
    "Therefore, a **minimum of 1** should be used as a min_site_qual to remove low quality sites. If a site is annotated as **targeted**, the site will be kept regardless of its qual value, however, the alternate observation counts for the site may be reset to zero depending on the min_target_site_qual value described below.\n",
    "\n",
    "#### min_target_site_qual, default=-1\n",
    "If a variant site is targeted but the site qual is lower than this,\n",
    "reset the alternate observation counts to 0. It may be best to leave\n",
    "this at the default value since there is usually additional evidence\n",
    "that a targeted variant exists in a samples compared to a de novo\n",
    "variant, i.e. those variants that are targeted had been observed in other samples/studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# provide a file that maps gene names to gene IDs\n",
    "# this is necessary when targeted variant annotations use\n",
    "# gene names instead of gene IDs\n",
    "geneid_to_genename = \"/opt/project_resources/geneid_to_genename.tsv\"\n",
    "# annotate targted amino acid changes in the tables.\n",
    "target_aa_annotation = \"/opt/project_resources/targets.tsv\"\n",
    "# decompose multi amino acid changes and combine counts of\n",
    "# resulting single amino acid changes\n",
    "aggregate_aminoacids = True\n",
    "# decompose MNVs and combine counts for resulting SNVs\n",
    "aggregate_nucleotides = True\n",
    "# annotate targeted nucleotide changes in the tables.\n",
    "target_nt_annotation = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide a file that maps gene names to gene IDs\n",
    "# this is necessary when targeted variant annotations use\n",
    "# gene names instead of gene IDs. Otherwise provide None\n",
    "geneid_to_genename = '/opt/project_resources/geneid_to_genename.tsv'\n",
    "# annotate targeted amino acid changes in the tables\n",
    "# using the file, or otherwise provide None\n",
    "target_aa_annotation = '/opt/project_resources/targets.tsv'\n",
    "# decompose multi amino acid changes and combine counts of\n",
    "# resulting single amino acid changes\n",
    "aggregate_aminoacids = True\n",
    "# decompose MNVs and combine counts for resulting SNVs\n",
    "aggregate_nucleotides = True\n",
    "# annotate targeted nucleotide changes in the tables.\n",
    "target_nt_annotation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# analysis settings dictionary\n",
    "settings = settings\n",
    "# provide the path to the settings file\n",
    "# if settings dictionary has not been loaded\n",
    "settings_file = None\n",
    "# use snpEff to annotate the variants\n",
    "annotate = True\n",
    "# additional vt options for decomposing nucleotides.\n",
    "# Supply [\"-a\"] to include indels and complex variants\n",
    "# in decomposition, or other options shown above if desired.\n",
    "decompose_options = []\n",
    "# was the initial vcf file was annotated by snpEff?\n",
    "annotated_vcf = False\n",
    "# create tables for variants as they are in the vcf file\n",
    "# without decomposing compex variants or indels.\n",
    "# Multiallelic variants will be split into biallelic.\n",
    "aggregate_none = True\n",
    "# filter variant sites for quality\n",
    "min_site_qual = 1\n",
    "# reset targeted variant counts to zero\n",
    "# when the site quality is below this value\n",
    "min_target_site_qual = -1\n",
    "# reset genotypes in the vcf file to NA\n",
    "# and depth to 0 if FORMAT/GQ value for a variant/sample\n",
    "# is below this value:\n",
    "min_genotype_qual = -1\n",
    "# reset alt allele count in the vcf file to 0\n",
    "# if FORMAT/QA value divided by FORMAT/AO for a variant/sample\n",
    "# is below this value:\n",
    "min_mean_alt_qual = -1 # average quality cut off for variants\n",
    "# There are also available, similar filters for:\n",
    "# min_mean_ref_qual : resetting low qual reference allele counts\n",
    "# min_alt_qual : similar to min_mean_alt_qual, but for total qual score\n",
    "# min_ref_qual : similar to min_alt_qual but for reference alleles\n",
    "\n",
    "# prefix for output files, if desired.\n",
    "# this is useful when different quality thresholds etc will be used\n",
    "# to avoid overwriting the files. For example, if min_genotype_qual = 1\n",
    "# and min_mean_alt_qual = 15 is used, a suitable prefix could be\n",
    "# \"gq1.mqa15.\"\n",
    "output_prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decompose_blocksub v0.5\n",
      "\n",
      "options:     input VCF file        /opt/user/stats_and_variant_calling/split.variants_mutations_only.vcf.gz\n",
      "         [o] output VCF file       /opt/user/stats_and_variant_calling/decomposed.variants_mutations_only.vcf.gz\n",
      "         [a] align/aggressive mode false\n",
      "\n",
      "\n",
      "stats: no. variants                       : 789\n",
      "       no. biallelic block substitutions  : 76\n",
      "\n",
      "       no. additional SNPs                : 416\n",
      "       no. variants after decomposition   : 1129\n",
      "\n",
      "Time elapsed: 0.34s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "\n",
    "# input vcf file\n",
    "vcf_file = vcf_file.split(\"/\")[-1].replace('.vcf.gz','_mutations_only.vcf.gz')\n",
    "mip.vcf_to_tables_fb(\n",
    "     vcf_file,\n",
    "     settings=settings,\n",
    "     settings_file=settings_file,\n",
    "     annotate=annotate,\n",
    "     geneid_to_genename=geneid_to_genename,\n",
    "     target_aa_annotation=target_aa_annotation,\n",
    "     aggregate_aminoacids=aggregate_aminoacids,\n",
    "     target_nt_annotation=target_nt_annotation, \n",
    "     aggregate_nucleotides=aggregate_nucleotides, \n",
    "     decompose_options=decompose_options,\n",
    "     annotated_vcf=annotated_vcf,\n",
    "     aggregate_none=aggregate_none,\n",
    "     min_site_qual=min_site_qual,\n",
    "     min_target_site_qual=min_target_site_qual,\n",
    "     min_genotype_qual=min_genotype_qual,\n",
    "     min_mean_alt_qual=min_mean_alt_qual,\n",
    "     output_prefix=output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables created\n",
    "alternate_XX_table.csv files will contain the ALT allele count for that table type while coverage_XX_table.csv will contain the depth of coverage at each locus.\n",
    "### Nucleotide changes (aggregated)\n",
    "For some projects we may be interested in specific single nucleotide changes. For these, it would make sense to decompose multi nucleotide changes and combine counts of the same single nucleotide changes. Two tables will be generated for count and coverage data for aggregated nucleotide changes:  \n",
    "\n",
    "**alternate_AN_table.csv** file in the analysis directory is created if aggregate_nucleotides option was selected when creating data tables. This table has the UMI counts for each alternate nucleotide.  \n",
    "\n",
    "**coverage_AN_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_AN_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1. When calls from multiple variants are aggregated; if all 0/0 then -> 0, if any 0/0 and non-0/0 then -> 1, if all 1/1 then -> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino acid changes (aggregated)\n",
    "For some projects we may be interested in the amino acid changes, particularly specific, targeted amino acid changes, such as drug resistance mutations in *Plasmodium falciparum*, which is the data set provided for pipeline test. For these type of projects, we may want to analyze the variants from the amino acid perspective, rather than nucleotide changes which is standard output for variant callers.  \n",
    "\n",
    "**alternate_AA_table.csv** file in the analysis directory is created if aggregate_aminoacids option was selected when creating data tables. This table has the UMI counts for each alternate amino acid.  \n",
    "\n",
    "**coverage_AA_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_AA_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1. When calls from multiple variants are aggregated; if all 0/0 then -> 0, if any 0/0 and non-0/0 then -> 1, if all 1/1 then -> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleotide changes (not aggregated)\n",
    "For some projects we may be interested in keeping composite variants as they are called by the pipeline. These will include MNVs, comlplex variants including indels, etc. Two tables will be generated for count and coverage data for original nucleotide changes:  \n",
    "\n",
    "**alternate_table.csv** file in the analysis directory is created if aggregate_none option was selected when creating data tables. This table has the UMI counts for each alternate nucleotide.  \n",
    "\n",
    "**coverage_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## freebayes help documentation\n",
    "Below are the various sections of freebayes --help output showing examples and options.\n",
    "```bash\n",
    "citation: Erik Garrison, Gabor Marth\n",
    "          \"Haplotype-based variant detection from short-read sequencing\"\n",
    "          arXiv:1207.3907 (http://arxiv.org/abs/1207.3907)\n",
    "\n",
    "author:   Erik Garrison <erik.garrison@bc.edu>, Marth Lab, Boston College, 2010-2014\n",
    "version:  v1.3.1-dirty\n",
    "```\n",
    "\n",
    "\n",
    "### overview:\n",
    "```bash\n",
    "    To call variants from aligned short-read sequencing data, supply BAM files and\n",
    "    a reference.  FreeBayes will provide VCF output on standard out describing SNPs,\n",
    "    indels, and complex variants in samples in the input alignments.\n",
    "\n",
    "    By default, FreeBayes will consider variants supported by at least 2\n",
    "    observations in a single sample (-C) and also by at least 20% of the reads from\n",
    "    a single sample (-F).  These settings are suitable to low to high depth\n",
    "    sequencing in haploid and diploid samples, but users working with polyploid or\n",
    "    pooled samples may wish to adjust them depending on the characteristics of\n",
    "    their sequencing data.\n",
    "\n",
    "    FreeBayes is capable of calling variant haplotypes shorter than a read length\n",
    "    where multiple polymorphisms segregate on the same read.  The maximum distance\n",
    "    between polymorphisms phased in this way is determined by the\n",
    "    --max-complex-gap, which defaults to 3bp.  In practice, this can comfortably be\n",
    "    set to half the read length.\n",
    "\n",
    "    Ploidy may be set to any level (-p), but by default all samples are assumed to\n",
    "    be diploid.  FreeBayes can model per-sample and per-region variation in\n",
    "    copy-number (-A) using a copy-number variation map.\n",
    "\n",
    "    FreeBayes can act as a frequency-based pooled caller and describe variants\n",
    "    and haplotypes in terms of observation frequency rather than called genotypes.\n",
    "    To do so, use --pooled-continuous and set input filters to a suitable level.\n",
    "    Allele observation counts will be described by AO and RO fields in the VCF output.\n",
    "\n",
    "```\n",
    "\n",
    "### examples:\n",
    "```bash\n",
    "    # call variants assuming a diploid sample\n",
    "    freebayes -f ref.fa aln.bam >var.vcf\n",
    "\n",
    "    # call variants assuming a diploid sample, providing gVCF output\n",
    "    freebayes -f ref.fa --gvcf aln.bam >var.gvcf\n",
    "\n",
    "    # require at least 5 supporting observations to consider a variant\n",
    "    freebayes -f ref.fa -C 5 aln.bam >var.vcf\n",
    "\n",
    "    # discard alignments overlapping positions where total read depth is greater than 200\n",
    "    freebayes -f ref.fa -g 200 aln.bam >var.vcf\n",
    "\n",
    "    # use a different ploidy\n",
    "    freebayes -f ref.fa -p 4 aln.bam >var.vcf\n",
    "\n",
    "    # assume a pooled sample with a known number of genome copies\n",
    "    freebayes -f ref.fa -p 20 --pooled-discrete aln.bam >var.vcf\n",
    "\n",
    "    # generate frequency-based calls for all variants passing input thresholds\n",
    "    freebayes -f ref.fa -F 0.01 -C 1 --pooled-continuous aln.bam >var.vcf\n",
    "\n",
    "    # use an input VCF (bgzipped + tabix indexed) to force calls at particular alleles\n",
    "    freebayes -f ref.fa -@ in.vcf.gz aln.bam >var.vcf\n",
    "\n",
    "    # generate long haplotype calls over known variants\n",
    "    freebayes -f ref.fa --haplotype-basis-alleles in.vcf.gz \\\n",
    "                        --haplotype-length 50 aln.bam\n",
    "\n",
    "    # naive variant calling: simply annotate observation counts of SNPs and indels\n",
    "    freebayes -f ref.fa --haplotype-length 0 --min-alternate-count 1 \\\n",
    "        --min-alternate-fraction 0 --pooled-continuous --report-monomorphic >var.vcf\n",
    "```\n",
    "\n",
    "### input:\n",
    "```bash\n",
    "   -b --bam FILE   Add FILE to the set of BAM files to be analyzed.\n",
    "   -L --bam-list FILE\n",
    "                   A file containing a list of BAM files to be analyzed.\n",
    "   -c --stdin      Read BAM input on stdin.  \n",
    "   -f --fasta-reference FILE\n",
    "                   Use FILE as the reference sequence for analysis.\n",
    "                   An index file (FILE.fai) will be created if none exists.\n",
    "                   If neither --targets nor --region are specified, FreeBayes\n",
    "                   will analyze every position in this reference.\n",
    "   -t --targets FILE\n",
    "                   Limit analysis to targets listed in the BED-format FILE.\n",
    "   -r --region <chrom>:<start_position>-<end_position>\n",
    "                   Limit analysis to the specified region, 0-base coordinates,\n",
    "                   end_position not included (same as BED format).\n",
    "                   Either '-' or '..' maybe used as a separator.\n",
    "   -s --samples FILE\n",
    "                   Limit analysis to samples listed (one per line) in the FILE.\n",
    "                   By default FreeBayes will analyze all samples in its input\n",
    "                   BAM files.\n",
    "   --populations FILE\n",
    "                   Each line of FILE should list a sample and a population which\n",
    "                   it is part of.  The population-based bayesian inference model\n",
    "                   will then be partitioned on the basis of the populations.\n",
    "   -A --cnv-map FILE\n",
    "                   Read a copy number map from the BED file FILE, which has\n",
    "                   either a sample-level ploidy:\n",
    "                      sample_name copy_number\n",
    "                   or a region-specific format:\n",
    "                      seq_name start end sample_name copy_number\n",
    "                   ... for each region in each sample which does not have the\n",
    "                   default copy number as set by --ploidy. These fields can be delimited\n",
    "                   by space or tab.\n",
    "\n",
    "```\n",
    "\n",
    "### output:\n",
    "```bash\n",
    "   -v --vcf FILE   Output VCF-format results to FILE. (default: stdout)\n",
    "   --gvcf\n",
    "                   Write gVCF output, which indicates coverage in uncalled regions.\n",
    "   --gvcf-chunk NUM\n",
    "                   When writing gVCF output emit a record for every NUM bases.\n",
    "   -& --gvcf-dont-use-chunk BOOL\n",
    "                   When writing the gVCF output emit a record for all bases if\n",
    "                   set to \"true\" , will also route an int to --gvcf-chunk\n",
    "                   similar to --output-mode EMIT_ALL_SITES from GATK\n",
    "   -@ --variant-input VCF\n",
    "                   Use variants reported in VCF file as input to the algorithm.\n",
    "                   Variants in this file will included in the output even if\n",
    "                   there is not enough support in the data to pass input filters.\n",
    "   -l --only-use-input-alleles\n",
    "                   Only provide variant calls and genotype likelihoods for sites\n",
    "                   and alleles which are provided in the VCF input, and provide\n",
    "                   output in the VCF for all input alleles, not just those which\n",
    "                   have support in the data. \n",
    "   --haplotype-basis-alleles VCF\n",
    "                   When specified, only variant alleles provided in this input\n",
    "                   VCF will be used for the construction of complex or haplotype\n",
    "                   alleles.\n",
    "   --report-all-haplotype-alleles\n",
    "                   At sites where genotypes are made over haplotype alleles,\n",
    "                   provide information about all alleles in output, not only\n",
    "                   those which are called.   \n",
    "   --report-monomorphic\n",
    "                   Report even loci which appear to be monomorphic, and report all\n",
    "                   considered alleles, even those which are not in called genotypes.\n",
    "                   Loci which do not have any potential alternates have '.' for ALT.\n",
    "   -P --pvar N     Report sites if the probability that there is a polymorphism\n",
    "                   at the site is greater than N.  default: 0.0.  Note that post-\n",
    "                   filtering is generally recommended over the use of this parameter.\n",
    "   --strict-vcf\n",
    "                   Generate strict VCF format (FORMAT/GQ will be an int)\n",
    "\n",
    "```\n",
    "\n",
    "### population model:\n",
    "```bash\n",
    "-T --theta N    The expected mutation rate or pairwise nucleotide diversity\n",
    "                   among the population under analysis.  This serves as the\n",
    "                   single parameter to the Ewens Sampling Formula prior model\n",
    "                   default: 0.001\n",
    "   -p --ploidy N   Sets the default ploidy for the analysis to N.  default: 2\n",
    "   -J --pooled-discrete\n",
    "                   Assume that samples result from pooled sequencing.\n",
    "                   Model pooled samples using discrete genotypes across pools.\n",
    "                   When using this flag, set --ploidy to the number of\n",
    "                   alleles in each sample or use the --cnv-map to define\n",
    "                   per-sample ploidy.\n",
    "   -K --pooled-continuous\n",
    "                   Output all alleles which pass input filters, regardles of\n",
    "                   genotyping outcome or model.\n",
    "```\n",
    "### reference allele:\n",
    "```bash\n",
    "   -Z --use-reference-allele\n",
    "                   This flag includes the reference allele in the analysis as\n",
    "                   if it is another sample from the same population.\n",
    "   --reference-quality MQ,BQ\n",
    "                   Assign mapping quality of MQ to the reference allele at each\n",
    "                   site and base quality of BQ.  default: 100,60\n",
    "```\n",
    "### allele scope:\n",
    "```bash\n",
    "   -n --use-best-n-alleles N\n",
    "                   Evaluate only the best N SNP alleles, ranked by sum of\n",
    "                   supporting quality scores.  (Set to 0 to use all; default: all)\n",
    "   -E --max-complex-gap N\n",
    "      --haplotype-length N\n",
    "                   Allow haplotype calls with contiguous embedded matches of up\n",
    "                   to this length. Set N=-1 to disable clumping. (default: 3)\n",
    "   --min-repeat-size N\n",
    "                   When assembling observations across repeats, require the total repeat\n",
    "                   length at least this many bp.  (default: 5)\n",
    "   --min-repeat-entropy N\n",
    "                   To detect interrupted repeats, build across sequence until it has\n",
    "                   entropy > N bits per bp. Set to 0 to turn off. (default: 1)\n",
    "   --no-partial-observations\n",
    "                   Exclude observations which do not fully span the dynamically-determined\n",
    "                   detection window.  (default, use all observations, dividing partial\n",
    "                   support across matching haplotypes when generating haplotypes.)\n",
    "\n",
    "  These flags are meant for testing.\n",
    "  They are not meant for filtering the output.\n",
    "  They actually filter the input to the algorithm by throwing away alignments.\n",
    "  This hurts performance by hiding information from the Bayesian model.\n",
    "  Do not use them unless you can validate that they improve results!\n",
    "\n",
    "   -I --throw-away-snp-obs     Remove SNP observations from input.\n",
    "   -i --throw-away-indels-obs  Remove indel observations from input.\n",
    "   -X --throw-away-mnp-obs     Remove MNP observations from input.\n",
    "   -u --throw-away-complex-obs Remove complex allele observations from input.\n",
    "\n",
    "  If you need to break apart haplotype calls to obtain one class of alleles,\n",
    "  run the call with default parameters, then normalize and subset the VCF:\n",
    "    freebayes ... | vcfallelicprimitives -kg >calls.vcf\n",
    "  For example, this would retain only biallelic SNPs.\n",
    "    <calls.vcf vcfsnps | vcfbiallelic >biallelic_snp_calls.vcf\n",
    "```\n",
    "### indel realignment:\n",
    "```bash\n",
    "   -O --dont-left-align-indels\n",
    "                   Turn off left-alignment of indels, which is enabled by default.\n",
    "\n",
    "```\n",
    "\n",
    "### input filters:\n",
    "```bash\n",
    "   -4 --use-duplicate-reads\n",
    "                   Include duplicate-marked alignments in the analysis.\n",
    "                   default: exclude duplicates marked as such in alignments\n",
    "   -m --min-mapping-quality Q\n",
    "                   Exclude alignments from analysis if they have a mapping\n",
    "                   quality less than Q.  default: 1\n",
    "   -q --min-base-quality Q\n",
    "                   Exclude alleles from analysis if their supporting base\n",
    "                   quality is less than Q.  default: 0\n",
    "   -R --min-supporting-allele-qsum Q\n",
    "                   Consider any allele in which the sum of qualities of supporting\n",
    "                   observations is at least Q.  default: 0\n",
    "   -Y --min-supporting-mapping-qsum Q\n",
    "                   Consider any allele in which and the sum of mapping qualities of\n",
    "                   supporting reads is at least Q.  default: 0\n",
    "   -Q --mismatch-base-quality-threshold Q\n",
    "                   Count mismatches toward --read-mismatch-limit if the base\n",
    "                   quality of the mismatch is >= Q.  default: 10\n",
    "   -U --read-mismatch-limit N\n",
    "                   Exclude reads with more than N mismatches where each mismatch\n",
    "                   has base quality >= mismatch-base-quality-threshold.\n",
    "                   default: ~unbounded\n",
    "   -z --read-max-mismatch-fraction N\n",
    "                   Exclude reads with more than N [0,1] fraction of mismatches where\n",
    "                   each mismatch has base quality >= mismatch-base-quality-threshold\n",
    "                   default: 1.0\n",
    "   -$ --read-snp-limit N\n",
    "                   Exclude reads with more than N base mismatches, ignoring gaps\n",
    "                   with quality >= mismatch-base-quality-threshold.\n",
    "                   default: ~unbounded\n",
    "   -e --read-indel-limit N\n",
    "                   Exclude reads with more than N separate gaps.\n",
    "                   default: ~unbounded\n",
    "   -0 --standard-filters  Use stringent input base and mapping quality filters\n",
    "                   Equivalent to -m 30 -q 20 -R 0 -S 0\n",
    "   -F --min-alternate-fraction N\n",
    "                   Require at least this fraction of observations supporting\n",
    "                   an alternate allele within a single individual in the\n",
    "                   in order to evaluate the position.  default: 0.05\n",
    "   -C --min-alternate-count N\n",
    "                   Require at least this count of observations supporting\n",
    "                   an alternate allele within a single individual in order\n",
    "                   to evaluate the position.  default: 2\n",
    "   -3 --min-alternate-qsum N\n",
    "                   Require at least this sum of quality of observations supporting\n",
    "                   an alternate allele within a single individual in order\n",
    "                   to evaluate the position.  default: 0\n",
    "   -G --min-alternate-total N\n",
    "                   Require at least this count of observations supporting\n",
    "                   an alternate allele within the total population in order\n",
    "                   to use the allele in analysis.  default: 1\n",
    "   --min-coverage N\n",
    "                   Require at least this coverage to process a site. default: 0\n",
    "   --limit-coverage N\n",
    "                   Downsample per-sample coverage to this level if greater than this coverage.\n",
    "                   default: no limit\n",
    "   -g --skip-coverage N\n",
    "                   Skip processing of alignments overlapping positions with coverage >N.\n",
    "                   This filters sites above this coverage, but will also reduce data nearby.\n",
    "                   default: no limit\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### population priors:\n",
    "```bash\n",
    "   -k --no-population-priors\n",
    "                   Equivalent to --pooled-discrete --hwe-priors-off and removal of\n",
    "                   Ewens Sampling Formula component of priors.\n",
    "```\n",
    "### mappability priors:\n",
    "```bash\n",
    "   -w --hwe-priors-off\n",
    "                   Disable estimation of the probability of the combination\n",
    "                   arising under HWE given the allele frequency as estimated\n",
    "                   by observation frequency. \n",
    "   -V --binomial-obs-priors-off\n",
    "                   Disable incorporation of prior expectations about observations.\n",
    "                   Uses read placement probability, strand balance probability,\n",
    "                   and read position (5'-3') probability.\n",
    "   -a --allele-balance-priors-off\n",
    "                   Disable use of aggregate probability of observation balance between alleles\n",
    "                   as a component of the priors.\n",
    "```\n",
    "### genotype likelihoods:\n",
    "```bash\n",
    "   --observation-bias FILE\n",
    "                   Read length-dependent allele observation biases from FILE.\n",
    "                   The format is [length] [alignment efficiency relative to reference]\n",
    "                   where the efficiency is 1 if there is no relative observation bias.\n",
    "   --base-quality-cap Q\n",
    "                   Limit estimated observation quality by capping base quality at Q.\n",
    "   --prob-contamination F\n",
    "                   An estimate of contamination to use for all samples.  default: 10e-9\n",
    "   --legacy-gls    Use legacy (polybayes equivalent) genotype likelihood calculations\n",
    "   --contamination-estimates FILE\n",
    "                   A file containing per-sample estimates of contamination, such as\n",
    "                   those generated by VerifyBamID.  The format should be:\n",
    "                       sample p(read=R|genotype=AR) p(read=A|genotype=AA)\n",
    "                   Sample '*' can be used to set default contamination estimates.\n",
    "```\n",
    "### algorithmic features:\n",
    "```bash\n",
    "   --report-genotype-likelihood-max\n",
    "                   Report genotypes using the maximum-likelihood estimate provided\n",
    "                   from genotype likelihoods.\n",
    "   -B --genotyping-max-iterations N\n",
    "                   Iterate no more than N times during genotyping step. default: 1000.\n",
    "   --genotyping-max-banddepth N\n",
    "                   Integrate no deeper than the Nth best genotype by likelihood when\n",
    "                   genotyping. default: 6.   \n",
    "   -W --posterior-integration-limits N,M\n",
    "                   Integrate all genotype combinations in our posterior space\n",
    "                   which include no more than N samples with their Mth best\n",
    "                   data likelihood. default: 1,3.\n",
    "   -N --exclude-unobserved-genotypes\n",
    "                   Skip sample genotypings for which the sample has no supporting reads.\n",
    "   -S --genotype-variant-threshold N\n",
    "                   Limit posterior integration to samples where the second-best\n",
    "                   genotype likelihood is no more than log(N) from the highest\n",
    "                   genotype likelihood for the sample.  default: ~unbounded\n",
    "   -j --use-mapping-quality\n",
    "                   Use mapping quality of alleles when calculating data likelihoods.\n",
    "   -H --harmonic-indel-quality\n",
    "                   Use a weighted sum of base qualities around an indel, scaled by the\n",
    "                   distance from the indel.  By default use a minimum BQ in flanking sequence.\n",
    "   -D --read-dependence-factor N\n",
    "                   Incorporate non-independence of reads by scaling successive\n",
    "                   observations by this factor during data likelihood\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "299px",
    "width": "248px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1114px",
    "left": "977px",
    "top": "163px",
    "width": "256.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
