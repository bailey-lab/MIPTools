{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code visibility (relevant for HTML only)\n",
    "Use the Show/Hide Code button on the top left to make to make the code visible or hide it. It will be hidden in the HTML files by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use code cells in this notebook\n",
    "If a code cell starts with \n",
    "```python\n",
    "# RUN\n",
    "```\n",
    "Run the cell by CTRL+Enter, or the Run button above.  \n",
    "\n",
    "If a code cell starts with\n",
    "```python\n",
    "# USER INPUT\n",
    "```\n",
    "User input is needed before running the cell. Usually there will be a cell preceding this which gives an example for the values to be provided.\n",
    "\n",
    "If a code cell starts with\n",
    "```python\n",
    "# OPTIONAL USER INPUT\n",
    "```\n",
    "User input is needed before running the cell. However, some defaults are provided, so make sure that either the settings will work for your run, or change them appropriately.\n",
    "\n",
    "If a cell starts with\n",
    "#### Example cell\n",
    "These cells are not code cells but examples of user inputs from the test data analysis for the actual code cell that follows it, informing the user about the formatting etc.\n",
    "\n",
    "**Important note on entering input:** When entering user input, please make sure you follow the formatting provided in the example cells. For example, when the parameter is text, make sure you have quotation marks around the parameters but when it is a number, do not enclose in quotes. If it is a list, then provide a list in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes reloading.\n",
      "functions reloading\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "import sys\n",
    "sys.path.append(\"/opt/extras/MIPTools/src\")\n",
    "import mip_functions as mip\n",
    "import probe_summary_generator\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import allel\n",
    "wdir = \"/opt/analysis/\"\n",
    "data_dir = \"/opt/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions reloading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'mip_functions' from '/opt/extras/MIPTools/src/mip_functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autoreload import reload\n",
    "reload(mip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "\n",
    "# provide the MIPWrangler output files\n",
    "# which must be located in the /opt/data directory within the container.\n",
    "# if more than one run is to be merged, provide all files\n",
    "info_files = [\"test_run_JJJ_DR1_VAR4_20191126.txt.gz\"] \n",
    "\n",
    "# sample sheets associated with each wrangler file,\n",
    "# in the same order as the wrangler files.\n",
    "sample_sheets = [\"sample_list.tsv\"]\n",
    "\n",
    "# No input below\n",
    "info_files = [data_dir + i for i in info_files]\n",
    "sample_sheets = [data_dir + s for s in sample_sheets]\n",
    "pd.concat([pd.read_table(s) for s in sample_sheets],\n",
    "         ignore_index=True).groupby([\"sample_set\", \"probe_set\"]).first()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide the MIPWrangler output files\n",
    "# which must be located in the /opt/data directory within the container.\n",
    "# if more than one run is to be merged, provide all files\n",
    "info_files = [] \n",
    "\n",
    "# sample sheets associated with each wrangler file,\n",
    "# in the same order as the wrangler files.\n",
    "sample_sheets = []\n",
    "\n",
    "# No input below\n",
    "info_files = [data_dir + i for i in info_files]\n",
    "sample_sheets = [data_dir + s for s in sample_sheets]\n",
    "pd.concat([pd.read_table(s) for s in sample_sheets],\n",
    "         ignore_index=True).groupby([\"sample_set\", \"probe_set\"]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the unique sample_set, probe_set combinations in the sample sheets provided. Select which combinations should be used for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "sample_groups = [[\"JJJ\", \"DR1,VAR4\"]]\n",
    "```\n",
    "\n",
    "If more than one combination is to be used, the input will be a list of lists, for example:\n",
    "```python\n",
    "sample_groups = [[\"sample_set_1\", \"probe_set_1\"], [\"sample_set_2\", \"probe_set_2\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "sample_groups = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the species and the probe set used\n",
    "These two are important parameters to determine which files will be used for analysis.  \n",
    "\n",
    "\n",
    "For the species, the options are: \"pf\" for *Plasmodium falciparum*, \"pv\" for *Plasmodium vivax*, \"hg19\" for *Homo sapiens* genome assembly hg19/GRCh37 and \"hg38\" for *Homo sapiens* genome assembly hg38/GRCh38  \n",
    "___\n",
    "Probe sets also must be specified. Check the output of the sample sheet summary above under **probe_set** field for a reminder of what the probe set of interest is. This is usually a three letter code or codes separated by a comma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "species = \"pf\"\n",
    "probe_sets_used = \"DR1,VAR4\"\n",
    "```\n",
    "\n",
    "It is also possible to analyse just a subset of probe sets that has been used. For example, if the data has both DR1 and VAR4 probe sets but I want to analyse only the DR1 set:\n",
    "```python\n",
    "species = \"pf\"\n",
    "probe_sets_used = \"DR1\"\n",
    "```\n",
    "Note that I'd still need to specifiy \"DR1,VAR4\" in the sample_groups above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "species =  \n",
    "probe_sets_used = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# available cpu count\n",
    "processorNumber = 20\n",
    "\n",
    "## extra bwa options for haplotype alignment\n",
    "# use \"-a\" for getting all alignments\n",
    "# use \"-L 500\" to penalize soft clipping \n",
    "# use \"-t\" to set number of available processors\n",
    "bwaExtra = [\"-t\", str(processorNumber)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "# available cpu count\n",
    "processorNumber = 20\n",
    "\n",
    "## extra bwa options for haplotype alignment\n",
    "# use \"-a\" for getting all alignments\n",
    "# use \"-L 500\" to penalize soft clipping \n",
    "# use \"-t\" to set number of available processors\n",
    "bwaExtra = [\"-t\", str(processorNumber)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get/Set the analysis settings\n",
    "Use the settings template for the species specified to get the  analysis settings and change the vaules specified in the above cell. This will create a template_settings.txt file in your analysis directory and a settings.txt file to be used for the analysis. These files also will serve as a reference of analysis settings for the sake of reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# copy the template settings file\n",
    "temp_settings_file = \"/opt/resources/templates/analysis_settings_templates/settings.txt\"\n",
    "subprocess.call([\"scp\", temp_settings_file, \"/opt/analysis/template_settings.txt\"])\n",
    "\n",
    "# extract the settings template\n",
    "temp_settings = mip.get_analysis_settings(\"/opt/analysis/template_settings.txt\")\n",
    "\n",
    "# update bwa settings with the options set above\n",
    "bwaOptions = temp_settings[\"bwaOptions\"]\n",
    "try:\n",
    "    bwaOptions.extend(bwaExtra)\n",
    "except AttributeError:\n",
    "    bwaOptions = [bwaOptions]\n",
    "    bwaOptions.extend(bwaExtra)\n",
    "\n",
    "# Create a list from the probe_sets string\n",
    "mipSetKey = probe_sets_used.split(\",\") + [\"\"]\n",
    "\n",
    "# create a dictionary for which settings should be updated\n",
    "# using the user specified parameters.\n",
    "update_keys = {\"processorNumber\": processorNumber,\n",
    "               \"bwaOptions\": bwaOptions,\n",
    "               \"species\": species,\n",
    "               \"mipSetKey\" : mipSetKey}\n",
    "# update the settings\n",
    "for k, v in update_keys.items():\n",
    "    temp_settings[k] = v\n",
    "# create a settings file in the analysis directory.\n",
    "settings_file = \"settings.txt\"\n",
    "settings_path = os.path.join(wdir, settings_file)\n",
    "mip.write_analysis_settings(temp_settings, settings_path)\n",
    "settings = mip.get_analysis_settings(wdir + settings_file)\n",
    "# create probe sets dictionary\n",
    "mip.update_probe_sets(\"/opt/project_resources/mip_ids/mipsets.csv\",\n",
    "                     \"/opt/project_resources/mip_ids/probe_sets.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Process run data\n",
    "First section of the data analysis involves processing the MIPWrangler output files, combining data from multiple runs (if necessary), mapping haplotypes and creating summary files and plots showing how the sequencing runs went."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIPWrangler output file processing\n",
    "Below operation combines output files from multiple runs, summing up count data belonging to the same libraries.  \n",
    "\n",
    "Libraries are labeled by combining three fields in the sample sheet: sample_name-sample_set-replicate, which makes the Sample ID. If two different libraries has the same Sample ID (same three fields, but a different LibraryPrep identifier), the overlapping libraries will be assigned new replicate numbers such that there are no shared IDs any more. A warning will be printed in that case, and the original sample ID and the new one will be written to the samples.tsv file generated in the analysis directory.\n",
    "\n",
    "If only a single output file is used, then the below operation just filters and renames some columns from the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "if len(info_files) > 1:\n",
    "    mip.combine_info_files(wdir,\n",
    "                           settings_file, \n",
    "                           info_files,\n",
    "                           sample_sheets,\n",
    "                           settings[\"mipsterFile\"],\n",
    "                           sample_sets=sample_groups)\n",
    "else:\n",
    "    mip.process_info_file(wdir,\n",
    "                          settings_file, \n",
    "                          info_files,\n",
    "                          sample_sheets,\n",
    "                          settings[\"mipsterFile\"],\n",
    "                          sample_sets=sample_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and map haplotype sequences\n",
    "Align each haplotype sequence to the reference genome. Remove off target haplotypes. All haplotype mappings will be saved to the disk so off targets can be inspected if needed. \n",
    "\n",
    "Some filters can be applied to remove noise:\n",
    "*  minHaplotypeBarcodes: minimum total UMI cut off across all samples.\n",
    "*  minHaplotypeSamples: minimum number of samples a haplotype is observed in.\n",
    "*  minHaplotypeSampleFraction: minimum fraction of samples a haplotype is observed in.  \n",
    "\n",
    "It is probably safe to apply minimal count filters like at least 10 UMIs across samples and at least two samples. However, most data sets will be easily handled without these filters. So it may be better to not filter at this step unless the downstream operations are taking too much resources. However, filters can and should be applied after variant calls are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# filter haplotype sequences based on the number of total supporting UMIs\n",
    "settings[\"minHaplotypeBarcodes\"] = 1\n",
    "# filter haplotype sequences based on the number of samples they were observed in\n",
    "settings[\"minHaplotypeSamples\"] = 1\n",
    "# filter haplotype sequences based on the fraction of samples they were observed in\n",
    "settings[\"minHaplotypeSampleFraction\"] = 0.0001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "# filter haplotype sequences based on the number of total supporting UMIs\n",
    "settings[\"minHaplotypeBarcodes\"] = 1\n",
    "# filter haplotype sequences based on the number of samples they were observed in\n",
    "settings[\"minHaplotypeSamples\"] = 1\n",
    "# filter haplotype sequences based on the fraction of samples they were observed in\n",
    "settings[\"minHaplotypeSampleFraction\"] = 0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN\n",
    "mip.get_vcf_haplotypes(settings)\n",
    "mip.get_haplotype_counts(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the mapping results\n",
    "Plotting the probe coverage by samples is a good  way to see overall experiment perfomance. It shows if a probe has at least 1 barcode (or however many is specified below) for a given sample.  \n",
    "\n",
    "Dark columns point to poor performing probes whereas dark rows indicate poor samples. Note that this excludes samples with no reads at all. Use \"all_barcode_counts.csv\" file if those are of interest as well.\n",
    "\n",
    "Some parameters can be supplied to the plotting function as noted in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# coverage filter: anything below this number will be considered absent\n",
    "barcode_threshold = 10\n",
    "# font size for tick labels for x and y axis\n",
    "tick_label_size=5\n",
    "# font size for heat map color bar\n",
    "cbar_label_size=5\n",
    "# figure resolution\n",
    "dpi=300\n",
    "# present/absent colors\n",
    "absent_color='black'\n",
    "present_color='green'\n",
    "# Save the plot in the analysis directory?\n",
    "# If false, plots the graph here.\n",
    "save=False\n",
    "# How frequent the x and y-axis ticks should be\n",
    "# every nth column will have  a tick\n",
    "ytick_freq=None\n",
    "xtick_freq=None\n",
    "# rotation of xtick labels\n",
    "xtick_rotation=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "barcode_counts = pd.read_csv(wdir + \"barcode_counts.csv\",\n",
    "             header = [0,1], index_col = 0)\n",
    "mip.plot_performance(barcode_counts,\n",
    "                     barcode_threshold=barcode_threshold,\n",
    "                     tick_label_size=tick_label_size,\n",
    "                     cbar_label_size=cbar_label_size,\n",
    "                     dpi=dpi,\n",
    "                     absent_color=absent_color,\n",
    "                     present_color=present_color,\n",
    "                     save=save,\n",
    "                     ytick_freq=ytick_freq,\n",
    "                     xtick_freq=xtick_freq,\n",
    "                     xtick_rotation=xtick_rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at summary stats \n",
    "There are summary statistics and meta data (if provided) we can use to determine if coverage is enough, whether further sequencing is necessary, and how to proceed if further sequencing will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "sample_summary = pd.read_csv(wdir + \"sample_summary.csv\")\n",
    "sample_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot total barcode count vs probe coverage\n",
    "A scatter plot of total barcode count vs number of probes covered at a certain barcode count is a good way to see how the relationship between total coverage and probe coverage, which is useful in determining how to proceed to the next experiments or analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "f = sns.pairplot(data = sample_summary,\n",
    "                x_vars = \"Barcode Count\",\n",
    "                y_vars = \"targets_with_10_barcodes\",\n",
    "                plot_kws={\"s\": 10})\n",
    "f.fig.set_size_inches(5,5)\n",
    "f.fig.set_dpi(150)\n",
    "f.fig.axes[0].set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repooling capture reactions for further sequencing. Factors to consider:\n",
    "1. What do you we want to accomplish? In most cases, we would like to get enough coverage for a number of probes for each sample. For example, the test data contains **50 probes** in total. Let's say it is sufficient if we had a coverage of **10** or more for each probe for a sample. Then, we would not want to sequence any more of that sample. \n",
    "```python\n",
    "target_coverage_count = 50\n",
    "target_coverage_key='targets_with_10_barcodes'\n",
    "```\n",
    "Alternatively, we can set a goal of a fraction of total probes to reach a certain coverage rather than an absolute number of probes. For 95% of the maximum number of probes observed (47 in this case): \n",
    "```python\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key='targets_with_10_barcodes'\n",
    "```\n",
    "Although we set our goal to 47 probes, it is likely that some sample will never reach that number regardless of how much we sequence, if there is a deletion in the region, for example. So it makes sense to set a total coverage threshold after which we don't expect more data. Looking at the plot above, it seems like after 1000 barcode counts, we would reach our goal for most samples. \n",
    "```python\n",
    "high_barcode_threshold = 10000\n",
    "```\n",
    "Another metric to use for determining if we want to sequence a sample more is the average read count per barcode count. This value indicates we have sequenced each unique molecular index in our sample so many times, so when the value is high, it is unlikely that we'd get more UMIs by sequencing the same library more. It makes more sense for a fresh MIP capture from these samples if more data is needed.\n",
    "```python\n",
    "barcode_coverage_threshold=10\n",
    "```\n",
    "Some samples perform very poorly for one reason or another. There are two options for these samples for repooling consideration: 1) Repool as much as we can for the next run, 2) Assuming there is a problem in the capture reaction, set up a new MIP capture reaction for these samples. It makes more sense to use option 1 if this is the first sequencing data using this library. Use option 2 if this library have been repooled at a higher volume already, but still producing poor data.\n",
    "```python\n",
    "barcode_count_threshold=100 # samples below total barcode count of this value is considered low coverage\n",
    "low_coverage_action='Repool' # what to do for low coverage samples (Repool or Recapture)\n",
    "```\n",
    "Sometimes a handful of samples show uneven coverage of loci, i.e. they have very good coverage of a handful of loci but poor coverage in others, which may point to a problem with the sample or the experiment in general. These samples are determined by comparing the subset of samples that reached the goal we set (completed samples) and those that have not. We look at the number of barcodes per probe for _completed_ samples and get 25th percentile (or other percentile as set) and assume that if a sample on average has this many barcodes per target, it should have reached the set goal. For example, if on average _completed_ samples, i.e. samples that cover 47 probes at 10 barcodes or more, have 10000 total barcodes, they would have ~200 (10000/47) barcodes per target covered. And if an _incomplete_ sample has 5000 total barcodes and only 10 targets covered, this value would be 500 for that sample and it would be flagged as **uneven coverage** in repooling document.\n",
    "```python\n",
    "assesment_key='targets_with_1_barcodes' # coverage key to compare \"complete\" and \"incomplete\" samples\n",
    "good_coverage_quantile=0.25 # percentile to set the threshold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "high_barcode_threshold = 10000\n",
    "target_coverage_count = None\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key = 'targets_with_10_barcodes'\n",
    "barcode_coverage_threshold = 10\n",
    "barcode_count_threshold = 100\n",
    "low_coverage_action = 'Recapture'\n",
    "assesment_key = 'targets_with_1_barcodes'\n",
    "good_coverage_quantile = 0.25\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "high_barcode_threshold = \n",
    "low_coverage_action = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "target_coverage_count = None\n",
    "target_coverage_fraction = 0.95\n",
    "target_coverage_key = 'targets_with_10_barcodes'\n",
    "barcode_coverage_threshold = 10\n",
    "barcode_count_threshold = 100\n",
    "assesment_key = 'targets_with_1_barcodes'\n",
    "good_coverage_quantile = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "meta = pd.read_csv(wdir + \"run_meta.csv\")\n",
    "data_summary = pd.merge(sample_summary, meta)\n",
    "mip.repool(wdir, \n",
    "           data_summary, \n",
    "           high_barcode_threshold, \n",
    "           target_coverage_count=target_coverage_count, \n",
    "           target_coverage_fraction=target_coverage_fraction, \n",
    "           target_coverage_key=target_coverage_key,\n",
    "           barcode_coverage_threshold=barcode_coverage_threshold,\n",
    "           barcode_count_threshold=barcode_count_threshold, \n",
    "           low_coverage_action=low_coverage_action,\n",
    "           assesment_key=assesment_key,\n",
    "           good_coverage_quantile=good_coverage_quantile,\n",
    "           output_file='repool.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the repool document\n",
    "Library to completion field in the repool document has the value (volume) of how much from a sample should be pooled for re-sequencing. These values are only rough estimates and care should be taken to make sure there will be enough material to sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "pd.read_csv(wdir + \"repool.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Variant Calling\n",
    "Second part of the analysis involves variant calling and variant analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options for freebayes wrapper\n",
    "```Python\n",
    "align = True # Default is set to true, fastq files and bam files per sample\n",
    "# will be created in 'fastq_dir' and 'bam_dir'. \n",
    "# it should be set to false if bam files are available.\n",
    "\n",
    "settings = settings # analysis settings dictionary created above.\n",
    "\n",
    "bam_files = None # default is to use all bam files within the bam_dir.\n",
    "# if specific files should be used, then they can be specified in a list.\n",
    "\n",
    "verbose = True # prints errors and warnings as well as saving to disk.\n",
    "# if set to false, it will print that there is an error which will\n",
    "# be saved to disk which should be inspected for details.\n",
    "\n",
    "targets_file = None # force calls on specific loci even if there is\n",
    "# no observations satisfying filter criteria. Useful in cases of targeted\n",
    "# mutations such as drug resistance mutations.\n",
    "# Usually a file at \"/opt/project_resources/targets.tsv\" would be present\n",
    "# if the project requires it. Then targets_file should be set to this path.\n",
    "\n",
    "# paths for input-output files with default values that can be left unchanged\n",
    "fastq_dir, bam_dir, vcf_file, settings_file, errors_file, warnings_file\n",
    "\n",
    "# additional options to pass to freebayes directly:\n",
    "options = [] # see below for suggestions and possibilities.\n",
    "```\n",
    "#### Additional options for freebayes caller. \n",
    "Most of the freebayes options are shown below in the **freebayes help** section at the bottom of this document. Some options are integrated into the python wrapper freebayes_call, but others should be added depending on the data type, species etc.\n",
    "\n",
    "integrated options:\n",
    "```bash\n",
    "    -r region\n",
    "            limit calls to a specific region. \n",
    "            This is done internally, splitting the results into contigs and processing each contig\n",
    "            separately (in parallel if multiple cpus are available).\n",
    "            Per-contig vcf files are concatenated at the end into a single file.\n",
    "    -@ targets.vcf\n",
    "            force calls on positions provided in the vcf file\n",
    "            a vcf file is generated if a tab separated file containing targets are provided.\n",
    "    -L --bam-list\n",
    "            a list of bam files to be used. By default, all bams in bams directory will be used.\n",
    "            A list of specific bams can be specified to freebayes_call as bam_files option.\n",
    "```\n",
    "options to consider adding for parasite sequencing:\n",
    "```bash\n",
    "    --pooled-continuous\n",
    "             This option does not make assumptions about the ploidy when making genotype calls.\n",
    "             It makes sense for a mixed ploidy sample such as parasite infected blood DNA.\n",
    "             variants are still called as diploid. \n",
    "    --min-alternate-fraction 0.01\n",
    "             since we assume a pooled continuous sample, it would be better to set a within\n",
    "             samlpe allele frequency threshold to remove noise. \n",
    "             this is likely not needed when dealing with a diploid sample because a frequency \n",
    "             of 0.01 will likely be considered noise for a diploid sample.\n",
    "    --min-alternate-count 2\n",
    "             number of reads supporting a variant to consider for genotype calls.\n",
    "             having this at at least 2 is good. It will be possible to process\n",
    "             variants with 1 reads in postprocessing steps if a specific variant\n",
    "             is observed at least in one sample at > 1 reads. So this removes the \n",
    "             variant from consideration if no sample has > 1 reads supporting it.\n",
    "    --min-alternate-total 10\n",
    "             total read support for a variant across samples.\n",
    "```\n",
    "options to consider for human sequences:\n",
    "```bash\n",
    "    --min-mapping-quality 0\n",
    "             default for this setting is 1. I do not think this is helping much in \n",
    "             addressing mapping issues. However, reads in copy number variant regions\n",
    "             may have 0 mapping quality. These would be worth to keep, but they\n",
    "             should be handled appropriately at postprocessing steps.\n",
    "    --min-alternate-count 2\n",
    "    --min-alternate-fraction 0.05 (default)\n",
    "    --min-alternate-total 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Example cell\n",
    "```python\n",
    "# provide freebayes options.\n",
    "# These will be directy passed to freebayes\n",
    "options = [\"--pooled-continuous\",\n",
    "           \"--min-alternate-fraction\", \"0.01\",\n",
    "           \"--min-alternate-count\", \"2\",\n",
    "           \"--min-alternate-total\", \"10\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide freebayes options.\n",
    "# These will be directy passed to freebayes\n",
    "options = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "align=True\n",
    "verbose=True\n",
    "# where to save generated fastq files\n",
    "fastq_dir=\"/opt/analysis/padded_fastqs\"\n",
    "# where to save generated bam files\n",
    "bam_dir=\"/opt/analysis/padded_bams\"\n",
    "# where to save the output vcf file\n",
    "vcf_file=\"/opt/analysis/variants.vcf.gz\"\n",
    "# where is the targeted variants file\n",
    "targets_file=\"/opt/project_resources/targets.tsv\"\n",
    "# where to save errors and warnings generated by freebayes\n",
    "errors_file=\"/opt/analysis/freebayes_errors.txt\"\n",
    "warnings_file=\"/opt/analysis/freebayes_warnings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "r = mip.freebayes_call(\n",
    "        settings=settings,\n",
    "        options=options,\n",
    "        align=align,\n",
    "        verbose=verbose,\n",
    "        fastq_dir=fastq_dir,\n",
    "        bam_dir=bam_dir,\n",
    "        vcf_file=vcf_file,\n",
    "        targets_file=targets_file,\n",
    "        bam_files=None,\n",
    "        errors_file=errors_file,\n",
    "        warnings_file=warnings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions reloading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'mip_functions' from '/opt/extras/MIPTools/src/mip_functions.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = mip.get_analysis_settings(\"settings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN \n",
    "\n",
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# where to save generated fastq files\n",
    "fastq_dir = \"/opt/analysis/padded_fastqs\"\n",
    "# where to save generated bam files\n",
    "bam_dir = \"/opt/analysis/padded_bams\"\n",
    "# where to save the output vcf file\n",
    "vcf_file = \"/opt/analysis/gatk.vcf.gz\"\n",
    "# where is the targeted variants file\n",
    "targets_file = \"/opt/project_resources/targets.tsv\"\n",
    "# file name to save the genomics database prior to joint genotype calling\n",
    "gdb_file = \"my_database.gdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions reloading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'mip_functions' from '/opt/extras/MIPTools/src/mip_functions.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip.gatk_file_prep(bam_dir=bam_dir,\n",
    "                   fastq_dir=fastq_dir,\n",
    "                   targets_file=targets_file,\n",
    "                   settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotype_caller_options = [\"--java-options\", \"-Xmx16G\", \n",
    "                            \"-ERC\", \"GVCF\", \n",
    "                            \"--max-reads-per-alignment-start\", \"0\",\n",
    "                            \"--max-alternate-alleles\", \"10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip.gatk_haplotype_caller(options=haplotype_caller_options,\n",
    "                          bam_dir=bam_dir,\n",
    "                          settings=settings,\n",
    "                         sample_map=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype_gvcfs_options = [\"--java-options\", \"-Xmx32G\",  \n",
    "                          \"--standard-min-confidence-threshold-for-calling\", \"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error ocurred when during genomics DB import. Please see the /opt/analysis/gatk_genotype_gvcfs_output.txt for details.\n"
     ]
    }
   ],
   "source": [
    "mip.genotype_gvcfs(settings=settings,\n",
    "                   bam_dir=bam_dir,\n",
    "                   options=genotype_gvcfs_options,\n",
    "                   gdb=gdb_file, vcf_file=vcf_file,\n",
    "                   sample_map=None, keep_control_mutant=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should create the vcf file variants.vcf.gz in the analysis directory (assuming the vcf_file parameter was not changed).  \n",
    "\n",
    "We can annotate it using snpEff by running the following cell. However, this is not necessary if postprocessing steps described later will be carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "annotated_vcf = \"ann.\" + vcf_file.split(\"/\")[-1]\n",
    "annotated_vcf = os.path.join(wdir, annotated_vcf)\n",
    "res = mip.annotate_vcf_file(settings, vcf_file=vcf_file,\n",
    "                            annotated_vcf_file=annotated_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing of called variants\n",
    "Freebayes produces high quality vcf files with haplotype based variant calls. This is important for getting more accurate calls, especially for complex regions where SNVs may overlap with indels and there may be many possible alleles as opposed to a simple biallelic SNV call.\n",
    "\n",
    "However, it may be desired to \"decompose\" these complex variants for some applications. For example, if we are interested in knowing the prevalence of a specific drug resistance mutation, it would make sense to combine all variants containing this mutation even though they may be part of different haplotypes, and hence are represented in the vcf in different variants.\n",
    "\n",
    "vcf_to_tables function takes the vcf file generated by freebayes and generates allele count and coverage data in table form. It is possible to decompose and aggregate amino acid and/or nucleotide level variants. 3 files containing count data are generated: alternate_table.csv, reference_table.csv, coverage_table.csv, for alt allele, ref allele and coverage count values for each variant, respectively.\n",
    "\n",
    "It first separates the multiallelic calls to bi-allelic calls.\n",
    "\n",
    "#### annotate, default=True\n",
    "It then annotates variants using snpEff.\n",
    "\n",
    "#### geneid_to_genename, default=None\n",
    "Variant annotation provides a gene ID (e.g. PF3D7_0709000) but it does not provide common gene names (e.g. crt). If common names are used in target files, or they are desired in general, a tab separated gene ID to gene name file can be used. **gene_name and gene_id** columns are required. If no file is provided, gene name will be the same as the gene ID.\n",
    "\n",
    "#### aggregate_aminoacids, default=False\n",
    "If aminoacid level aggregation is requested, it decomposes multi amino acid missense variants into single components and aggregates the alternate allele and coverage counts per amino acid change. For example, Asn75Glu change for crt gene is a known drug resistance mutation in Plasmodium falciparum. There may be 3 separate variants in the vcf file that contain this mutation: Asn75Glu, MetAsn75IleGlu, Asn75Glu_del76-80*. All three has the missense variant Asn75Glu. While the first two  are simple changes, the third is a complex change including a 5 amino acid deletion and a stop codon following Asn75Glu. In this case, it makes sense to combine the counts of the first two variants towards Asn75Glu counts but the third one is debatable because of the complexity; i.e. the drug resistance mutation Asn75Glu probably is not that improtant in that context because of the stop codon following it. So we decompose the simple changes and aggregate but leave complex changes as they are. If aminoacid aggregation is carried out, file names will contain AA tag.\n",
    "\n",
    "#### target_aa_annotation, default=None\n",
    "It is also possible to annotate the targeted variants (such as Asn75Glu above) in the generated tables as 'Targeted' in case some analysis should be carried out on targeted variants only. A tab separated file containing the annotation details is required for this operation. **gene_name, aminoacid_change and mutation_name** are required fields. If a variants gene_name and aminoacid_change are matching to a row in the target file, that variant will be marked as targeted and will have the correspondign mutation name. Note that if common gene name conversion (see above) is not used, the gene_name column in this file must match the actual gene ID and not the common name. It may be more convenient to keep the gene IDs in the target file as well and use that file for ID to name mapping. **aggregate_aminoacids must be set to True** for this option to be used.\n",
    "\n",
    "#### aggregate_nucleotides, default=False\n",
    "A similar aggregation can be done at nucleotide level. If specified, biallelic variants will be decomposed using the tool **vt decompose_blocksub**. By default it decomposes block substitutions that do not include indels. However, it is also possible to decompose complex variants including indels by providing -a option. For possible decompose options see vt help:\n",
    "```bash\n",
    "vt decompose_blocksub options : \n",
    "  -p  Output phased genotypes and PS tags for decomposed variants [false]\n",
    "  -m  keep MNVs (multi-nucleotide variants) [false]\n",
    "  -a  enable aggressive/alignment mode [false]\n",
    "  -d  MNVs max distance (when -m option is used) [2]\n",
    "  -o  output VCF file [-]\n",
    "  -I  file containing list of intervals []\n",
    "  -i  intervals []\n",
    "  -?  displays help\n",
    "```\n",
    "If nucleotide level aggregation is done, the file names will include AN tag.\n",
    "\n",
    "#### target_nt_annotation, default=None\n",
    "Annotation of targeted nucleotides requires a file similar to the targeted amino acid annotation. However, the required fields for this annotation are: CHROM, POS, REF, ALT and mutation_name. **aggregate_nucleotides must be set to True** for this option to be used.\n",
    "\n",
    "#### aggregate_none, default=True\n",
    "It is also possible to generate count tables without doing any aggregation. This will generate the 3 count files, and all of the variant information included in the vcf file will be a separate column in the table's index. For annotated initial vcf files, or if annotate option is selected, each subfield in the INFO/ANN field will have its own column.\n",
    "\n",
    "#### min_site_qual, default=-1\n",
    "Filter variant sites for a minimum QUAL value assigned by the variant caller. This value is described in freebayes manual as:\n",
    "```bash\n",
    "Of primary interest to most users is the QUAL field, which estimates the probability that there is a polymorphism at the loci described by the record. In freebayes, this value can be understood as 1 - P(locus is homozygous given the data). It is recommended that users use this value to filter their results, rather than accepting anything output by freebayes as ground truth.\n",
    "\n",
    "By default, records are output even if they have very low probability of variation, in expectation that the VCF will be filtered using tools such as vcffilter in vcflib, which is also included in the repository under vcflib/. For instance,\n",
    "\n",
    "freebayes -f ref.fa aln.bam | vcffilter -f \"QUAL > 20\" >results.vcf\n",
    "\n",
    "removes any sites with estimated probability of not being polymorphic less than phred 20 (aka 0.01), or probability of polymorphism > 0.99.\n",
    "\n",
    "In simulation, the receiver-operator characteristic (ROC) tends to have a very sharp inflection between Q1 and Q30, depending on input data characteristics, and a filter setting in this range should provide decent performance. Users are encouraged to examine their output and both variants which are retained and those they filter out. Most problems tend to occur in low-depth areas, and so users may wish to remove these as well, which can also be done by filtering on the DP flag.\n",
    "```\n",
    "Therefore, a **minimum of 1** should be used as a min_site_qual to remove low quality sites. If a site is annotated as **targeted**, the site will be kept regardless of its qual value, however, the alternate observation counts for the site may be reset to zero depending on the min_target_site_qual value described below.\n",
    "\n",
    "#### min_target_site_qual, default=-1\n",
    "If a variant site is targeted but the site qual is lower than this,\n",
    "reset the alternate observation counts to 0. It may be best to leave\n",
    "this at the default value since there is usually additional evidence\n",
    "that a targeted variant exists in a samples compared to a de novo\n",
    "variant, i.e. those variants that are targeted had been observed in other samples/studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# provide a file that maps gene names to gene IDs\n",
    "# this is necessary when targeted variant annotations use\n",
    "# gene names instead of gene IDs\n",
    "geneid_to_genename = \"/opt/project_resources/geneid_to_genename.tsv\"\n",
    "# annotate targted amino acid changes in the tables.\n",
    "target_aa_annotation = \"/opt/project_resources/targets.tsv\"\n",
    "# decompose multi amino acid changes and combine counts of\n",
    "# resulting single amino acid changes\n",
    "aggregate_aminoacids = True\n",
    "# decompose MNVs and combine counts for resulting SNVs\n",
    "aggregate_nucleotides = True\n",
    "# annotate targeted nucleotide changes in the tables.\n",
    "target_nt_annotation = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide a file that maps gene names to gene IDs\n",
    "# this is necessary when targeted variant annotations use\n",
    "# gene names instead of gene IDs\n",
    "geneid_to_genename = \n",
    "# annotate targted amino acid changes in the tables.\n",
    "target_aa_annotation = \n",
    "# decompose multi amino acid changes and combine counts of\n",
    "# resulting single amino acid changes\n",
    "aggregate_aminoacids = \n",
    "# decompose MNVs and combine counts for resulting SNVs\n",
    "aggregate_nucleotides = \n",
    "# annotate targeted nucleotide changes in the tables.\n",
    "target_nt_annotation = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# analysis settings dictionary\n",
    "settings=settings\n",
    "# provide the path to the settings file\n",
    "# if settings dictionary has not been loaded\n",
    "settings_file=None\n",
    "# use snpEff to annotate the variants\n",
    "annotate=True\n",
    "# additional vt options for decomposing nucleotides.\n",
    "# Supply [\"-a\"] to include indels and complex variants\n",
    "# in decomposition, or other options shown above if desired.\n",
    "decompose_options=[]\n",
    "# was the initial vcf file was annotated by snpEff?\n",
    "annotated_vcf=False\n",
    "# create tables for variants as they are in the vcf file\n",
    "# without decomposing compex variants or indels.\n",
    "# Multiallelic variants will be split into biallelic.\n",
    "aggregate_none=True\n",
    "# filter variant sites for quality\n",
    "min_site_qual=1\n",
    "# reset targeted variant counts to zero\n",
    "# when the site quality is below this value\n",
    "min_target_site_qual=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# input vcf file\n",
    "vcf_file = vcf_file.split(\"/\")[-1]\n",
    "mip.vcf_to_tables(\n",
    "     vcf_file,\n",
    "     settings=settings,\n",
    "     settings_file=settings_file,\n",
    "     annotate=annotate,\n",
    "     geneid_to_genename=geneid_to_genename,\n",
    "     target_aa_annotation=target_aa_annotation,\n",
    "     aggregate_aminoacids=aggregate_aminoacids,\n",
    "     target_nt_annotation=target_nt_annotation, \n",
    "     aggregate_nucleotides=aggregate_nucleotides, \n",
    "     decompose_options=decompose_options,\n",
    "     annotated_vcf=annotated_vcf,\n",
    "     aggregate_none=aggregate_none,\n",
    "     min_site_qual=min_site_qual,\n",
    "     min_target_site_qual=min_target_site_qual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables created\n",
    "alternate_XX_table.csv files will contain the ALT allele count for that table type while coverage_XX_table.csv will contain the coverage at each locus.\n",
    "### Nucleotide changes (aggregated)\n",
    "For some projects we may be interested in specific single nucleotide changes. For these, it would make sense to decompose multi nucleotide changes and combine counts of the same single nucleotide changes. Two tables will be generated for count and coverage data for aggregated nucleotide changes:  \n",
    "\n",
    "**alternate_AN_table.csv** file in the analysis directory is created if aggregate_nucleotides option was selected when creating data tables. This table has the UMI counts for each alternate nucleotide.  \n",
    "\n",
    "**coverage_AN_table.csv** file is the corresponding coverage depth for each variant's position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino acid changes (aggregated)\n",
    "For some projects we may be interested in the amino acid changes, particularly specific, targeted amino acid changes, such as drug resistance mutations in *Plasmodium falciparum*, which is the data set provided for pipeline test. For these type of projects, we may want to analyze the variants from the amino acid perspective, rather than nucleotide changes which is standard output for variant callers.  \n",
    "\n",
    "**alternate_AA_table.csv** file in the analysis directory is created if aggregate_aminoacids option was selected when creating data tables. This table has the UMI counts for each alternate amino acid.  \n",
    "\n",
    "**coverage_AA_table.csv** file is the corresponding coverage depth for each variant's position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleotide changes (not aggregated)\n",
    "For some projects we may be interested in keeping composite variants as they are called by the pipeline. These will include MNVs, comlplex variants including indels, etc. Two tables will be generated for count and coverage data for original nucleotide changes:  \n",
    "\n",
    "**alternate_table.csv** file in the analysis directory is created if aggregate_none option was selected when creating data tables. This table has the UMI counts for each alternate nucleotide.  \n",
    "\n",
    "**coverage_table.csv** file is the corresponding coverage depth for each variant's position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose which tables to analyse\n",
    "Select the type of data to analyse. Make sure the count file is matching the coverage file. e.g. alternate_XX_table and coverage_XX_table, XX must be the same value (AA, AN or nothing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "mutation_count_file = \"/opt/analysis/alternate_AA_table.csv\"\n",
    "mutation_coverage_file = \"/opt/analysis/coverage_AA_table.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "mutation_count_file = \n",
    "mutation_coverage_file = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "mutation_counts = pd.read_csv(mutation_count_file,\n",
    "                              header=list(range(6)),\n",
    "                              index_col=0)\n",
    "mutation_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "mutation_coverage = pd.read_csv(mutation_coverage_file,\n",
    "                                index_col=0,\n",
    "                                header=list(range(6)))\n",
    "mutation_coverage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call genotypes and prevalences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genotypes** for mutations can also be generated in a similar way to the genotypes for variants.  \n",
    "Possible filters:    \n",
    "1.  **min_coverage**: how many barcodes are needed to for a genomic position for a sample to reliable call (possible mixed or heterozygous) genotypes. If we set min_coverage = 10, any locus within a sample that is covered below this threshold will have an NA genotype.\n",
    "2.  **min_count**: if a genomic position have enough coverage, how many barcodes supporting an MUT (non-WT) aminoacid call is needed for a reliable call. If we set min_count = 2, any mutation with an MUT call that has less than 2 barcodes supporting the MUT call will revert to WT.\n",
    "3.  **min_freq**: a minimum within sample allele frequency threshold to consider a variant valid. If set to 0.01, for example, a variant locus in a sample that is at 0.005 frequency for the MUT allele within the sample, the locus would be called WT, if the within sample AF is between 0.01 and 0.99, it would be considered HET, and if > 0.99, it would be homozygous MUT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# filter mutation counts for minimum count parameter\n",
    "# by setting counts to zero if it is below threshold\n",
    "min_count = 2\n",
    "# filter loci without enough coverage by setting\n",
    "# coverage to zero if it is below threshold\n",
    "min_coverage = 10\n",
    "# call genotypes using the minimum within sample\n",
    "# allele frequency\n",
    "min_freq = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT \n",
    "\n",
    "# filter mutation counts for minimum count parameter\n",
    "# by setting counts to zero if it is below threshold\n",
    "min_count = \n",
    "# filter loci without enough coverage by setting\n",
    "# coverage to zero if it is below threshold\n",
    "min_coverage = \n",
    "# call genotypes using the minimum within sample\n",
    "# allele frequency\n",
    "min_freq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# filter mutation counts for minimum count parameter\n",
    "# by setting counts to zero if it is below threshold\n",
    "filtered_mutation_counts = mutation_counts.applymap(\n",
    "    lambda x: 0 if x < min_count else x)\n",
    "# filter loci without enough coverage by setting\n",
    "# coverage to zero if it is below threshold\n",
    "filtered_mutation_coverage = mutation_coverage.applymap(\n",
    "    lambda x: 0 if x < min_coverage else x)\n",
    "# calculate within sample frequency\n",
    "freq = filtered_mutation_counts / filtered_mutation_coverage\n",
    "freq.replace(np.inf, np.nan, inplace=True)\n",
    "# call genotypes using the minimum within sample\n",
    "# allele frequency parameter from the settings file\n",
    "genotypes = freq.applymap(\n",
    "    lambda x: np.nan if (np.isnan(x) or np.isinf(x))\n",
    "    else 0 if x == 0\n",
    "    else 0 if x < min_freq\n",
    "    else 1 if x < (1 - min_freq)\n",
    "    else 2)\n",
    "prevalence = freq.applymap(\n",
    "    lambda x: np.nan if (np.isnan(x) or np.isinf(x))\n",
    "    else 0 if x == 0\n",
    "    else 0 if x < min_freq\n",
    "    else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "genotypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "prevalence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## freebayes help\n",
    "Below are the various sections of freebayes --help output showing examples and options.\n",
    "```bash\n",
    "citation: Erik Garrison, Gabor Marth\n",
    "          \"Haplotype-based variant detection from short-read sequencing\"\n",
    "          arXiv:1207.3907 (http://arxiv.org/abs/1207.3907)\n",
    "\n",
    "author:   Erik Garrison <erik.garrison@bc.edu>, Marth Lab, Boston College, 2010-2014\n",
    "version:  v1.3.1-dirty\n",
    "```\n",
    "\n",
    "\n",
    "### overview:\n",
    "```bash\n",
    "    To call variants from aligned short-read sequencing data, supply BAM files and\n",
    "    a reference.  FreeBayes will provide VCF output on standard out describing SNPs,\n",
    "    indels, and complex variants in samples in the input alignments.\n",
    "\n",
    "    By default, FreeBayes will consider variants supported by at least 2\n",
    "    observations in a single sample (-C) and also by at least 20% of the reads from\n",
    "    a single sample (-F).  These settings are suitable to low to high depth\n",
    "    sequencing in haploid and diploid samples, but users working with polyploid or\n",
    "    pooled samples may wish to adjust them depending on the characteristics of\n",
    "    their sequencing data.\n",
    "\n",
    "    FreeBayes is capable of calling variant haplotypes shorter than a read length\n",
    "    where multiple polymorphisms segregate on the same read.  The maximum distance\n",
    "    between polymorphisms phased in this way is determined by the\n",
    "    --max-complex-gap, which defaults to 3bp.  In practice, this can comfortably be\n",
    "    set to half the read length.\n",
    "\n",
    "    Ploidy may be set to any level (-p), but by default all samples are assumed to\n",
    "    be diploid.  FreeBayes can model per-sample and per-region variation in\n",
    "    copy-number (-A) using a copy-number variation map.\n",
    "\n",
    "    FreeBayes can act as a frequency-based pooled caller and describe variants\n",
    "    and haplotypes in terms of observation frequency rather than called genotypes.\n",
    "    To do so, use --pooled-continuous and set input filters to a suitable level.\n",
    "    Allele observation counts will be described by AO and RO fields in the VCF output.\n",
    "\n",
    "```\n",
    "\n",
    "### examples:\n",
    "```bash\n",
    "    # call variants assuming a diploid sample\n",
    "    freebayes -f ref.fa aln.bam >var.vcf\n",
    "\n",
    "    # call variants assuming a diploid sample, providing gVCF output\n",
    "    freebayes -f ref.fa --gvcf aln.bam >var.gvcf\n",
    "\n",
    "    # require at least 5 supporting observations to consider a variant\n",
    "    freebayes -f ref.fa -C 5 aln.bam >var.vcf\n",
    "\n",
    "    # discard alignments overlapping positions where total read depth is greater than 200\n",
    "    freebayes -f ref.fa -g 200 aln.bam >var.vcf\n",
    "\n",
    "    # use a different ploidy\n",
    "    freebayes -f ref.fa -p 4 aln.bam >var.vcf\n",
    "\n",
    "    # assume a pooled sample with a known number of genome copies\n",
    "    freebayes -f ref.fa -p 20 --pooled-discrete aln.bam >var.vcf\n",
    "\n",
    "    # generate frequency-based calls for all variants passing input thresholds\n",
    "    freebayes -f ref.fa -F 0.01 -C 1 --pooled-continuous aln.bam >var.vcf\n",
    "\n",
    "    # use an input VCF (bgzipped + tabix indexed) to force calls at particular alleles\n",
    "    freebayes -f ref.fa -@ in.vcf.gz aln.bam >var.vcf\n",
    "\n",
    "    # generate long haplotype calls over known variants\n",
    "    freebayes -f ref.fa --haplotype-basis-alleles in.vcf.gz \\\n",
    "                        --haplotype-length 50 aln.bam\n",
    "\n",
    "    # naive variant calling: simply annotate observation counts of SNPs and indels\n",
    "    freebayes -f ref.fa --haplotype-length 0 --min-alternate-count 1 \\\n",
    "        --min-alternate-fraction 0 --pooled-continuous --report-monomorphic >var.vcf\n",
    "```\n",
    "\n",
    "### input:\n",
    "```bash\n",
    "   -b --bam FILE   Add FILE to the set of BAM files to be analyzed.\n",
    "   -L --bam-list FILE\n",
    "                   A file containing a list of BAM files to be analyzed.\n",
    "   -c --stdin      Read BAM input on stdin.  \n",
    "   -f --fasta-reference FILE\n",
    "                   Use FILE as the reference sequence for analysis.\n",
    "                   An index file (FILE.fai) will be created if none exists.\n",
    "                   If neither --targets nor --region are specified, FreeBayes\n",
    "                   will analyze every position in this reference.\n",
    "   -t --targets FILE\n",
    "                   Limit analysis to targets listed in the BED-format FILE.\n",
    "   -r --region <chrom>:<start_position>-<end_position>\n",
    "                   Limit analysis to the specified region, 0-base coordinates,\n",
    "                   end_position not included (same as BED format).\n",
    "                   Either '-' or '..' maybe used as a separator.\n",
    "   -s --samples FILE\n",
    "                   Limit analysis to samples listed (one per line) in the FILE.\n",
    "                   By default FreeBayes will analyze all samples in its input\n",
    "                   BAM files.\n",
    "   --populations FILE\n",
    "                   Each line of FILE should list a sample and a population which\n",
    "                   it is part of.  The population-based bayesian inference model\n",
    "                   will then be partitioned on the basis of the populations.\n",
    "   -A --cnv-map FILE\n",
    "                   Read a copy number map from the BED file FILE, which has\n",
    "                   either a sample-level ploidy:\n",
    "                      sample_name copy_number\n",
    "                   or a region-specific format:\n",
    "                      seq_name start end sample_name copy_number\n",
    "                   ... for each region in each sample which does not have the\n",
    "                   default copy number as set by --ploidy. These fields can be delimited\n",
    "                   by space or tab.\n",
    "\n",
    "```\n",
    "\n",
    "### output:\n",
    "```bash\n",
    "   -v --vcf FILE   Output VCF-format results to FILE. (default: stdout)\n",
    "   --gvcf\n",
    "                   Write gVCF output, which indicates coverage in uncalled regions.\n",
    "   --gvcf-chunk NUM\n",
    "                   When writing gVCF output emit a record for every NUM bases.\n",
    "   -& --gvcf-dont-use-chunk BOOL\n",
    "                   When writing the gVCF output emit a record for all bases if\n",
    "                   set to \"true\" , will also route an int to --gvcf-chunk\n",
    "                   similar to --output-mode EMIT_ALL_SITES from GATK\n",
    "   -@ --variant-input VCF\n",
    "                   Use variants reported in VCF file as input to the algorithm.\n",
    "                   Variants in this file will included in the output even if\n",
    "                   there is not enough support in the data to pass input filters.\n",
    "   -l --only-use-input-alleles\n",
    "                   Only provide variant calls and genotype likelihoods for sites\n",
    "                   and alleles which are provided in the VCF input, and provide\n",
    "                   output in the VCF for all input alleles, not just those which\n",
    "                   have support in the data. \n",
    "   --haplotype-basis-alleles VCF\n",
    "                   When specified, only variant alleles provided in this input\n",
    "                   VCF will be used for the construction of complex or haplotype\n",
    "                   alleles.\n",
    "   --report-all-haplotype-alleles\n",
    "                   At sites where genotypes are made over haplotype alleles,\n",
    "                   provide information about all alleles in output, not only\n",
    "                   those which are called.   \n",
    "   --report-monomorphic\n",
    "                   Report even loci which appear to be monomorphic, and report all\n",
    "                   considered alleles, even those which are not in called genotypes.\n",
    "                   Loci which do not have any potential alternates have '.' for ALT.\n",
    "   -P --pvar N     Report sites if the probability that there is a polymorphism\n",
    "                   at the site is greater than N.  default: 0.0.  Note that post-\n",
    "                   filtering is generally recommended over the use of this parameter.\n",
    "   --strict-vcf\n",
    "                   Generate strict VCF format (FORMAT/GQ will be an int)\n",
    "\n",
    "```\n",
    "\n",
    "### population model:\n",
    "```bash\n",
    "-T --theta N    The expected mutation rate or pairwise nucleotide diversity\n",
    "                   among the population under analysis.  This serves as the\n",
    "                   single parameter to the Ewens Sampling Formula prior model\n",
    "                   default: 0.001\n",
    "   -p --ploidy N   Sets the default ploidy for the analysis to N.  default: 2\n",
    "   -J --pooled-discrete\n",
    "                   Assume that samples result from pooled sequencing.\n",
    "                   Model pooled samples using discrete genotypes across pools.\n",
    "                   When using this flag, set --ploidy to the number of\n",
    "                   alleles in each sample or use the --cnv-map to define\n",
    "                   per-sample ploidy.\n",
    "   -K --pooled-continuous\n",
    "                   Output all alleles which pass input filters, regardles of\n",
    "                   genotyping outcome or model.\n",
    "```\n",
    "### reference allele:\n",
    "```bash\n",
    "   -Z --use-reference-allele\n",
    "                   This flag includes the reference allele in the analysis as\n",
    "                   if it is another sample from the same population.\n",
    "   --reference-quality MQ,BQ\n",
    "                   Assign mapping quality of MQ to the reference allele at each\n",
    "                   site and base quality of BQ.  default: 100,60\n",
    "```\n",
    "### allele scope:\n",
    "```bash\n",
    "   -n --use-best-n-alleles N\n",
    "                   Evaluate only the best N SNP alleles, ranked by sum of\n",
    "                   supporting quality scores.  (Set to 0 to use all; default: all)\n",
    "   -E --max-complex-gap N\n",
    "      --haplotype-length N\n",
    "                   Allow haplotype calls with contiguous embedded matches of up\n",
    "                   to this length. Set N=-1 to disable clumping. (default: 3)\n",
    "   --min-repeat-size N\n",
    "                   When assembling observations across repeats, require the total repeat\n",
    "                   length at least this many bp.  (default: 5)\n",
    "   --min-repeat-entropy N\n",
    "                   To detect interrupted repeats, build across sequence until it has\n",
    "                   entropy > N bits per bp. Set to 0 to turn off. (default: 1)\n",
    "   --no-partial-observations\n",
    "                   Exclude observations which do not fully span the dynamically-determined\n",
    "                   detection window.  (default, use all observations, dividing partial\n",
    "                   support across matching haplotypes when generating haplotypes.)\n",
    "\n",
    "  These flags are meant for testing.\n",
    "  They are not meant for filtering the output.\n",
    "  They actually filter the input to the algorithm by throwing away alignments.\n",
    "  This hurts performance by hiding information from the Bayesian model.\n",
    "  Do not use them unless you can validate that they improve results!\n",
    "\n",
    "   -I --throw-away-snp-obs     Remove SNP observations from input.\n",
    "   -i --throw-away-indels-obs  Remove indel observations from input.\n",
    "   -X --throw-away-mnp-obs     Remove MNP observations from input.\n",
    "   -u --throw-away-complex-obs Remove complex allele observations from input.\n",
    "\n",
    "  If you need to break apart haplotype calls to obtain one class of alleles,\n",
    "  run the call with default parameters, then normalize and subset the VCF:\n",
    "    freebayes ... | vcfallelicprimitives -kg >calls.vcf\n",
    "  For example, this would retain only biallelic SNPs.\n",
    "    <calls.vcf vcfsnps | vcfbiallelic >biallelic_snp_calls.vcf\n",
    "```\n",
    "### indel realignment:\n",
    "```bash\n",
    "   -O --dont-left-align-indels\n",
    "                   Turn off left-alignment of indels, which is enabled by default.\n",
    "\n",
    "```\n",
    "\n",
    "### input filters:\n",
    "```bash\n",
    "   -4 --use-duplicate-reads\n",
    "                   Include duplicate-marked alignments in the analysis.\n",
    "                   default: exclude duplicates marked as such in alignments\n",
    "   -m --min-mapping-quality Q\n",
    "                   Exclude alignments from analysis if they have a mapping\n",
    "                   quality less than Q.  default: 1\n",
    "   -q --min-base-quality Q\n",
    "                   Exclude alleles from analysis if their supporting base\n",
    "                   quality is less than Q.  default: 0\n",
    "   -R --min-supporting-allele-qsum Q\n",
    "                   Consider any allele in which the sum of qualities of supporting\n",
    "                   observations is at least Q.  default: 0\n",
    "   -Y --min-supporting-mapping-qsum Q\n",
    "                   Consider any allele in which and the sum of mapping qualities of\n",
    "                   supporting reads is at least Q.  default: 0\n",
    "   -Q --mismatch-base-quality-threshold Q\n",
    "                   Count mismatches toward --read-mismatch-limit if the base\n",
    "                   quality of the mismatch is >= Q.  default: 10\n",
    "   -U --read-mismatch-limit N\n",
    "                   Exclude reads with more than N mismatches where each mismatch\n",
    "                   has base quality >= mismatch-base-quality-threshold.\n",
    "                   default: ~unbounded\n",
    "   -z --read-max-mismatch-fraction N\n",
    "                   Exclude reads with more than N [0,1] fraction of mismatches where\n",
    "                   each mismatch has base quality >= mismatch-base-quality-threshold\n",
    "                   default: 1.0\n",
    "   -$ --read-snp-limit N\n",
    "                   Exclude reads with more than N base mismatches, ignoring gaps\n",
    "                   with quality >= mismatch-base-quality-threshold.\n",
    "                   default: ~unbounded\n",
    "   -e --read-indel-limit N\n",
    "                   Exclude reads with more than N separate gaps.\n",
    "                   default: ~unbounded\n",
    "   -0 --standard-filters  Use stringent input base and mapping quality filters\n",
    "                   Equivalent to -m 30 -q 20 -R 0 -S 0\n",
    "   -F --min-alternate-fraction N\n",
    "                   Require at least this fraction of observations supporting\n",
    "                   an alternate allele within a single individual in the\n",
    "                   in order to evaluate the position.  default: 0.05\n",
    "   -C --min-alternate-count N\n",
    "                   Require at least this count of observations supporting\n",
    "                   an alternate allele within a single individual in order\n",
    "                   to evaluate the position.  default: 2\n",
    "   -3 --min-alternate-qsum N\n",
    "                   Require at least this sum of quality of observations supporting\n",
    "                   an alternate allele within a single individual in order\n",
    "                   to evaluate the position.  default: 0\n",
    "   -G --min-alternate-total N\n",
    "                   Require at least this count of observations supporting\n",
    "                   an alternate allele within the total population in order\n",
    "                   to use the allele in analysis.  default: 1\n",
    "   --min-coverage N\n",
    "                   Require at least this coverage to process a site. default: 0\n",
    "   --limit-coverage N\n",
    "                   Downsample per-sample coverage to this level if greater than this coverage.\n",
    "                   default: no limit\n",
    "   -g --skip-coverage N\n",
    "                   Skip processing of alignments overlapping positions with coverage >N.\n",
    "                   This filters sites above this coverage, but will also reduce data nearby.\n",
    "                   default: no limit\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### population priors:\n",
    "```bash\n",
    "   -k --no-population-priors\n",
    "                   Equivalent to --pooled-discrete --hwe-priors-off and removal of\n",
    "                   Ewens Sampling Formula component of priors.\n",
    "```\n",
    "### mappability priors:\n",
    "```bash\n",
    "   -w --hwe-priors-off\n",
    "                   Disable estimation of the probability of the combination\n",
    "                   arising under HWE given the allele frequency as estimated\n",
    "                   by observation frequency. \n",
    "   -V --binomial-obs-priors-off\n",
    "                   Disable incorporation of prior expectations about observations.\n",
    "                   Uses read placement probability, strand balance probability,\n",
    "                   and read position (5'-3') probability.\n",
    "   -a --allele-balance-priors-off\n",
    "                   Disable use of aggregate probability of observation balance between alleles\n",
    "                   as a component of the priors.\n",
    "```\n",
    "### genotype likelihoods:\n",
    "```bash\n",
    "   --observation-bias FILE\n",
    "                   Read length-dependent allele observation biases from FILE.\n",
    "                   The format is [length] [alignment efficiency relative to reference]\n",
    "                   where the efficiency is 1 if there is no relative observation bias.\n",
    "   --base-quality-cap Q\n",
    "                   Limit estimated observation quality by capping base quality at Q.\n",
    "   --prob-contamination F\n",
    "                   An estimate of contamination to use for all samples.  default: 10e-9\n",
    "   --legacy-gls    Use legacy (polybayes equivalent) genotype likelihood calculations\n",
    "   --contamination-estimates FILE\n",
    "                   A file containing per-sample estimates of contamination, such as\n",
    "                   those generated by VerifyBamID.  The format should be:\n",
    "                       sample p(read=R|genotype=AR) p(read=A|genotype=AA)\n",
    "                   Sample '*' can be used to set default contamination estimates.\n",
    "```\n",
    "### algorithmic features:\n",
    "```bash\n",
    "   --report-genotype-likelihood-max\n",
    "                   Report genotypes using the maximum-likelihood estimate provided\n",
    "                   from genotype likelihoods.\n",
    "   -B --genotyping-max-iterations N\n",
    "                   Iterate no more than N times during genotyping step. default: 1000.\n",
    "   --genotyping-max-banddepth N\n",
    "                   Integrate no deeper than the Nth best genotype by likelihood when\n",
    "                   genotyping. default: 6.   \n",
    "   -W --posterior-integration-limits N,M\n",
    "                   Integrate all genotype combinations in our posterior space\n",
    "                   which include no more than N samples with their Mth best\n",
    "                   data likelihood. default: 1,3.\n",
    "   -N --exclude-unobserved-genotypes\n",
    "                   Skip sample genotypings for which the sample has no supporting reads.\n",
    "   -S --genotype-variant-threshold N\n",
    "                   Limit posterior integration to samples where the second-best\n",
    "                   genotype likelihood is no more than log(N) from the highest\n",
    "                   genotype likelihood for the sample.  default: ~unbounded\n",
    "   -j --use-mapping-quality\n",
    "                   Use mapping quality of alleles when calculating data likelihoods.\n",
    "   -H --harmonic-indel-quality\n",
    "                   Use a weighted sum of base qualities around an indel, scaled by the\n",
    "                   distance from the indel.  By default use a minimum BQ in flanking sequence.\n",
    "   -D --read-dependence-factor N\n",
    "                   Incorporate non-independence of reads by scaling successive\n",
    "                   observations by this factor during data likelihood\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
