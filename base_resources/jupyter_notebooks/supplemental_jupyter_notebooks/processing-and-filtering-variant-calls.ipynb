{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes reloading.\n",
      "functions reloading\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "import sys\n",
    "sys.path.append(\"/opt/src\")\n",
    "import mip_functions as mip\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Variant Calls\n",
    "Freebayes produces high quality vcf files with haplotype based variant calls. This is important for getting more accurate calls, especially for complex regions where SNVs may overlap with indels and there may be many possible alleles as opposed to a simple biallelic SNV call.   \n",
    "\n",
    "haplotype based variant example:  \n",
    "\n",
    "chr1  1000 AAA,AGC,TGC  \n",
    "\n",
    "However, it may be desired to \"decompose\" these complex variants for some applications. For example, if we are interested in knowing the prevalence of a specific drug resistance mutation, it would make sense to combine all variants containing this mutation even though they may be part of different haplotypes, and hence are represented in the vcf in different variants.  \n",
    "\n",
    "Decomposed variants:  \n",
    "\n",
    "chr1  1000 A T  \n",
    "chr1  1001 A G  \n",
    "chr1  1002 A C  \n",
    "\n",
    "vcf_to_tables function takes the vcf file generated by freebayes and generates allele count and coverage data in table form. It is possible to decompose and aggregate amino acid and/or nucleotide level variants. 3 files containing count data are generated: alternate_table.csv, reference_table.csv, coverage_table.csv, for alt allele, ref allele and coverage count values for each variant, respectively.\n",
    "\n",
    "It first separates the multiallelic calls to bi-allelic calls.\n",
    "\n",
    "#### annotate, default=True\n",
    "It then annotates variants using snpEff.\n",
    "\n",
    "#### geneid_to_genename, default=None\n",
    "Variant annotation provides a gene ID (e.g. PF3D7_0709000) but it does not provide common gene names (e.g. crt). If common names are used in target files, or they are desired in general, a tab separated gene ID to gene name file can be used. **gene_name and gene_id** columns are required. If no file is provided, gene name will be the same as the gene ID.\n",
    "\n",
    "#### aggregate_aminoacids, default=False\n",
    "If aminoacid level aggregation is requested, it decomposes multi amino acid missense variants into single components and aggregates the alternate allele and coverage counts per amino acid change. For example, Asn75Glu change for crt gene is a known drug resistance mutation in Plasmodium falciparum. There may be 3 separate variants in the vcf file that contain this mutation: Asn75Glu, MetAsn75IleGlu, Asn75Glu_del76-80*. All three has the missense variant Asn75Glu. While the first two  are simple changes, the third is a complex change including a 5 amino acid deletion and a stop codon following Asn75Glu. In this case, it makes sense to combine the counts of the first two variants towards Asn75Glu counts but the third one is debatable because of the complexity; i.e. the drug resistance mutation Asn75Glu probably is not that improtant in that context because of the stop codon following it. So we decompose the simple changes and aggregate but leave complex changes as they are. If aminoacid aggregation is carried out, file names will contain AA tag.\n",
    "\n",
    "#### target_aa_annotation, default=None\n",
    "It is also possible to annotate the targeted variants (such as Asn75Glu above) in the generated tables as 'Targeted' in case some analysis should be carried out on targeted variants only. A tab separated file containing the annotation details is required for this operation. **gene_name, aminoacid_change and mutation_name** are required fields. If a variants gene_name and aminoacid_change are matching to a row in the target file, that variant will be marked as targeted and will have the correspondign mutation name. Note that if common gene name conversion (see above) is not used, the gene_name column in this file must match the actual gene ID and not the common name. It may be more convenient to keep the gene IDs in the target file as well and use that file for ID to name mapping. **aggregate_aminoacids must be set to True** for this option to be used.\n",
    "\n",
    "#### aggregate_nucleotides, default=False\n",
    "A similar aggregation can be done at nucleotide level. If specified, biallelic variants will be decomposed using the tool **vt decompose_blocksub**. By default it decomposes block substitutions that do not include indels. However, it is also possible to decompose complex variants including indels by providing -a option. For possible decompose options see vt help:\n",
    "```bash\n",
    "vt decompose_blocksub options : \n",
    "  -p  Output phased genotypes and PS tags for decomposed variants [false]\n",
    "  -m  keep MNVs (multi-nucleotide variants) [false]\n",
    "  -a  enable aggressive/alignment mode [false]\n",
    "  -d  MNVs max distance (when -m option is used) [2]\n",
    "  -o  output VCF file [-]\n",
    "  -I  file containing list of intervals []\n",
    "  -i  intervals []\n",
    "  -?  displays help\n",
    "```\n",
    "If nucleotide level aggregation is done, the file names will include AN tag.\n",
    "\n",
    "#### target_nt_annotation, default=None\n",
    "Annotation of targeted nucleotides requires a file similar to the targeted amino acid annotation. However, the required fields for this annotation are: CHROM, POS, REF, ALT and mutation_name. **aggregate_nucleotides must be set to True** for this option to be used.\n",
    "\n",
    "#### aggregate_none, default=False\n",
    "It is also possible to generate count tables without doing any aggregation. This will generate the 3 count files, and all of the variant information included in the vcf file will be a separate column in the table's index. For annotated initial vcf files, or if annotate option is selected, each subfield in the INFO/ANN field will have its own column.\n",
    "\n",
    "#### min_site_qual, default=-1\n",
    "Filter variant sites for a minimum QUAL value assigned by the variant caller. This value is described in freebayes manual as:\n",
    "```bash\n",
    "Of primary interest to most users is the QUAL field, which estimates the probability that there is a polymorphism at the loci described by the record. In freebayes, this value can be understood as 1 - P(locus is homozygous given the data). It is recommended that users use this value to filter their results, rather than accepting anything output by freebayes as ground truth.\n",
    "\n",
    "By default, records are output even if they have very low probability of variation, in expectation that the VCF will be filtered using tools such as vcffilter in vcflib, which is also included in the repository under vcflib/. For instance,\n",
    "\n",
    "freebayes -f ref.fa aln.bam | vcffilter -f \"QUAL > 20\" >results.vcf\n",
    "\n",
    "removes any sites with estimated probability of not being polymorphic less than phred 20 (aka 0.01), or probability of polymorphism > 0.99.\n",
    "\n",
    "In simulation, the receiver-operator characteristic (ROC) tends to have a very sharp inflection between Q1 and Q30, depending on input data characteristics, and a filter setting in this range should provide decent performance. Users are encouraged to examine their output and both variants which are retained and those they filter out. Most problems tend to occur in low-depth areas, and so users may wish to remove these as well, which can also be done by filtering on the DP flag.\n",
    "```\n",
    "Therefore, a **minimum of 1** should be used as a min_site_qual to remove low quality sites. If a site is annotated as **targeted**, the site will be kept regardless of its qual value, however, the alternate observation counts for the site may be reset to zero depending on the min_target_site_qual value described below.\n",
    "\n",
    "#### min_target_site_qual, default=-1\n",
    "If a variant site is targeted but the site qual is lower than this,\n",
    "reset the alternate observation counts to 0. It may be best to leave\n",
    "this at the default value since there is usually additional evidence\n",
    "that a targeted variant exists in a samples compared to a de novo\n",
    "variant, i.e. those variants that are targeted had been observed in other samples/studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# provide a file that maps gene names to gene IDs\n",
    "# this is necessary when targeted variant annotations use\n",
    "# gene names instead of gene IDs\n",
    "geneid_to_genename = \"/opt/project_resources/geneid_to_genename.tsv\"\n",
    "# annotate targted amino acid changes in the tables.\n",
    "target_aa_annotation = \"/opt/project_resources/targets.tsv\"\n",
    "# decompose multi amino acid changes and combine counts of\n",
    "# resulting single amino acid changes\n",
    "aggregate_aminoacids = True\n",
    "# decompose MNVs and combine counts for resulting SNVs\n",
    "aggregate_nucleotides = True\n",
    "# annotate targeted nucleotide changes in the tables.\n",
    "target_nt_annotation = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# provide a file that maps gene names to gene IDs\n",
    "# this is necessary when targeted variant annotations use\n",
    "# gene names instead of gene IDs. Otherwise provide None\n",
    "geneid_to_genename = None\n",
    "# annotate targeted amino acid changes in the tables\n",
    "# using the file, or otherwise provide None\n",
    "target_aa_annotation = None\n",
    "# decompose multi amino acid changes and combine counts of\n",
    "# resulting single amino acid changes\n",
    "aggregate_aminoacids = None\n",
    "# decompose MNVs and combine counts for resulting SNVs\n",
    "aggregate_nucleotides = None\n",
    "# annotate targeted nucleotide changes in the tables.\n",
    "target_nt_annotation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USER INPUT\n",
    "\n",
    "# analysis settings dictionary\n",
    "#settings = settings\n",
    "# provide the path to the settings file\n",
    "# if settings dictionary has not been loaded\n",
    "settings_file = None\n",
    "# use snpEff to annotate the variants\n",
    "annotate = True\n",
    "# additional vt options for decomposing nucleotides.\n",
    "# Supply [\"-a\"] to include indels and complex variants\n",
    "# in decomposition, or other options shown above if desired.\n",
    "decompose_options = []\n",
    "# was the initial vcf file was annotated by snpEff?\n",
    "annotated_vcf = False\n",
    "# create tables for variants as they are in the vcf file\n",
    "# without decomposing compex variants or indels.\n",
    "# Multiallelic variants will be split into biallelic.\n",
    "aggregate_none = True\n",
    "# filter variant sites for quality\n",
    "min_site_qual = 1\n",
    "# reset targeted variant counts to zero\n",
    "# when the site quality is below this value\n",
    "min_target_site_qual = 0\n",
    "# reset genotypes in the vcf file to NA\n",
    "# and depth to 0 if FORMAT/GQ value for a variant/sample\n",
    "# is below this value:\n",
    "min_genotype_qual = 1\n",
    "# reset alt allele count in the vcf file to 0\n",
    "# if FORMAT/QA value divided by FORMAT/AO for a variant/sample\n",
    "# is below this value:\n",
    "min_mean_alt_qual = 15 # average quality cut off for variants\n",
    "# There are also available, similar filters for:\n",
    "# min_mean_ref_qual : resetting low qual reference allele counts\n",
    "# min_alt_qual : similar to min_mean_alt_qual, but for total qual score\n",
    "# min_ref_qual : similar to min_alt_qual but for reference alleles\n",
    "\n",
    "# prefix for output files, if desired.\n",
    "# this is useful when different quality thresholds etc will be used\n",
    "# to avoid overwriting the files. For example, if min_genotype_qual = 1\n",
    "# and min_mean_alt_qual = 15 is used, a suitable prefix could be\n",
    "# \"gq1.mqa15.\"\n",
    "output_prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# input vcf file\n",
    "vcf_file = vcf_file.split(\"/\")[-1]\n",
    "mip.vcf_to_tables_fb(\n",
    "     vcf_file,\n",
    "     settings=settings,\n",
    "     settings_file=settings_file,\n",
    "     annotate=annotate,\n",
    "     geneid_to_genename=geneid_to_genename,\n",
    "     target_aa_annotation=target_aa_annotation,\n",
    "     aggregate_aminoacids=aggregate_aminoacids,\n",
    "     target_nt_annotation=target_nt_annotation, \n",
    "     aggregate_nucleotides=aggregate_nucleotides, \n",
    "     decompose_options=decompose_options,\n",
    "     annotated_vcf=annotated_vcf,\n",
    "     aggregate_none=aggregate_none,\n",
    "     min_site_qual=min_site_qual,\n",
    "     min_target_site_qual=min_target_site_qual,\n",
    "     min_genotype_qual=min_genotype_qual,\n",
    "     min_mean_alt_qual=min_mean_alt_qual,\n",
    "     output_prefix=output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables created\n",
    "alternate_XX_table.csv files will contain the ALT allele count for that table type while coverage_XX_table.csv will contain the depth of coverage at each locus.\n",
    "### Nucleotide changes (aggregated)\n",
    "For some projects we may be interested in specific single nucleotide changes. For these, it would make sense to decompose multi nucleotide changes and combine counts of the same single nucleotide changes. Two tables will be generated for count and coverage data for aggregated nucleotide changes:  \n",
    "\n",
    "**alternate_AN_table.csv** file in the analysis directory is created if aggregate_nucleotides option was selected when creating data tables. This table has the UMI counts for each alternate nucleotide.  \n",
    "\n",
    "**coverage_AN_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_AN_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1. When calls from multiple variants are aggregated; if all 0/0 then -> 0, if any 0/0 and non-0/0 then -> 1, if all 1/1 then -> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino acid changes (aggregated)\n",
    "For some projects we may be interested in the amino acid changes, particularly specific, targeted amino acid changes, such as drug resistance mutations in *Plasmodium falciparum*, which is the data set provided for pipeline test. For these type of projects, we may want to analyze the variants from the amino acid perspective, rather than nucleotide changes which is standard output for variant callers.  \n",
    "\n",
    "**alternate_AA_table.csv** file in the analysis directory is created if aggregate_aminoacids option was selected when creating data tables. This table has the UMI counts for each alternate amino acid.  \n",
    "\n",
    "**coverage_AA_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_AA_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1. When calls from multiple variants are aggregated; if all 0/0 then -> 0, if any 0/0 and non-0/0 then -> 1, if all 1/1 then -> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleotide changes (not aggregated)\n",
    "For some projects we may be interested in keeping composite variants as they are called by the pipeline. These will include MNVs, comlplex variants including indels, etc. Two tables will be generated for count and coverage data for original nucleotide changes:  \n",
    "\n",
    "**alternate_table.csv** file in the analysis directory is created if aggregate_none option was selected when creating data tables. This table has the UMI counts for each alternate nucleotide.  \n",
    "\n",
    "**coverage_table.csv** file is the corresponding coverage depth for each variant's position.  \n",
    "\n",
    "**genotypes_table.csv** file contains the aggregated value of the genotypes called by freebayes: 0/0->0, 0/1->1, 1/1->2, N/A (.) ->-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling genotypes, prevalences and filtering data\n",
    "The original vcf file created by freebayes contain the genotypes determined by the program itself. In addition, genotype values for aggregated and non-aggregated nucleotides and aminoacids are also available as *_genotypes_table.csv files as described above.  \n",
    "\n",
    "However, the default parameters generating the vcf file are not very strict. In this part of the analysis we will apply various filters to the count tables and generate genotype calls based on those filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose which tables to analyse\n",
    "Select the type of data to analyse. Make sure the count file is matching the coverage file. e.g. alternate_XX_table and coverage_XX_table, XX must be the same value (AA, AN or nothing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "mutation_count_file = \"/opt/analysis/q1.mqa15.alternate_AA_table.csv\"\n",
    "mutation_coverage_file = \"/opt/analysis/q1.mqa15.coverage_AA_table.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "mutation_count_file = \n",
    "mutation_coverage_file = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "mutation_counts = pd.read_csv(mutation_count_file,\n",
    "                              header=list(range(6)),\n",
    "                              index_col=0)\n",
    "mutation_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "mutation_coverage = pd.read_csv(mutation_coverage_file,\n",
    "                                index_col=0,\n",
    "                                header=list(range(6)))\n",
    "mutation_coverage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your filters   \n",
    "1.  **min_coverage**: how many UMIs are needed to for a genomic position for a sample to reliably call genotypes. If we set min_coverage = 10, any locus within a sample that is covered below this threshold will have an NA genotype.\n",
    "2.  **min_count**: if a genomic position have enough coverage, how many UMIs supporting an ALT allele call is needed for a reliable call. If we set min_count = 2, any mutation with an  call that has less than 2 barcodes supporting the ALT call will revert to REF.\n",
    "3.  **min_freq**: a minimum within sample allele frequency threshold to consider a variant valid. If set to 0.01, for example, a variant locus in a sample that is at 0.005 frequency for the ALT allele within the sample, the locus would be called REF, if the within sample AF is between 0.01 and 0.99, it would be considered HET, and if > 0.99, it would be homozygous ALT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example cell\n",
    "```python\n",
    "# filter mutation counts for minimum count parameter\n",
    "# by setting counts to zero if it is below threshold\n",
    "min_count = 2\n",
    "# filter loci without enough coverage by setting\n",
    "# coverage to zero if it is below threshold\n",
    "min_coverage = 10\n",
    "# call genotypes using the minimum within sample\n",
    "# allele frequency\n",
    "min_freq = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT \n",
    "\n",
    "# filter mutation counts for minimum count parameter\n",
    "# by setting counts to zero if it is below threshold\n",
    "min_count = \n",
    "# filter loci without enough coverage by setting\n",
    "# coverage to zero if it is below threshold\n",
    "min_coverage = \n",
    "# call genotypes using the minimum within sample\n",
    "# allele frequency\n",
    "min_freq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# import the PCA module which has genotype calling and\n",
    "# filtering functions \n",
    "import PCA\n",
    "\n",
    "gt_calls = PCA.call_genotypes(mutation_counts, mutation_coverage,\n",
    "                              min_count, min_coverage, min_freq)\n",
    "gt_calls.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the dataframes generated by call_genotypes function and how  are they generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filtered_mutation_counts**: take the mutation_counts table, if a cell's value is below *min_count*, reset that cell's value to zero, otherwise leave as is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "filtered_mutation_counts = gt_calls[\"filtered_mutation_counts\"]\n",
    "filtered_mutation_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filtered_mutation_coverage**: take the mutation_coverage table, if a cell's value is below *min_coverage*, reset that cell's value to zero, otherwise leave as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "filtered_mutation_coverage = gt_calls[\"filtered_mutation_coverage\"]\n",
    "filtered_mutation_coverage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wsaf**: divide *filtered_mutation_counts* table by *filtered_mutation_coverage* table, yielding within sample allele frequencies.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "freq = gt_calls[\"wsaf\"]\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**genotypes**: take the *wsaf* table, if a cell's value is less than *min_freq* set the genotype value to 0 (homozygous wild type); if the cell's value is more than (*1 - min_freq*) set the genotype value to 2 (homozygous mutant), if the cell's value is between *min_freq* and (*1 - min_freq*) set the genotype value to 1 (heterozygous/mixed).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "genotypes = gt_calls[\"genotypes\"]\n",
    "genotypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prevalences**: take the *genotypes* table, if a cell's value is 2, reset its value to 1; otherwise leave as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "prevalences = gt_calls[\"prevalences\"]\n",
    "prevalences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter genotypes / prevalences\n",
    "It is generally a good idea to do some basic noise removal once the genotypes are created. Some suggestions are provided here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter variants that are always at low WSAF\n",
    "If a variant is only seen at a low frequency within samples, it is a good indication that it could be just noise. Here we will set a number of samples and minimum WSAF threshold to remove such noise.\n",
    "\n",
    "```python\n",
    "num_samples_wsaf = 2\n",
    "min_wsaf = 0.5\n",
    "wsaf_filter = ((freq > min_wsaf).sum()) >= num_samples_wsaf\n",
    "```\n",
    "\n",
    "The above options will keep the variants that are in at > 0.5 WSAF in at least 2 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "num_samples_wsaf = \n",
    "min_wsaf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsaf_filter = ((freq > min_wsaf).sum()) >= num_samples_wsaf\n",
    "print((\"{} of {} variants will remain after the wsaf filter\").format(\n",
    "    wsaf_filter.sum(), freq.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter variants that are observed with low UMI counts\n",
    "If a variant is only supported by a low number of UMIs across the entire sample set, it is another indication of noise.\n",
    "\n",
    "```python\n",
    "num_samples_umi = 2\n",
    "min_umi = 3\n",
    "umi_filter = ((filtered_mutation_counts >= min_umi).sum()) > num_samples_umi\n",
    "```\n",
    "\n",
    "The above options will keep the variants that are supported by at least 3 UMIs in at least 2 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "num_samples_umi = \n",
    "min_umi = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "umi_filter = ((filtered_mutation_counts >= min_umi).sum()) > num_samples_umi\n",
    "print((\"{} of {} variants will remain after the UMI filter\").format(\n",
    "    umi_filter.sum(), freq.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep variants that were targeted\n",
    "In most projects there are a number of variants that we would like to report, even if they are not seen in the sample set. We would like to stop those variants from being removed by the above filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "targ = freq.columns.get_level_values(\"Targeted\") == \"Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine filters\n",
    "Keep the variants that are either targeted or passing filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_mask = targ | (wsaf_filter & umi_filter)\n",
    "print((\"{} variants will remain in the final call set.\\n\"\n",
    "       \"{} variants were targeted and will be kept; and {} will be removed by \"\n",
    "       \"the combined UMI and WSAF filters.\").format(\n",
    "    variant_mask.sum(), targ.sum(), (wsaf_filter & umi_filter).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data tables with the combined filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_genotypes = genotypes.loc[:, variant_mask]\n",
    "filtered_genotypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_prevalences = prevalences.loc[:, variant_mask]\n",
    "filtered_prevalences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the tables you want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_prevalences.to_csv(\"filtered_prevalences.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "299px",
    "width": "248px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1114px",
    "left": "977px",
    "top": "163px",
    "width": "256.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
