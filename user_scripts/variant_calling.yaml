#This step runs after the wrangler step. The wrangler step generates a
#sample_sheet that is in the output folder of the wrangler step and is named
#sample_sheet.tsv. If you wrangled in some other way, you'll need to rename your
#sample sheet as sample_sheet.tsv and put it in the wrangler output folder

#location of the folder that has basic information about your MIPs. Can be an
#absolute or relative path. Don't include a '/' at the end of the folder name
project_resources: /nfs/jbailey5/baileyweb/asimkin/miptools/miptools_tutorial/test-data/DR1_project_resources

#location of the folder that has basic information about your species genome.
#Can be an absolute or relative path. Don't include a '/' at the end of the
#folder name
species_resources: /nfs/jbailey5/baileyweb/asimkin/miptools/miptools_tutorial/test-data/pf_species_resources

#location of the folder that has output files generated by the wrangler step.
#Can be an absolute or relative path. Don't include a '/' at the end of the
#folder name
wrangler_directory: /nfs/jbailey5/baileyweb/asimkin/miptools/miptools_tutorial/test-data/unlocking_folders

#name of the primary table to use from the wrangling step. Can usually be left
#at default of allInfo.tsv.gz
wrangled_file: allInfo.tsv.gz

#where you would like the output to go. Can be an absolute or relative path.
#Doesn't need to exist yet. Don't include a '/' at the end of the folder name
output_directory: /nfs/jbailey5/baileyweb/asimkin/miptools/miptools_tutorial/test-data/freebayes_threads3

#which sif file to use
sif_file: /nfs/jbailey5/baileyweb/bailey_share/bin/miptools_dev_24-03-18.sif
species: pf #abbreviated name of the species you're analyzing. Needs to match a
#value found in first column of file_locations.tsv (from species_resources
#directory). In the tutorial dataset, this is 'pf'

#the probe sets here need to be an exact match for the values in your sample
#sheet - the first value is the value you'd like to pull from the sample_set
#column of the sample sheet, and the second value is the value you'd like to
#pull from the probe_set column of the sample sheet. E.g. to pull all samples
#that have 'JJJ' listed in the sample_set column and 'DR1,VAR4' in the
#'probe_set' column, you would use [['JJJ', 'DR1,VAR4']]. You can also pull
#multiple combinations if the samples you're interested in have multiple
#sample_set column values or multiple probe_set column values, e.g.
#[['JJJ','DR1,VAR4'], ['JJJ', 'DR1']] would pull samples that match 'JJJ' 
#sample_set and 'DR1,VAR4' probe_set, but also samples that match 'JJJ'
#sample_set and 'DR1' probe_set 
sample_groups: [['JJJ', 'DR1,VAR4']]

#The actual probe set you'd like to analyze. This needs to match the probeset
#you provided in your project_resources folder. The setting above is for
#selecting samples based on the value present in the probe_set column of the
#sample sheet (which may include multiple comma delimited probe sets for each
#sample) whereas the one below is for selecting the single probe set you'd like
#to analyze in this pipeline. Needs to include the square brackets and quotes
#e.g. ['DR1']
probe_sets_used: ['DR1']

#number of processors to use for the less resource-intensive parts
#of the pipeline
processor_number: 16

#number of processors to use for freebayes, which is extra memory intensive and
#therefore more likely to crash. Set this threadcount lower to avoid this.
freebayes_threads: 8

#bwa settings go here. No need to edit this unless you'd like to decrease the
#number of threads to use for bwa. bwa runs quickly and is not usually the cause
#of any crashes.
bwa_extra: ['-t', '16']

#leave these at their default values unless you'd like to only analyze
#haplotypes that have a minimum number of UMIs, or that are seen in a minimum
#number of samples, or that account for a minimum fraction of the UMIs that
#exist in any given sample. The recommendation here is to not do any filtering
#at this step (we'd like to see the output data in as 'raw' a format as
#possible) unless you have too many haplotypes for the analysis to finish.
min_haplotype_barcodes: 1
min_haplotype_samples: 1
min_haplotype_sample_fraction: 0.0001

#repool spreadsheet settings. These are for determining which samples to repool
#or recapture, and for determining whether each sample should be repooled or
#recaptured. Current settings indicate that if 95% of the mips in a sample
#(target_coverage_fraction=0.95) hit 10 UMIs per sample (target_coverage_key
#=targets_with_10_barcodes) then this sample doesn't need to be repooled or
#recaptured. Samples that get 10,000 UMIs or more in total (high_barcode_
#threshold=10000) also don't need to be resequenced or repooled. If a sample has
#more than 10 reads per UMI on average (barcode_coverage_threshold=10) then it
#should probably be recaptured with more MIPs, because the same small number of
#UMIs are being sequenced again and again. If the total number of UMIs for a
#sample is below 100 (barcode_count_threshold=100) then recommend recapturing
#(low_coverage_action=Recapture). Samples that have more than the 25th
#percentile of the coverage (of at least 1 UMI per sample (assessment_key=
#'targets_with_1_barcodes')) seen for completed samples (good_coverage_quantile=
#0.25) and yet still aren't complete are marked as 'uneven coverage'. These
#settings only impact the advice that the repool.csv spreadsheet will give you
#and can probably be mostly left alone, except that the best choice for high_
#barcode_threshold will be strongly impacted by the number of MIPs in your project
#design.
high_barcode_threshold: 10000
low_coverage_action: Recapture
target_coverage_count: null
target_coverage_fraction: 0.95
target_coverage_key: targets_with_10_barcodes
barcode_coverage_threshold: 10
barcode_count_threshold: 100
assessment_key: targets_with_1_barcodes
good_coverage_quantile: 0.25


#This block is for variant calling with for freebayes. These are the settings
#that we've found work best for Plasmodium falciparum.
freebayes_settings: ["--pooled-continuous", "--min-alternate-fraction", "0.01",
  "--min-alternate-count", "2", "--haplotype-length", "3",
  "--min-alternate-total", "10", "--use-best-n-alleles", "70",
  "--genotype-qualities", "--gvcf", "--gvcf-dont-use-chunk", "true"]

#This block is for settings for generating final allele tables. 'geneid_to_
#genename' is the location within your project resources folder of a file that
#tells this pipeline which "common names" to assign to each official gene ID.
#'target_aa_annotation' is a file of known targeted amino acid mutations of the
#genome that we'd like freebayes to make calls for even if the regions are not
#mutated. 'target_nt_annotation' is the same thing but for noncoding regions of
#the genome. 'aggregate_nucleotides' controls whether complex nucleotide changes
#that result in the same outcome get lumped together or analyzed separately,
#while 'aggregate_aminoacids' does the same thing for protein-coding mutations.
#'annotate' controls whether the VCF file gets annotated by SNPEff. 'decompose_
#options' is for more granular control of the splitting of complex mutations
#into simple ones and can be left empty for now. If 'aggregate_none' is set to
#'true' then no aggregation across different types of complex mutations will be
#performed. 'annotated_vcf' tells the pipeline whether the input VCF file is
#already annotated with SNPEff, and 'output_prefix' currently needs to be set to
#empty quotes. This function will change the prefixes of the output tables but
#will also cause the snakemake pipeline to error out due to 'missing' the final
#expected output tables with their default names.
geneid_to_genename: /opt/project_resources/geneid_to_genename.tsv
target_aa_annotation: /opt/project_resources/targets.tsv
target_nt_annotation: null
aggregate_nucleotides: true
aggregate_aminoacids: true
annotate: true
decompose_options: []
aggregate_none: true
annotated_vcf: false
output_prefix: ''
