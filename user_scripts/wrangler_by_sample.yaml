#folder where your output will go.
output_folder: /nfs/jbailey5/baileyweb/asimkin/miptools/miptools_tutorial/test-data/wrangler_snakemake

#how many CPUs (or threads or processors) to use in parallel. More CPUs means
#faster run times (if your computer has at least this many processors) but also
#means more RAM used. Doubling processors doubles the RAM, and if you run out of
#RAM, the job will crash. If the job crashes, set cpu_count lower and run it
#again (it will pick up at the step where the crash occurred).
cpu_count: 16

#location of sample sheet
input_sample_sheet: /nfs/jbailey5/baileyweb/asimkin/miptools/miptools_tutorial/test-data/test_data/sample_list.tsv

#location of project resources
project_resources: /nfs/jbailey5/baileyweb/asimkin/miptools/miptools_tutorial/test-data/DR1_project_resources

#location of input fastq file
fastq_dir: /nfs/jbailey5/baileyweb/asimkin/miptools/miptools_tutorial/test-data/test_data/fastq

#location of sif file to use
miptools_sif: /nfs/jbailey5/baileyweb/bailey_share/bin/miptools_dev_24-02-12.sif

#only rows from the sample sheet that contain exact matches to the probe sets
#listed here (after splitting the probe_set column with commas) will be
#analyzed.
probe_sets_used: DR1

#only rows from the sample_set column of the sample sheet that are an exact
#match to the sample set listed below will be analyzed
sample_set_used: JJJ

#if you have a very large number of samples that won't run all the way from
#start to finish in one pass, or if you'd like to troubleshoot this pipeline,
#you can have it generate an intermediate output other than the final step.
#Input should be a number. By default, this should be set to 6. Options are:
#1 (extraction of reads for each sample)
#2 (UMI correction for each sample)
#3 (correction for samples that had the wrong sample barcodes assigned)
#4 (haplotype clustering for each sample)
#5 (haplotype clustering for each mip)
#6 (final output table)
output_choice: 6

#memory_mb_per_step applies only when submitting on a slurm cluster. Since in
#our experience slurm submissions slow down analysis, you can leave this at its
#default value.
memory_mb_per_step: 20000

#number of umis to include for each mip/sample combo. This keeps massively over-
#sequenced stuff from bogging down the analysis. 20000 is default, but in
#practice, we haven't extensively tested this feature so recommend setting it to
#a very high number (e.g. 20000000000) to avoid downsampling altogether.
downsample_umi_count: 20000000000

#when downsampling occurs, this random seed allows you to make sure thesame
#random seeds get chosen when re-running a sample (or you could change the
#number to see if results are robust when different random umis are chosen).
#Since we're not recommending downsampling yet, you can leave this value alone.
downsample_seed: 312

